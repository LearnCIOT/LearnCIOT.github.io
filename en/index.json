[
  {
    "content": "1. Introduction Table Of Contents Environment Perception Open Data Civil IoT Taiwan Civil IoT Taiwan Data Service Platform References Environment Perception Human beings are full of curiosity about the changes in the natural world, which can probably trace back to the activities of ancient agricultural civilizations, looking up at the stars and stargazing at night. By the Renaissance in the 16th century AD, Copernicus deduced the “heliocentric theory” based on the existing astronomical data. After Galileo and Newton, it became the origin of modern science. Since modern times, due to the rapid development of semiconductor manufacturing technology, the tools that people use to perceive changes in the living environment have become more and more diverse, sophisticated and miniaturized. In order to catch up with the ever-changing trends of information technology, combined with the real-time perception of sensors and the connection of the Internet, data transmission is getting faster and more popular, and a large amount of observation data is generated, transmitted and archived. In response to massive data, scientists have been working hard to find the laws of environmental changes, explore the relationship between these laws and disasters, and obtain prediction results to improve people’s quality of life, thereby improving the efficiency of disaster prevention and relief, and promoting a more friendly relationship between people and the environment. The goal of creating a beautiful and sustainable living environment has become a significant issue of common concern to individuals, groups, societies and countries in the world today.\nOpen Data “Open data” refers to a type of electronic data (including but not limited to text, data, pictures, videos, sounds, etc.) that is publicly permitted and encouraged to be used by all sectors of society through certain procedures and formats. According to the definition of the Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/), open data must meet the following conditions:\nOpen license or status: The content of the open data must be in the public domain and released with an open license without any additional restrictions. Access: The content of open data must be freely accessible from the Internet, but under reasonable conditions, access to open data may also allow a one-time fee and allow for conditional incidental terms. Machine readability: The content of open data must be easily accessible, processed and modified by computers. Open format: The content of open data must be in an available form, which can be accessed using free or open software. With the prevalence of open trends such as open source code, open government, and open science, open data has gradually become a standard that governments and scientific communities follow when engaging in various policy advocacy and academic research. The recently booming environment-aware IoT has become one of the most anticipated open data projects due to its widespread distribution in the public domain and its observed environmental information being closely related to the public.\nAt present, most of the common open data on the Internet use data formats such as JSON, CSV or XML. The full name of JSON format is Javascript Object Notation, which is a lightweight structured data exchange standard. Its content consists of properties and values, which is easy for computers or users to read and use, and is often used for data presentation, transmission and storage on the website.\nThe full name of the CSV format is Comma-Separated Values. As the name suggests, it is a form of storing table data in plain text, where each data is a column and each attribute in the data is a column. All properties must be stored as plain text in a specific order, with a specific symbol to separate different properties. Commonly used for file import and export of applications, network data transmission, historical data storage, etc.\nThe full name of the XML format is Extensible Markup Language, which is an extensible markup language simplified from the Standard Generalized Markup Language (SGML), allowing users to define the required tags. It is often used for the presentation of document file data and the exchange of network data in order to create documents containing structured information.\nCurrently, open data commonly used in Taiwan are aggregated on the following platforms:\nTaiwan government open data platform (https://data.gov.tw/en/) Taiwan open weather data (https://opendata.cwb.gov.tw/devManual/insrtuction) Taiwan EPA open data platform (https://data.epa.gov.tw/en) Civil IoT Taiwan The “Civil IoT Taiwan” project is developed to address the four urgent needs of the public in order to integrate and close to the public services related to people’s livelihood, including air quality, earthquake, water resources, disaster prevention and other issues. In the “Digital Technology” section of the “Forward-looking Infrastructure Development Program” project, the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, the Academia Sinica, and the Council of Agriculture have jointly constructed a large-scale inter-ministerial government project, which applies big data, artificial intelligence, and Internet of things technologies to build various smart life service systems to help the government and the public face the challenges brought about by environmental changes. At the same time, this project also considers the experience of different end-users, including government decision-making units, academia, industry, and the general public. The goal is to enhance intelligent governance, assist the development of industry/academia, and improve the happiness of the public.\nAt present, the four major areas covered by the Civil IoT Taiwan project are:\nWater resources: The Civil IoT Taiwan project cooperates with the water conservancy units of the central and local governments to develop and deploy various water resource sensors and build various hydrological observation facilities and farmland irrigation water sensors. The goal of this project is 1) to effectively integrate surface, groundwater, emerging water source information, and related monitoring information; 2) to establish a water resource IoT portal and dynamic analysis and management platform for irrigation water distribution to strengthen the flood control operation system of each river bureau; 3) to establish the sewer cloud management platform that can perform remote, automated and intelligent management for relevant units at all levels; and 4) to achieve the goal of combined use and promote various applications such as intelligent control and management of water sources, dynamic analysis and management of irrigation water distribution, value-added applications of sewage and sewer, and road flood warning. The opening of data will promote the development of water resources data applications and public-private partnership decision-making. Air quality: The Civil IoT Taiwan project, in conjunction with the Environmental Protection Agency, the Ministry of Economic Affairs, the Ministry of Science and Technology, and Academia Sinica, started with the deployment of air quality sensing infrastructure, developed air quality sensors, and established a sensor performance testing and verification center. And many air quality sensors for different purposes are widely distributed throughout Taiwan. This project launches a computing operation platform for the Internet of Things and an intelligent environment sensing data center by collecting a large amount of sensing data at shorter time and space scales. It builds a visualization platform for air quality sensing data. On the one hand, it promotes the development of the Internet of Things for air quality sensing. On the other hand, it provides high-resolution evidence data for intelligent environmental protection inspections to identify polluted hot spots and effectively conduct environmental governance. At the same time, the project also strengthens the research and development capacity of its domestic technology during the plan period. It establishes the development of domestic, independent airborne product sensing technology. Earthquakes: The Civil IoT Taiwan project also significantly increases the number of on-site earthquake quick-reporting stations to provide high-density, high-quality earthquake quick-reporting information for everyday seismic activity in Taiwan. At the same time, seismic and geophysical observatories were added and upgraded to improve the quality and resolution of observational data, including Global Navigation Satellite System (GNSS) stations, underground seismic stations, strong earthquake stations, geomagnetic stations, and groundwater stations. In addition, the project is enhancing its GNSS stations, underground seismic stations, and drone observations for regional monitoring of the Datun Volcano. Given Taiwan’s unique island characteristics, it has strengthened its forecasting capabilities for solid earthquakes and tsunamis and expanded submarine optical cables and related sub and land stations. In addition, the project has constructed an integrated data management system for Taiwan’s seismic and geophysical data through the integration and application of big data. The system can facilitate the integration of local and regional earthquake quick report information, strengthen the monitoring of Taiwan’s fault areas and the Datun volcanic area, and provide fast and complete earthquake rapid report information transmission, thereby providing the public with real-time early warning of strong earthquakes and promoting the development of the earthquake disaster prevention industry. Disaster prevention and relief: The Civil IoT Taiwan project gathers 58 types of warning data covering air, water, land, disaster, and people’s livelihood into a single “Civil Alert Information Platform” to provide the public with real-time disaster prevention and relief information. Moreover, incorporating the emergency management information cloud system (EMIC2.0) and other decision-making assisting systems, the platform provides disaster prevention personnel with various disaster situations, notifications, and disaster relief resources to help in multiple decision-making work. At the same time, the relevant historical data is collected and released in a unified data format, which is made available for the disaster prevention industry to analyze, use, and promote the development of the disaster information industry chain. Civil IoT Taiwan Data Service Platform In addition, the Civil IoT Taiwan project also plans the Civil IoT Taiwan Data Service Platform to accommodate various data generated by the Civil IoT Taiwan project and provide stable and high-quality sensing data for various environmental management purposes. In the spirit of open data, the platform adopts a unified data format, offers real-time data interfaces and historical data query services, improves user browsing and search speed, and sets up sensor data storage mechanisms to help scientific computing and artificial intelligence applications. The data service platform can narrow the gap in environmental information and provide more real-time and comprehensive environmental data, allowing the public to view perception information and temporal and spatial changes in real time and to understand the living environment at any time. The data provided by the platform can also be used as a basis for industrial value-added, thereby stimulating the creativity of the people and providing high-quality services for solving their problems of the people.\nCurrently, the Civil IoT Taiwan Data Service Platform uses the open data format of the OGC SensorThings API. Please refer to the following slides and pictures for relevant data format descriptions and data content in various fields:\nIntroduction to Civil IoT Taiwan Data Service Platform [PPT] Introduction to Civil IoT Taiwan Data Service Platform and OGC SensorThings API [Video] (in Chinese) Water Resources [PPT] Water Resources IoT [Video] (in Chinese) Air Quality [PPT] Environment Quality Sensing IoT [Video] (in Chinese) [PPT] Deployment of Micro PM2.5 Sensors [Video] (in Chinese) Earthquake [PPT] Joint Sea-Land Earthquake Observation [Video] (in Chinese) [PPT] Composite Earthquake Quick Report [Video] (in Chinese) Disaster prevention and relief [PPT] Civil Alert Open Data [Video] (in Chinese) [PPT] Integration of Disaster Prevention and Relief Information Systems [Video] (in Chinese) Since 2018, the Civil IoT Taiwan project has also held a series of data application competitions, data innovation hackathons, and physical and virtual exhibitions to allow more people to participate in the Civil IoT Taiwan project and its data platform. The project also designed a series of training materials and business coaching, including the team build-up, the idea, and the application service. After several years of accumulation, the project has successfully developed a successful case, which shows that the Civil IoT Taiwan project is not only a hardware construction for government units but also has successfully transformed into a basic information construction for people’s livelihood. The project provides a steady stream of high-quality sensor data to improve people’s lives and promote more innovative, convenient, and caring information services.\nFor examples of applications and solutions in various fields of Civil IoT Taiwan data, please refer to the following website resources:\nCivil IoT Taiwan Service and Solution Guide: Water resources Civil IoT Taiwan Service and Solution Guide: Air qualuty Civil IoT Taiwan Service and Solution Guide: Earthquake Civil IoT Taiwan Service and Solution Guide: Disaster prevention and relief References Open Definition: defining open in open data, open content and open knowledge. Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/) Civil IoT Taiwan (https://ci.taiwan.gov.tw) Civil IoT Taiwan Virtual Expo: Dialogue in Civil IoT (https://ci.taiwan.gov.tw/dialogue-in-civil-iot) Civil IoT Taiwan Service and Solution Guide (https://www.civiliottw.tadpi.org.tw) Civil IoT Taiwan Data Service Platform (https://ci.taiwan.gov.tw/dsp/) XML - Wikipedia (https://en.wikipedia.org/wiki/XML) JSON - Wikipedia (https://en.wikipedia.org/wiki/JSON) Comma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values) Standard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language) OGC SensorThings API Documentation (https://developers.sensorup.com/docs/) ",
    "description": "Introduction",
    "tags": [
      "Introduction"
    ],
    "title": "1. Introduction",
    "uri": "/en/ch1/"
  },
  {
    "content": "2. Overview of the Materials In this topic, we will introduce the overall structure of the developed materials of Civil IoT Taiwan Open Data, as well as the programming language Python and the development platform Google Colab used in the materials. In addition to conceptual descriptions, we also provide a large number of extended learning resources from shallow to deep, so that interested readers can further explore and learn according to their own needs.\n2.1. Material ArchitectureIntroduction of the material architecture\n2.2. Material ToolsA brief introduction of the programming language Python and the development platform Google Colab used in the materials\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "2. Overview of the Materials",
    "uri": "/en/ch2/"
  },
  {
    "content": "3. Data Access In this topic, we will describe how to directly access the data of the Civil IoT Taiwan Data Service Platform using simple Python syntax through the pyCIOT package. To gain a deeper understanding of the different ways to access data with examples, we divide this topic into two units for a more in-depth introduction:\n3.1. Basic Data Access MethodsWe introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.\n3.2. Data Access under Spatial or Temporal ConditionsWe introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "3. Data Access",
    "uri": "/en/ch3/"
  },
  {
    "content": "4. Time Series Data Analysis In this topic, we will introduce time series data analysis methods for IoT data. We will develop the following three units for a more in-depth exploration by using the Civil IoT Taiwan Data Service Platform.\n4.1. Time Series Data ProcessingWe use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.\n4.2. Time Series Data ForecastWe use the sensing data of the Civil IoT Taiwan Data Service Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare the prediction results of different data prediction models. We use graphics to present the data and discuss the significance of the data prediction of the dataset at different time resolutions in the real field, as well as possible derived applications.\n4.3. Time Series Data ClusteringWe introduce advanced data grouping analysis. We first present two time-series feature extraction methods, Fourier transform and wavelet transform, and briefly explain the similarities and differences between the two transform methods. We introduce two different time series comparison methods, Euclidean distance and dynamic time warping (DTW), and apply existing clustering algorithms accordingly. We then discuss data clustering with different temporal resolutions, as well as their representation and potential applications in the real world.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "4. Time Series Data Analysis",
    "uri": "/en/ch4/"
  },
  {
    "content": "5. Spatial Data Analysis In this topic, we introduce a series of spatial data processing and analysis methods according to the geospatial characteristics of IoT data. Through the use of the Civil IoT Taiwan Data Service Platform, we will develop the following units for a more in-depth introduction.\n5.1. Geospatial FilteringWe use Civil IoT Taiwan's earthquake and disaster prevention and relief data and filter the data of specific administrative areas by overlaying the administrative area boundary map obtained from the government open data platform. Then, we generate the image file of the data distribution location after the superimposed map. In addition, we also demonstrate how to Nest specific geometric topological regions and output nested results to files for drawing operations.\n5.2. Geospatial AnalysisWe use Civil IoT Taiwan's sensor data to introduce more advanced geospatial analysis. Using the GPS location coordinates in the site information, we first use the package to find the largest convex polygon (Convex Hull) to frame the geographical area covered by the sensor. Then we apply the Voronoi Diagram package to draw the distribution status of each sensor on the map according to the sensor and crops out the area of influence of each sensor. We adopt the spatial interpolation method for the space between sensors, applying different spatial interpolation algorithms. We then populate the spatial map with values based on the sensor values and generate a corresponding image output.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "5. Spatial Data Analysis",
    "uri": "/en/ch5/"
  },
  {
    "content": "6. Data Applications In this topic, we will focus on derivative applications using Civil IoT Taiwan open data. We strengthen the value and application services of Civil IoT Taiwan’s open data by importing other library packages and analysis algorithms. The units we expect to develop include:\n6.1. Machine Learning PreliminariesWe use air quality and water level data, combined with weather observations, using machine learning for data classification and data grouping. We demonstrate the standard process of machine learning and introduce how to further predict data through data classification and how to further explore data through data grouping.\n6.2. Anomaly DetectionWe use air quality data to demonstrate the anomaly detection framework commonly used in Taiwan's micro air quality sensing data. We learn by doing, step by step, from data preparation and feature extraction to data analysis, statistics, and induction. The readers will experience how to gradually achieve advanced and practical data application services by superimposing basic data analysis methods.\n6.3. Joint Data CalibrationWe use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "6. Data Applications",
    "uri": "/en/ch6/"
  },
  {
    "content": "7. System Integration and Applications In this theme, we will focus on the integration and application of the Civil IoTTaiwan Open Data and other application software, and through the professional functions of other application software, to further deepen and develop the value of the Civil IoT Taiwan Open Data.\n7.1. QGIS ApplicationWe introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.\n7.2. Tableau ApplicationWe introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.\n7.3. Leafmap ApplicationsWe introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "7. System Applications",
    "uri": "/en/ch7/"
  },
  {
    "content": "This set of materials includes seven themes including “Introduction”, “Overview of the Materials”, “Data Access Method”, “Data Analysis in Time Dimension”, “Data Analysis in Space Dimension”, “Data Applications” and “System Applications”. The major themes are described as follows:\nIntroduction: We introduce the core of the entire project - “Civil IoT Taiwan Project,” including various achievements of the project in the past few years, as well as its open data in water resources, air quality, earthquakes, and disaster prevention and relief fields. We also present different existing materials, videos, and successful cases, so everyone can deeply understand the importance of the “Civil IoT Taiwan project” to people’s livelihood.\nOverview of the Materials: We present the structure of the entire material, as well as the Python programming language and Google Colab platform used in the material. Through simple text descriptions, we guide readers to quickly understand the structure of the entire material, and provides a rich list of external resources for those who are interested and in need to further explore related technologies.\nData Access Method: We introduce how to use simple Python syntax to directly access the data of the Civil IoT Taiwan Data Service Platform through the pyCIOT tool we developed and cut out two units for a more in-depth introduction according to different needs:\nBasic Data Access Method: We introduce how to obtain the latest sensing data of a single station for different aspects of water resources, air quality, earthquake, and disaster information in the Civil IoT Taiwan Data Service Platform for people’s livelihood, as well as how to obtain a list of all stations, and the latest current data of all stations. Specific Spatio-temporal Data Access Method: We introduce how to obtain the data of a particular station at an exact time or time range and find the latest current data of the nearest station. We also introduce how to find the latest data of all stations around the given coordinates from the Civil IoT Taiwan Data Service Platform. In addition to introducing data access methods, we also intersperse with fundamental exploratory data analysis (EDA) practices and use commonly used statistical methods to describe the data characteristics of different aspects of the data. Finally, we draw simple diagrams to let readers experience what it’s like to analyze data firsthand through learning by doing.\nData Analysis in Time Dimension: Given the time series characteristics of IoT data, we design three topics to demonstrate time-series data analysis using Civil IoT Taiwan data.\nTime Series Data Processing: We use the sensing data of the Civil IoT Taiwan Data Platform to guide readers to understand the use of moving averages, perform periodic analysis on time series data, and then disassemble the time series data into long-term trends, seasonal changes, regular changes, and random fluctuations. At the same time, change point and outlier detection techniques are applied to check the existing Civil IoT Taiwan Data using the current Python packages. The possible meanings behind the detection results are discussed. Time Series Data Forecast: We use sensory data from the Civil IoT Taiwan Data Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare different data prediction methods in a hands-on fashion. Combining the forecast results, we present the data in a graph and discuss the implications behind the forecast results for different datasets and temporal resolutions. Then, we also discuss possible derivative applications. Time Series Data Clustering: We introduce more advanced data clustering analysis. We first introduce two time-series feature extraction methods, namely Fourier transform and wavelet transform, and briefly explain the difference between the two transform methods. We then present two distance functions for time series data, Euclidean and Dynamic Time Warping (DTW) distance, and apply existing clustering packages using these two functions. Finally, we discuss the implications behind clustering results using different datasets and temporal resolutions in real-world applications and possible derived applications. Data Analysis in Space Dimension: Given the geospatial characteristics of Civil IoT Taiwan data, we introduce a series of spatial data analysis methods according to different needs and applications.\nGeospatial Filtering: We use the earthquake and disaster prevention/relief data of the Civil IoT Taiwan Data Platform and superimpose the administrative region boundary map data obtained from the government’s open data platform. Then, we generate a picture file of geospatial data distribution after generating the overlay map and demonstrate how to nest specific regions of geometric topology, output the nested results to a file, and perform drawing operations. Geospatial Analysis: We use sensing data from Civil IoT Taiwan Data Platform to introduce more advanced geospatial analysis. Using the GPS coordinates of the sensors, we first use an existing Python package to find the largest convex polygon (Convex Hull) to frame the geographic area covered by the sensors. Then, we apply the Voronoi Diagram algorithm to cut the map’s location based on the sensors’ distribution, cutting out the sphere of influence of each sensor. For the area between the sensors, we apply different spatial interpolation algorithms, fill the values on the spatial map according to the sensor values, and produce the corresponding image output. Data Applications: In this topic, we will focus on the derivative applications after accessing the Civil IoT Taiwan data platform. We will enhance the value and application services of Civil IoT Taiwan’s open data by importing other library suites and analysis algorithms. We will develop the following three subtopics in turn:\nA First Look at Machine Learning: We use air quality and water level category data, combined with weather observations, to perform predictive analysis of sensory values using machine learning packages. We demonstrate the standard process of machine learning and describe how to evaluate the effectiveness of predictive analytics and avoid the bias and overfitting problems that machine learning is prone to. Anomaly Detection: We use air quality category data to demonstrate sensor anomaly detection algorithms commonly used in Taiwanese micro air quality sensing data. In a learning-by-doing way, including data preparation, feature extraction, data analysis, statistics, and induction, anomaly detection algorithms’ principle and implementation process are reproduced step by step. We also guide readers to experience how to superimpose fundamental data analysis and machine learning to achieve advanced and practical data application services. Dynamic Calibration Model: We use air quality category data to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official stations. By learning by doing, it reproduces the principle and implementation process of the sensor dynamic calibration model algorithm, allowing readers to experience how to gradually realize advanced and fundamental data analysis by overlaying and machine learning steps to provide valuable data application services. System Applications: This topic focuses on the integration and application of Civil IoT Taiwan open data and other application software and further deepens and exerts the value of Civil IoT Taiwan available data through the professional functions of other application software. Units we develop include:\nQGIS Applications: QGIS is a free and open-source GIS software. With this software, users can organize, analyze and draw different geospatial data and thematic maps, presenting the distribution and information types of geographical phenomena. We use QGIS to process Civil IoT Taiwan data, including water, air quality, earthquake, and disaster prevention and relief categories. We overlay different information by clicking and dragging to perform as described in the “Geospatial Filtering” and “Geospatial Analysis” topics. We also discuss the pros and cons of QGIS software and when to use it. Tableau Applications: Tableau is an easy-to-use, powerful data visualization software that can easily connect databases and data files in various formats to create multiple beautiful statistical charts. We demonstrate how to use drag-and-drop to import data from four aspects of Civil IoT Taiwan, including water resources, air quality, earthquakes, and disaster prevention and relief categories, and make simple charts to compare numerical data. Geospatial data is also overlaid on the map, and time-series data is presented as graphs showing trends in values. In addition, through the interface of external data sources, we can integrate datasets from multiple different sources into a single map to generate beautiful reports in a short time. Leafmap Applications: Leafmap is a Python development toolkit deeply integrated with Google Earth Engine to directly manipulate and visualize analysis results on platforms such as Google Colab or Jupyter Notebook. We demonstrate the use of leafmap to process Civil IoT Taiwan data, including categories such as water resources, air quality, earthquake, disaster prevention and relief, and build a simple GIS system combined with Google Earth Engine’s rich satellite imagery. At the same time, Streamlit is a tool for Python users to quickly create a web application framework. We demonstrate the integration of leafmap application and Streamlit to form a web version of the GIS information service, expanding readers’ imagination for data analysis and future information services. References Civil IoT Taiwan. https://ci.taiwan.gov.tw Civil IoT Taiwan Data Platform. https://ci.taiwan.gov.tw/dsp/ QGIS - A Free and Open Source Geographic Information System. https://qgis.org/ Tableau - Business Intelligence and Analytics Software. https://www.tableau.com/ Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment. https://leafmap.org/ Streamlit - The fastest way to build and share data apps. https://streamlit.io/ Google Colaboratory. https://colab.research.google.com/ ",
    "description": "Introduction of the material architecture",
    "tags": [
      "Introduction"
    ],
    "title": "2.1. Material Architecture",
    "uri": "/en/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents What is pyCIOT? pyCIOT Basics Installation Import Package Data Access Air Quality Data Get all project codes: Air().get_source() Get all stations: Air().get_station() Get data of a station: Air().get_data() Water Resource Data Get all project codes: Water().get_source() Get all stations: Water().get_station() Get data of a station: Water().get_data() Earthquake Data Get all project codes: Quake().get_source() Get all stations: Quake().get_station() Get data of a station: Quake().get_data() Get data of an earthquake eventQuake().get_data() Weather Data Get all project codes: Weather().get_source() Get all stations: Weather().get_station() Get data of a station: Weather().get_data() CCTV Data Get all project codes: CCTV().get_source() Get data of a station: CCTV().get_data() Disaster Alert and Notification Data Get disaster alerts: Disaster().get_alert() Get historical data of disaster notifications: Disaster().get_notice() References This article describes how to use the pyCIOT package and basic access to air, water, earthquake, weather, CCTV, and disaster warning data of the Civil IoT Taiwan Data Service Platform. For different types of data, we describe how to get the latest sensing data for a single site, get a list of all sites, and get the latest current sensing data for all sites.\nThis article requires the reader to have basic terminal operation ability and have been exposed to the basic syntax of Python programming.\nWhat is pyCIOT? The Civil IoT Taiwan project provides a wide variety of data, and different data often have different data formats and access methods. Even if the data is under an open license, organizing the data can be cumbersome due to the different ways in which it is downloaded and processed. To solve this problem, we developed the pyCIOT suite to collect all public data on people’s livelihood publicly released by the government, and strive to lower the threshold for obtaining public data and reduce the cost of data processing.\npyCIOT Basics Installation We first download and install the pyCIOT suite using pip. pip is a package management system written in Python for installing and managing Python packages. The pyCIOT suite used this time is managed by the Python package index (pypi). We can use this command in the terminal to download the pyCIOT library locally, or download other required packages together:\n!pip install pyCIOT Import Package To use this package, just enter the import syntax and import pyCIOT.data:\n# Import pyCIOT.data from pyCIOT.data import * Depending on how the package is imported, the method is called differently. If you use the from ... import ... syntax, you don’t need to add the prefix when calling a method; but if you use import ... , you need to add it every time you call a method in its package. If you use import ... as ... , you can call a method based on the custom prefix after as .\nimport pyCIOT.data a = pyCIOT.data.Air().get_source() import pyCIOT.data as CIoT a = CIoT.Air().get_source() from pyCIOT.data import * a = Air().get_source() Data Access The data of the Civil IoT Taiwan project can be obtained through the following methods, including air, water, earthquake, weather, CCTV, etc.:\n.get_source() : Return all project codes in Civil IoT Taiwan Data Service Platform in array format according to the data type. .get_station(src='') : Return basic information of all station data in array format. The src parameter is optional to specify the project code to be queried. .get_data(src='', stationID='') : Return basic information of all stations and their latest measurement result in array format. The src parameter is optional to specify the project code to be queried, and the stationID parameter is optional to specify the device ID to be queried. The following applies to disaster notification data.\n.get_alert() : Return the alert data, along with the information of the event, in JSON format. .get_notice() : Return the notification data, along with the informaiton of the event, in JSON format. Note that since this package is still under revision, if it is inconsistent with the content of the pyCIOT Package Document, it shall prevail.\nAir Quality Data Get all project codes: Air().get_source() a = Air().get_source() print(a) ['OBS:EPA', 'OBS:EPA_IoT', 'OBS:AS_IoT', 'OBS:MOST_IoT', 'OBS:NCNU_IoT'] The followings are valid project codes for air quality data:\nOBS:EPA: national level monitoring stations by EPA OBS:EPA_IoT: low-cost air quality stations by EPA OBS:AS_IoT: micro air quality stations by Academia Sinica OBS:MOST_IoT: low-cost air quality stations by MOST OBS:NCNU_IoT: low-cost air quality stations by National Chi Nan University Get all stations: Air().get_station() b = Air().get_station(src=\"OBS:EPA_IoT\") b[0:5] [ { 'name': '智慧城鄉空品微型感測器-10287974676', 'description': '智慧城鄉空品微型感測器-10287974676', 'properties': { 'city': '新北市', 'areaType': '社區', 'isMobile': 'false', 'township': '鶯歌區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10287974676', 'locationId': 'TW040203A0507221', 'Description': '廣域SAQ-210', 'areaDescription': '鶯歌區' }, 'location': { 'latitude': 24.9507, 'longitude': 121.3408416, 'address': None } }, ... ] Get data of a station: Air().get_data() f = Air().get_data(src=\"OBS:EPA_IoT\", stationID=\"11613429495\") f [ {'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 30.6}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-28T06:23:08.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 9.8}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}} ] print(f[0]['description']) for f_data in f[0]['data']: if f_data['description'] == '溫度': print(f_data['description'], ': ', f_data['values'][0]['value'], ' (', f_data['values'][0]['timestamp'], ')', sep='') 智慧城鄉空品微型感測器-11613429495 溫度: 30.6 (2022-08-28T06:22:08.000Z) Water Resource Data Get all project codes: Water().get_source() Return the project names based on the input parameter:\nwater_level_station: Return the names of water level related projects (currently valid codes are WRA, WRA2 and IA) gate: Return the names of water gate related projects (currently valid codes are WRA, WRA2 and IA) pumping_station: Return the names of pumping station related projects (currently valid codes are WRA2 and TPE) sensor: Return the names of water sensor related projects (currently valid codes are WRA, WRA2, IA and CPAMI) `` (none): Return the names of all water related projects wa = Water().get_source() wa ['WATER_LEVEL:WRA_RIVER', 'WATER_LEVEL:WRA_GROUNDWATER', 'WATER_LEVEL:WRA2_DRAINAGE', 'WATER_LEVEL:IA_POND', 'WATER_LEVEL:IA_IRRIGATION', 'GATE:WRA', 'GATE:WRA2', 'GATE:IA', 'PUMPING:WRA2', 'PUMPING:TPE', 'FLOODING:WRA', 'FLOODING:WRA2'] The followings are valid project codes for water resource data:\nWRA: Water Resource Agency WRA2: Water Resource Agency（co-constructed with county and city governments） IA: Irrigation Agency CPAMI: Construction and Planning Agency TPE: Taipei City Get all stations: Water().get_station() wa = Water().get_station(src=\"WATER_LEVEL:WRA_RIVER\") wa[0] { 'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None} } Get data of a station: Water().get_data() wa = Water().get_data(src=\"WATER_LEVEL:WRA_RIVER\", stationID=\"01790145-cd7e-4498-9240-f0fcd9061df2\") wa [{'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'data': [{'name': '水位', 'description': ' Datastream_id=016e5ea0-7c7f-41a2-af41-eabacdbb613f, Datastream_FullName=延平.水位, Datastream_Description=現場觀測, Datastream_Category_type=河川水位站, Datastream_Category=水文', 'values': [{'timestamp': '2022-08-28T06:00:00.000Z', 'value': 157.41}]}], 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None}}] Earthquake Data Get all project codes: Quake().get_source() q = Quake().get_source() q ['EARTHQUAKE:CWB+NCREE'] The followings are valid project codes for earthquake data:\nEARTHQUAKE:CWB+NCREE: seismic monitoring stations by Central Weather Bureau and National Center for Research on Earthquake Engineering Get all stations: Quake().get_station() q = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") q[0:2] [{'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'properties': {'authority': '中央氣象局', 'stationID': 'EGC', 'deviceType': 'FBA', 'stationName': 'Jiqi'}, 'location': {'latitude': 23.708, 'longitude': 121.548, 'address': None}}, {'name': '地震監測站-Xilin-ESL', 'description': '地震監測站-Xilin-ESL', 'properties': {'authority': '中央氣象局', 'stationID': 'ESL', 'deviceType': 'FBA', 'stationName': 'Xilin'}, 'location': {'latitude': 23.812, 'longitude': 121.442, 'address': None}}] Get data of a station: Quake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\") q[-1] {'name': '第2022083號地震', 'description': '第2022083號地震', 'properties': {'depth': 43.6, 'authority': '中央氣象局', 'magnitude': 5.4}, 'data': [ { 'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18815)' },{ 'name': '地震監測站-Taichung City-TCU', 'description': '地震監測站-Taichung City-TCU', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18816)' }, ... ] 'location': {'latitude': 22.98, 'longitude': 121.37, 'address': None}} Get data of an earthquake eventQuake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\", eventID=\"2022083\") q Weather Data Get all project codes: Weather().get_source() w = Weather().get_source() w ['GENERAL:CWB', 'GENERAL:CWB_IoT', 'RAINFALL:CWB', 'RAINFALL:WRA', 'RAINFALL:WRA2', 'RAINFALL:IA', 'IMAGE:CWB'] Return the project names based on the input parameter:\nGENERAL: Return the names of weather station related projects RAINFALL: Return the names of rainfall monitoring station related projects IMAGE: Return the names of radar integrated echo map related projects (none): Return the names of all weather related projects The followings are valid project codes for weather data:\nGENERAL:CWB: standard weather stations by Central Weather Bureau GENERAL:CWB_IoT: automatic weather monitoring stations by Central Weather Bureau RAINFALL:CWB: rainfall monitoring stations by Central Weather Bureau RAINFALL:WRA: rainfall monitoring stations by Water Resource Agency RAINFALL:WRA2: rainfall monitoring stations by Water Resource Agency（co-constructed with county and city governments） RAINFALL:IA: rainfall monitoring stations by Irrigation Agency IMAGE:CWB: radar integrated echo map by Central Weather Bureau Get all stations: Weather().get_station() w = Weather().get_station(src=\"RAINFALL:CWB\") w [{'name': '雨量站-C1R120-上德文', 'description': '雨量站-C1R120-上德文', 'properties': {'city': '屏東縣', 'township': '三地門鄉', 'authority': '中央氣象局', 'stationID': 'C1R120', 'stationName': '上德文', 'stationType': '局屬無人測站'}, 'location': {'latitude': 22.765, 'longitude': 120.6964, 'address': None}}, ... ] Get data of a station: Weather().get_data() w = Weather().get_data(src=\"RAINFALL:CWB\", stationID=\"U2HA40\") w [{'name': '雨量站-U2HA40-臺大內茅埔', 'description': '雨量站-U2HA40-臺大內茅埔', 'properties': {'city': '南投縣', 'township': '信義鄉', 'authority': '中央氣象局', 'stationID': 'U2HA40', 'stationName': '臺大內茅埔', 'stationType': '中央氣象局'}, 'data': [{'name': 'HOUR_12', 'description': '12小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'MIN_10', 'description': '10分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'RAIN', 'description': '60分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_6', 'description': '6小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_3', 'description': '3小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_24', 'description': '24小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'NOW', 'description': '本日累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'ELEV', 'description': '高度', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 507.0}]}], 'location': {'latitude': 23.6915, 'longitude': 120.843, 'address': None}}] CCTV Data Get all project codes: CCTV().get_source() cctv = CCTV().get_source() cctv ['IMAGE:EPA', 'IMAGE:WRA', 'IMAGE:COA'] The followings are valid project codes for CCTV data:\nIMAGE:EPA: realtime images by EPA air quality monitoring stations IMAGE:WRA: images for water conservancy and disaster prevention by Water Resource Agency IMAGE:COA: realtime images of landslide observation stations by Council of Agriculture Get data of a station: CCTV().get_data() cEPA = CCTV().get_data(\"IMAGE:EPA\") cEPA[2] { 'name': '環保署空品監測即時影像器-萬里', 'description': '環保署-萬里-空品監測即時影像器', 'properties': { 'city': '新北市', 'basin': '北部空品區', 'authority': '環保署', 'stationName': '萬里', 'CCDIdentifier': '3'}, 'data': [ { 'name': '即時影像', 'description': '環保署-萬里-空品監測即時影像器', 'values': [ { 'timestamp': '2022-06-13T09:00:00.000Z', 'value': 'https://airtw.epa.gov.tw/AirSitePic/20220613/003-202206131700.jpg' } ] } ], 'location': {'latitude': 25.179667, 'longitude': 121.689881, 'address': None} } cCOA = CCTV().get_data(\"IMAGE:COA\") cCOA[0] {'name': '行政院農委會土石流觀測站影像-大粗坑下游攝影機', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'properties': {'city': '新北市', 'township': '瑞芳鎮', 'StationId': '7', 'authority': '行政院農委會', 'stationName': '大粗坑下游攝影機', 'CCDIdentifier': '2'}, 'data': [{'name': '即時影像', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'values': [{'timestamp': '2099-12-31T00:00:00.000Z', 'value': 'http://dfm.swcb.gov.tw/debrisFinal/ShowCCDImg-LG.asp?StationID=7\u0026CCDId=2'}]}], 'location': {'latitude': 25.090878, 'longitude': 121.837815, 'address': '新北市瑞芳鎮弓橋里大粗坑'}} Disaster Alert and Notification Data The Civil IoT Taiwan Data Service Platform provides more than 58 disaster alerts, with a total of more than 41 disaster notifications. Different units are responsible for different disaster alerts, and the central disaster response center is responsible for disaster notification.\nThe complete list of project codes is available in the pyCIOT Package Document。\nGet disaster alerts: Disaster().get_alert() d = Disaster().get_alert(\"5\") d {'id': 'https://alerts.ncdr.nat.gov.tw/Json.aspx', 'title': 'NCDR_CAP-即時防災資訊(Json)', 'updated': '2021-10-12T08:32:00+08:00', 'author': {'name': 'NCDR'}, 'link': {'@rel': 'self', '@href': 'https://alerts.ncdr.nat.gov.tw/JSONAtomFeed.ashx?AlertType=5'}, 'entry': [ { 'id': 'CWB-Weather_typhoon-warning_202110102030001', 'title': '颱風', 'updated': '2021-10-10T20:30:05+08:00', 'author': {'name': '中央氣象局'}, 'link': {'@rel': 'alternate', '@href': 'https://b-alertsline.cdn.hinet.net/Capstorage/CWB/2021/Typhoon_warnings/fifows_typhoon-warning_202110102030.cap'}, 'summary': { '@type': 'html', '#text': '1SEA18KOMPASU圓規2021-10-10T12:00:00+00:0018.20,126.702330992150輕度颱風TROPICAL STORM2021-10-11T12:00:00+00:0019.20,121.902835980220輕度颱風 圓規（國際命名 KOMPASU）10日20時的中心位置在北緯 18.2 度，東經 126.7 度，即在鵝鑾鼻的東南東方約 730 公里之海面上。中心氣壓 992 百帕，近中心最大風速每秒 23 公尺（約每小時 83 公里），相當於 9 級風，瞬間最大陣風每秒 30 公尺（約每小時 108 公里），相當於 11 級風，七級風暴風半徑 150 公里，十級風暴風半徑 – 公里。以每小時21公里速度，向西進行，預測11日20時的中心位置在北緯 19.2 度，東經 121.9 度，即在鵝鑾鼻的南南東方約 320 公里之海面上。根據最新資料顯示，第18號颱風中心目前在鵝鑾鼻東南東方海面，向西移動，其暴風圈正逐漸向巴士海峽接近，對巴士海峽將構成威脅。巴士海峽航行及作業船隻應嚴加戒備。第18號颱風外圍環流影響，易有短延時強降雨，今(10日)晚至明(11)日基隆北海岸、宜蘭地區及新北山區有局部大雨發生的機率，請注意。＊第18號颱風及其外圍環流影響，今(10日)晚至明(11)日巴士海峽及臺灣附近各海面風浪逐漸增大，基隆北海岸、東半部（含蘭嶼、綠島）、西南部、恆春半島沿海易有長浪發生，前往海邊活動請特別注意安全。＊第18號颱風外圍環流影響，今(10日)晚至明(11)日臺南以北、東半部(含蘭嶼、綠島)、恆春半島、澎湖、金門、馬祖沿海及空曠地區將有9至12級強陣風，內陸地區及其他沿海空曠地區亦有較強陣風，請注意。＊第18號颱風外圍環流沉降影響，明(11)日南投、彰化至臺南及金門地區高溫炎熱，局部地區有36度以上高溫發生的機率，請注意。＊本警報單之颱風半徑為平均半徑，第18號颱風之7級風暴風半徑西南象限較小約60公里，其他象限約180公里，平均半徑約為150公里。'}, 'category': {'@term': '颱風'} },{ ... }, ... ] } Get historical data of disaster notifications: Disaster().get_notice() d = Disaster().get_notice(\"ERA2_F1\") # 交通災情通報表（道路、橋梁部分） d \"maincmt\":{ \"prj_no\":\"專案代號\", \"org_name\":\"填報機關\", \"rpt_approval\":\"核定人\", \"rpt_phone\":\"聯絡電話\", \"rpt_mobile_phone\":\"行動電話\", \"rpt_no\":\"通報別\", \"rpt_user\":\"通報人\", \"rpt_time\":\"通報時間\" }, \"main\":{ \"prj_no\":\"2014224301\", \"org_name\":\"交通部公路總局\", ... }, \"detailcmt\":{ \"trfstatus\":\"狀態\", ... }, ... } References Python pyCIOT pypi package (https://pypi.org/project/pyCIOT/) Python pyCIOT Document (https://hackmd.io/@cclljj/pyCIOT_doc) ",
    "description": "We introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.",
    "tags": [
      "Python",
      "API",
      "Water",
      "Air",
      "Quake",
      "Disaster"
    ],
    "title": "3.1. Basic Data Access Methods",
    "uri": "/en/ch3/ch3.1/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Visualization Data Resample Moving Average Multi-line Charts Calendar Heatmap Data Quality Inspection Outlier Detection Change Point Detection) Missing Data Handling Data Decomposition References Time series data is data formed in the order of appearance in time. Usually, the time interval in the data will be the same (for example, one data every five minutes, or one data per hour), and the application fields are quite wide, such as financial information, space engineering, signal processing, etc. There are also many statistical related tools that can used in analysis. In addition, time series data is very close to everyday life. For example, with the intensification of global climate change, the global average temperature has become higher and higher in recent years, and the summer is unbearably hot. Also, certain seasons of the year tend to have particularly poor air quality, or certain times of the year tend to have worse air quality than others. If you want to know more about these changes in living environment, and how the corresponding sensor values change, you will need to use time series data analysis, which is to observe the relationship between data and time, and then get the results. This chapter will demonstrate using three types of data (air quality, water resources, weather) in the Civil IoT Taiwan Data Service Platform.\nGoal observe time series data using visualization tools check and process time series data decompose time series data to investigate its trend and seasonality Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, seaborn, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and calplot, which need to be installed by :\n!pip install --upgrade pip # Kats !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # calplot !pip install calplot After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import calplot import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import statsmodels.api as sm import os, zipfile from datetime import datetime, timedelta from dateutil import parser as datetime_parser from statsmodels.tsa.stattools import adfuller, kpss from statsmodels.tsa.seasonal import seasonal_decompose from kats.detectors.outlier import OutlierDetector from kats.detectors.cusum_detection import CUSUMDetector from kats.consts import TimeSeriesData, TimeSeriesIterator from IPython.core.pylabtools import figsize Data Access We use pandas for data processing. Pandas is a data science suite commonly used in Python language. It can also be thought of as a spreadsheet similar to Microsoft Excel in a programming language, and its Dataframe object provided by pandas can be thought of as a two-dimensional data structure. The dimensional data structure can store data in rows and columns, which is convenient for various data processing and operations.\nThe topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Visualization The first step in the processing of time series data is nothing more than to present the information one by one in chronological order so that users can see the changes in the overall data and derive more ideas and concepts for data analysis. Among them, using line charts to display data is the most commonly used data visualization method. For example, take the air quality data as an example:\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Data Resample As can be seen from the air quality data sequence diagram in the above figure, the distribution of data is actually very dense, and the changes in data values are sometimes small and violent. This is because the current sampling frequency of air quality data is about once every five minutes, and the collected environment is the surrounding environment information in life, so data density and fluctuation are inevitable. Because sampling every 5 minutes is too frequent, it is difficult to show general trends in ambient air pollution. Therefore, we adopt the method of resampling to calculate the average value of the data in a fixed time interval, so as to present the data of different time scales. For example, we use the following syntax to resample at a larger scale (hour, day, month) sampling rate according to the characteristics of the existing air quality data:\nair_hour = air.resample('H').mean() # hourly average air_day = air.resample('D').mean() # daily average air_month = air.resample('M').mean() # monthly average print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 Then we plot again with the hourly averaged resampling data, and we can see that the curve becomes clearer, but the fluctuation of the curve is still very large.\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air_hour[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Moving Average For the chart of the original data, if you want to see a smoother change trend of the curve, you can apply the moving average method. The idea is to set a sampling window on the time axis of the original data, move the position of the sampling window smoothly, and calculate the average value of all values in the sampling window.\nFor example, if the size of the sampling window is 10, it means that the current data and the previous 9 data are averaged each time. After such processing, the meaning of each data is not only a certain time point, but the average value of the original time point and the previous time point. This removes abrupt changes and makes the overall curve smoother, thereby making it easier to observe overall trends.\n# plt.figure(figsize=(15, 10), dpi=60) MA = air_hour MA10 = MA.rolling(window=500, min_periods=1).mean() MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # MA10.plot(figsize(15, 10)) The blue line in the above figure is the original data, and the orange line is the curve after moving average. It can be clearly found that the orange line can better represent the change trend of the overall value, and there is also a certain degree of regular fluctuation, which is worthy of further analysis.\nMulti-line Charts In addition to presenting the original data in the form of a simple line chart, another common data visualization method is to cut the data into several continuous segments periodically in the time dimension, draw line charts separately, and superimpose them on the same multi-line chart. For example, we can cut the above air quality data into four sub-data sets of 2019, 2020, 2021, and 2022 according to different years, and draw their respective line charts on the same multi-line chart, as shown in the figure below.\nair_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] years = air_month['year'].unique() print(air) np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) plt.show() In this multi-line chart, we can see that the 2019 data has a significant portion of missing values, while the 2022 data was only recorded till July at the time of writing. At the same time, it can be found that in the four-year line chart, the curves of different years all reached the lowest point in summer, began to rise in autumn, and reached the highest point in winter, showing roughly the same trend of change.\nCalendar Heatmap The calendar heat map is a data visualization method that combines the calendar map and the heat map, which can more intuitively browse the distribution of the data and find the regularity of different time scales. We use calplot, a calendar heatmap suite for Python, and input the daily PM2.5 averages. Then we select the specified color (the parameter name is cmap, and we set it to GnBu in the following example. for detailed color options, please refer to the reference materials), and we can get the effect of the following figure, in which blue represents the greater value, green or white means lower values, if not colored or a value of 0 means there is no data for the day. From the resulting plot, we can see that the months in the middle part (summer) are lighter and the months in the left part (winter) are darker, just in line with our previous observations using the multi-line chart.\n# cmap: color map (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: specify the format of the text pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") Data Quality Inspection After the basic visualization of time series data, we will introduce the basic detection and processing methods of data quality. We will use kats, a Python language data processing and analysis suite, to perform outlier detection, change point detection, and handling missing values in sequence.\nOutlier Detection Outliers are those values in the data that are significantly different from other values. These differences may affect our judgment and analysis of the data. Therefore, outliers need to be identified and then flagged, removed, or treated specially. .\nWe first convert the data stored in the variable air_hour from its original dataframe format to the TimeSeriesData format used by the kats package and save the converted data into a variable named air_ts. Then we re-plot the line chart of the time series data.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') air_ts.plot(cols=[\"PM25\"]) We then used the OutlierDetector tool in the kats suite to detect outliers in the time series data, where outliers were less than 1.5 times the first quartile (Q1) minus the interquartile range (IQR) or greater than The third quartile (Q3) value plus 1.5 times the interquartile range.\noutlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] Finally, we delete the detected outliers from the original data, and re-plot the chart to compare it with the original one. We can clearly find some outliers (for example, there is an abnormal peak in 2022-07) have been removed.\noutliers_removed = outlierDetection.remover(interpolate=False) outliers_removed outliers_removed.plot(cols=['y_0']) Change Point Detection) A change point is a point in time at which the data suddenly changes significantly, representing the occurrence of an event, a change in the state of the data, or a change in the distribution of the data. Therefore, change point detection is often regarded as an important preprocessing step for data analysis and data prediction.\nIn the following example, we use daily averages of air quality data for change point detection. We use the TimeSeriesData data format of the kats package to store the data and use the CUSUMDetector detector provided by kats for detection. We use red dots to represent detected change points in the plot. Unfortunately, in this example, no obvious point of change was observed. Readers are advised to refer to this example and bring in other data for more exercise and testing.\nair_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') detector = CUSUMDetector(air_ts) change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # print(\"The change point is on\", change_points[0][0].start_time) # plot the results plt.xticks(rotation=45) detector.plot(change_points) plt.show() Missing Data Handling Missing data is a common problem when conducting big data analysis. Some of these missing values are already missing at the time of data collection (such as sensor failure, network disconnection, etc.), and some are eliminated during data preprocessing (outliers or abnormality). However, for subsequent data processing and analysis, we often need the data to maintain a fixed sampling rate to facilitate the application of various methods and tools. Therefore, various methods for imputing missing data have been derived. Below we introduce three commonly used methods:\nMark missing data as Nan (Not a number): Nan stands for not a number and is used to represent undefined or unrepresentable values. If it is known that subsequent data analysis will additionally deal with these special cases of Nan, this method can be adopted to maintain the authenticity of the information. Forward filling method: If Nan has difficulty in subsequent data analysis, missing values must be filled with appropriate numerical data. The easiest way to do this is forward fill, which uses the previous value to fill in the current missing value. # forward fill df_ffill = air.ffill(inplace=False) df_ffill.plot() 3. \u0006K-Nearest Neighbor (KNN) method: As the name suggests, the KNN method finds the k values that are closest to the missing value, and then fills the missing value with the average of these k values.\ndef knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): n_by_2 = np.ceil(n/2) lower = np.max([0, int(i-n_by_2)]) upper = np.min([len(ts)+1, int(i+n_by_2)]) ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) out[i] = np.nanmean(ts_near) return out # KNN df_knn = air.copy() df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) df_knn.plot() Data Decomposition In the previous example of basic data processing, we have been able to roughly observe the changing trend of data values and discover potential regular changes. In order to further explore the regularity of time series data changes, we introduce the data decomposition method. In this way, the original time series data can be disassembled into trend waves (trend), periodic waves (seasonal) and residual waves (residual).\nWe first replicated the daily average data of air quality data as air_process and processed the missing data using forward filling. Then, we first presented the raw data directly in the form of a line chart.\nair_process = air_day.copy() # new.round(1).head(12) air_process.ffill(inplace=True) air_process.plot() Then we use the seasonal_decompose method to decompose the air_process data, in which we need to set a period parameter, which refers to the period in which the data is decomposed. We first set it to 30 days, and then after execution, it will produce four pictures in sequence: raw data, trend chart, seasonal chart, and residual chart.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) decompose.plot().set_size_inches((15, 15)) plt.show() In the trend graph (trend), we can also find that the graph of the original data has very similar characteristics, with higher values around January and lower values around July; while in the seasonal graph (seasonal), we can find that the data has a fixed periodic change in each cycle (30 days), which means that the air quality data has a one-month change.\nIf we change the periodic variable to 365, i.e. decompose the data on a larger time scale (one year), we can see a trend of higher values around January and lower values around July from the seasonal plot, and this trend change occurs on a regular and regular basis. At the same time, it can be seen from the trend chart that the overall trend is slowing down, indicating that the concentration of PM2.5 is gradually decreasing under the overall trend. The results also confirmed our previous findings that no change points were detected, as the change trend of PM2.5 was a steady decline without abrupt changes.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) decompose.plot().set_size_inches((15, 15)) plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "We use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.1. Time Series Data Processing",
    "uri": "/en/ch4/ch4.1/"
  },
  {
    "content": "\nTable Of Contents Intersect Buffer Multi-ring buffer Distance matrix Brief Summary In the past, if we’re curious about geospatial phenomena such as air quality, earthquakes or floods, we rely on information provided by public sectors or experts to grasp the spatial domain and intensity of phenomena.\nSuppose that we want to see the air quality in neighborhoods of our house, one important information source would be the Taiwan Air Quality Monitoring Network run by Environmental Protection Administration Executive Yuan. However, due to the high cost of implementation and small quantity of advanced meteorological stations, the nearest may actually be 10 kilometer far from where we are, which makes us doubt: is the air quality homogeneous within 10 kilometer? On the other hand, since it’s not so costly to implement microsensors, so data provided by microsensors of IoT could be closer to our living spaces, allowing us to understand how air quality may be influenced by schools, intersections and temples near our houses, or even our mothers’ cooking. So, for the first step, how can we find out the sensor stations that meet our needs and further use the data?\nEach station of IoT has a corresponding spatial placement. For stations that are more adjacent than others, their sensing values may share common trends because of the similar environmental factors surrounding them—this is the first law of geography: “All things are related, but nearby things are more related than distant things.” (Waldo R. Tobler)\nBesides, interfering factors surrounding individual stations may affect sensing values and lead to bigger fluctuations. Therefore, to ensure the data reliability, we need to set individual stations as centers, selecting ID of nearby stations and their sensing values according to administrative regions to which the center station belongs or specific distance (radius), and finally represent the data in forms of sheets or maps.\nIn this chapter, we’ll practice selections of spatial information with data from the air quality monitoring stations (Environmental Protection Administration, EPA), weather stations (Central Weather Bureau, CWB) and flood sensors (Water Resources Agency, WRA).\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # downlaod the county boundary shpfile from open database !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # get flood sensors' data by pyCIOT wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") wea_list = CIoT.Weather().get_station('GENERAL:CWB') county = gpd.read_file('county.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] # convert to geopandas.GeoDataFrame flood_list = wa + wa2 flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") weather_df = pd.DataFrame([],columns = ['name','lon', 'lat']) for i in wea_list: #print(i['data'][0]) df = pd.DataFrame([[i['name'],i['location']['longitude'],i['location']['latitude']]],columns = ['name','lon', 'lat']) weather_df = pd.concat([weather_df,df]) #print(df) result_df = weather_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) gdf_weather = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") Intersect Generally speaking, we can take administrative boundaries such as villages or towns as scopes, obtaining station IDs within one administrative region with the intersection of data, and we can use API to attain instantaneous values, hourly averages, daily averages and weekly averages of these stations. Also, we’re able to inspect if stations in the same administrative region share similar value trends, or to see if specific stations provide values significantly different from others.\nimport matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=gdf_weather) # this is plotting the datapoints from the EPA dataframe basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # plotting the city's boundaries here, with facecolor = none to # remove the polygon's fill color plt.tight_layout(); # selecting the polygon's geometry field to filter out points that basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_weather, basemap, how='intersection') fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=intersected_data) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Buffer Additionally, since parts of stations may be located in boundaries between two regions, the result may be biased if we take administrative boundaries as selection criteria. In this case, with the concept of buffering, we set the coordinates of stations as the center, assigning a distance radius to draw a virtual circle, in which we locate the stations that have buffers.\nOnce we have the concept of buffering, we can also set certain landmarks (such as schools, parks or factories, etc) as centers to find nearby stations. Furthermore, with a line (roads or rivers, for example) or a polygon (parks or industrial districts, for example) as the center, we can build up a searching scope to find out the station we want more specifically.\n# set the buffer distance for 0.05 degree fig, ax = plt.subplots(figsize=(6, 10)) buffer = intersected_data.buffer(0.05) buffer.plot(ax=ax, alpha=0.5) intersected_data.plot(ax=ax, color='red', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Multi-ring buffer We can surely set different distances as radius to create buffers of concentric circles, with which we can group nearby stations according to different distances/classes, seeing if nearby stations share value trends that are more similar.\n# set the buffer distance 0.05 degree=blue；0.1 degree=green；0.2 degree=orange；0.3 degree=red fig, ax = plt.subplots(figsize=(6, 10)) buffer_03 = intersected_data.buffer(0.3) buffer_03.plot(ax=ax, color='red', alpha=1) buffer_02 = intersected_data.buffer(0.2) buffer_02.plot(ax=ax, color='orange', alpha=1) buffer_01 = intersected_data.buffer(0.1) buffer_01.plot(ax=ax, color='green', alpha=1) buffer_005 = intersected_data.buffer(0.05) buffer_005.plot(ax=ax, alpha=1) intersected_data.plot(ax=ax, color='black', alpha=0.5) # intersect with the flood sensors buffer = gpd.GeoDataFrame(buffer_03,geometry=buffer_03) buffer = buffer.to_crs(4326) intersected_flood = gpd.overlay(gdf_flood, buffer, how='intersection') intersected_flood.plot(ax=ax, color='lightgray', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Distance matrix Since each observation station is assigned specific position coordinates, we can thus acquire the absolute distance between two stations by combining trigonometric functions with coordination values. Therefore, we can construct the distance matrix between all stations, which helps us quickly examine the distance relation between two stations and ensure if geographic proximity exists within (graph 3). In this example, we can transform the locations of flooding sensors in Taipei to distance matrices, which allows us to ensure if proximity exists in flood events.\ngdf_weather[\"ID\"] = gdf_weather.index df = pd.DataFrame(gdf_weather, columns=['ID','lon', 'lat']) df.set_index('ID') # convert to the dustance matrix from scipy.spatial import distance_matrix pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index) Brief Summary With methods explained above, we can create a station selection mechanism with concepts of administrative regions or topology, and examine the correlations between sensing values by analyzing geospatial features of stations.\nReferences Geopanda documentation (https://geopandas.org/en/stable/docs.html) Introduction to the shp file (https://en.wikipedia.org/wiki/Shapefile) Taiwan’s Datum and coordinate system (https://wiki.osgeo.org/wiki/Taiwan_datums) Introduction to WGS 84 (EPSG:4326) coordinate system (https://epsg.io/4326) Introduction to TWD 97 (EPSG:3826) coordinate system (https://epsg.io/3826) ",
    "description": "We use Civil IoT Taiwan's earthquake and disaster prevention and relief data and filter the data of specific administrative areas by overlaying the administrative area boundary map obtained from the government open data platform. Then, we generate the image file of the data distribution location after the superimposed map. In addition, we also demonstrate how to Nest specific geometric topological regions and output nested results to files for drawing operations.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "5.1. Geospatial Filtering",
    "uri": "/en/ch5/ch5.1/"
  },
  {
    "content": "\nTable Of Contents Preamble Classification Clustering Package Installation and Importing Case 1: Location Type Classification of Air Quality Sensors Data Cleansing Outlier removal Train data and test data Using the pre-built Sklearn models Case 2: Clustering of Air Quality Sensors Data download and preprocessing Dynamic Time Warping (DTW) K-Mean clustering Correlation between data clusters and geographical locations Correlation between data clusters and wind directions Case 3: Clustering and Classification Application of Water and Meteorological Data Data download and preprocessing Correlation of flood sensors and rain gauges Data clustering to find highly correlated rain gauges Data classification to predict flooding by rainfall data References Preamble In the previous chapters, we have introduced the rich open data content of Civil IoT Taiwan. At the same time, we have presented various data analyses and processing from the perspective of time and space dimensions. In this chapter, we will further explore applications of machine learning and introduces two classic machine learning problems, namely the classification problem and the clustering problem.\nClassification Classification problems are classic problems in machine learning theory. If we describe this problem more mathematically, we can assume that a set of data X has been classified, and the label set Y is obtained after each data is classified. The classification problem is constructing an effective classifier through this set of classified data and labels, which can find the corresponding label Y' for each piece of data from the unclassified data X'.\nTherefore, the focus of classification problems is to construct an efficient classifier. We will first build a model and use the labeled data for training to achieve this goal. Our goal is to make the model as close as possible to fit the distribution of these data and then use the final product model as a classifier to infer labels for unknown data.\nThis process of building a classifier is called supervised learning in machine learning. Standard classifier models include the Nearest Neighbors, SVM Classifier, Decision Tree, Random Forest, etc. In our later articles, we will not explain each model in depth but will only use these models directly as tools. Readers interested in these models can refer to relevant resources and do more in-depth exploration.\nClustering Clustering problems are very similar to classification problems. The main difference is that the classification problem uses labeled known data to infer unknown data, while clustering problems are entirely “out of thin air,” forming data into different groupings.\nIf we describe this problem more mathematically, we can assume that there is a set of entirely unlabeled data X. The clustering problem is to divide the data of X into k groups through a particular algorithm. The data in each data group has High similarity, while data within different groups are highly diverse.\nThe clustering algorithms are mainly based on the characteristics of the data and constantly judge the similarity and dissimilarities between the data. Then, they let similar data gathered together and made the different data mutually exclusive in the distribution. Standard clustering algorithms include K-Means, DBSCAN, Hierarchical Clustering, BIRCH, etc. In our later articles, we will not explain each model in depth but will only use these models directly as tools. Readers interested in these models can refer to relevant resources and explore them more in-depth.\nPackage Installation and Importing In this article, we will use the andas, numpy, matplotlib, json, os, glob, math, seaborn, tqdm, datetime, geopy, scipy, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use three additional packages that Colab does not have pre-installed: pyCIOT, fastdtw and sklearn, as well as the TaipeiSansTCBeta-Regular font, which need to be installed by :\n!pip3 install fastdtw --quiet !pip3 install scikit-learn --quiet !pip3 install pyCIOT --quiet !wget -q -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\u0026export=download After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nfrom pyCIOT.data import * import json, os, glob, math import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib.font_manager import fontManager from tqdm import tqdm_notebook as tqdm from datetime import datetime, timedelta import seaborn as sns sns.set(font_scale=0.8) fontManager.addfont('TaipeiSansTCBeta-Regular.ttf') mpl.rc('font', family='Taipei Sans TC Beta') import warnings warnings.simplefilter(action='ignore') import geopy.distance from scipy.spatial.distance import euclidean from fastdtw import fastdtw import sklearn from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.cluster import KMeans Next, we use the water level and air quality data of Civil IoT Taiwan to introduce data classification and clustering.\nCase 1: Location Type Classification of Air Quality Sensors In this case, we use the data of the Environmental Protection Agency’s micro air quality sensors (‘OBS:EPA_IoT’) in Civil IoT Taiwan to demonstrate data classification.\nWe first use the Air().get_data() method provided by pyCIOT to download the latest sensing data of all EPA micro air quality sensors. Note that this step may take a little longer due to a large amount of content.\nSource = 'OBS:EPA_IoT' Data = Air().get_data(src=Source) Data = [datapoint for datapoint in Data if len(datapoint['data']) == 3 and 'areaType' in datapoint['properties'].keys()] print(json.dumps(Data[0], indent=4, ensure_ascii=False)) { \"name\": \"智慧城鄉空品微型感測器-10287974676\", \"description\": \"智慧城鄉空品微型感測器-10287974676\", \"properties\": { \"city\": \"新北市\", \"areaType\": \"社區\", \"isMobile\": \"false\", \"township\": \"鶯歌區\", \"authority\": \"行政院環境保護署\", \"isDisplay\": \"true\", \"isOutdoor\": \"true\", \"stationID\": \"10287974676\", \"locationId\": \"TW040203A0507221\", \"Description\": \"廣域SAQ-210\", \"areaDescription\": \"鶯歌區\" }, \"data\": [ { \"name\": \"Relative humidity\", \"description\": \"相對溼度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 70.77 }, { \"name\": \"Temperature\", \"description\": \"溫度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 33.78 }, { \"name\": \"PM2.5\", \"description\": \"細懸浮微粒 PM2.5\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 9.09 } ], \"location\": { \"latitude\": 24.9507, \"longitude\": 121.3408416, \"address\": null } } We can find that in the data of each sensor, there is information such as temperature, relative humidity, and PM2.5 concentration. At the same time, the basic knowledge of the sensor is also recorded, such as city, town, machine number, location number, area type, etc. In our example, we will use “location type” as label data and train a classifier using sensory data (temperature, relative humidity, and PM2.5 concentration). We first observe the content state of the current “AreaType.”\nLabel = list(dict.fromkeys([datapoint['properties']['areaType'] for datapoint in Data if datapoint['properties']['areaType']])) count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"Before data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) Before data cleaning, There are 8620 records. { \"社區\": 223, \"交通\": 190, \"一般社區\": 2021, \"工業\": 12, \"測站比對\": 66, \"工業區\": 3333, \"交通區\": 683, \"鄰近工業區社區\": 948, \"輔助區\": 165, \"特殊區(民眾陳情熱區)\": 143, \"特殊區(敏感族群聚集區)\": 200, \"特殊區(測站比對)\": 32, \"輔助區(無測站區)\": 4, \"工業感測\": 196, \"特殊感測\": 4, \"輔助感測\": 295, \"交通感測\": 102, \"機動感測\": 2, \"社區感測\": 1 } Data Cleansing Since there are currently 8,620 records scattered in 19 types of places, we first merge similar types of area types to conform to the meaning of the data. Then, for simplicity, we only focus on four major area types: general communities, transportation areas, industrial areas, and industrial-adjacent communities. We rearrange the data using the following code:\nfor datapoint in Data: if datapoint['properties']['areaType'] == '社區': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '社區感測': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '交通': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '交通感測': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '工業': datapoint['properties']['areaType'] = '工業區' elif datapoint['properties']['areaType'] == '工業感測': datapoint['properties']['areaType'] = '工業區' if not datapoint['properties']['areaType'] in ['一般社區', '交通區', '工業區', '鄰近工業區社區']: datapoint['properties']['areaType'] = None Data = [datapoint for datapoint in Data if datapoint['properties']['areaType'] != None] Label = ['一般社區', '交通區', '工業區', '鄰近工業區社區'] count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"After data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) After data cleaning, There are 7709 records. { \"一般社區\": 2245, \"交通區\": 975, \"工業區\": 3541, \"鄰近工業區社區\": 948 } After data cleansing, 7,709 records were left, distributed in four major areas. For these records, we consider each data’s temperature, relative humidity, and PM2.5 concentration and use different colors in the three-dimensional data distribution map to represent the data of different types of areas.\nDataX, DataY = [], [] for datapoint in Data: TmpX = [None]*3 TmpY = None for rawdata_array in datapoint['data']: if(rawdata_array['name'] == 'Temperature'): TmpX[0] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'Relative humidity'): TmpX[1] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'PM2.5'): TmpX[2] = rawdata_array['values'][0].get('value') TmpY = Label.index(datapoint['properties']['areaType']) DataX.append(TmpX) DataY.append(TmpY) DataX_Numpy = np.array(DataX) DataY_Numpy = np.array(DataY) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature(℃)') ax.set_ylabel('Relative Humidity(%)') ax.set_zlabel('PM2.5(μg/$m^{3}$)') plt.show() The data distribution diagram shows that most of the data are gathered in a specific space, but a few are scattered in a very peripheral place. These data that are far from the group are called outliers. For data classification or clustering, outliers can easily lead our model or algorithm to extremes, thus losing its versatility, so we need to remove these data first.\nOutlier removal The method of removing outliers is nothing more than using the statistical characteristics of the data, which can be defined according to the needs of different application scenarios. In our example, we define one of the sensor data as an outlier if its value deviates from the mean by more than two standard deviations. After removing the data of these outliers, we redraw the three-dimensional distribution map to observe its distribution.\ndef Outlier_Filter(arr, k): Boolean_Arr = np.ones(arr.shape[0], dtype=bool) for i in range(arr.shape[1]): Boolean_Arr = Boolean_Arr \u0026 (abs(arr[:,i] - np.mean(arr[:,i])) \u003c k*np.std(arr[:,i])) return Boolean_Arr OutlierFilter = Outlier_Filter(DataX_Numpy, 2) DataX_Numpy = DataX_Numpy[OutlierFilter] DataY_Numpy = DataY_Numpy[OutlierFilter] print(\"After removing Outliers, there are {} records left.\".format(DataX_Numpy.shape[0])) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature(℃)') ax.set_ylabel('Relative Humidity(%)') ax.set_zlabel('PM2.5(μg/$m^{3}$)') plt.show() After removing Outliers, there are 7161 records left. Judging from the final results, we have removed 7,709 - 7,161 = 548 outlier records in total. The distribution of the remaining data in the three-dimensional space is relatively concentrated without deviations in the periphery. For ease of observation, we plot the distribution of the three data in different dimensions by selecting two dimensions at a time.\nplt.rc('legend',fontsize=\"large\") fig, axes = plt.subplots(1,3,figsize=(24, 6)) for i in range(DataX_Numpy.shape[1]): for j in range(len(Label)): axes[i].scatter(DataX_Numpy[DataY_Numpy==j][:,i%3],DataX_Numpy[DataY_Numpy==j][:,(i+1)%3], s=1, label=Label[j]) axes[i].legend(loc=2) Axis_label = ['Temperature(℃)', 'Relative Humidity(%)', 'PM2.5(μg/$m^{3}$)'] axes[i].set_xlabel(Axis_label[i%3]) axes[i].set_ylabel(Axis_label[(i+1)%3]) plt.tight_layout() From these three graphs, we can see a special relationship between the data of different colors (area types). Although it is challenging to describe directly, we will introduce a classification model to build a dedicated classifier.\nTrain data and test data Before entering the model training of the classifier, we have another step to deal with: splitting the existing data set into training and test data. As the name suggests, the training data will be used to tune the classifier’s model, while the test data will be used to test how well the built classifier works on new data. We use the following sample program to split the dataset into training and testing data at a ratio of 4:1.\nindices = np.random.permutation(DataX_Numpy.shape[0]) Train_idx, Test_idx = indices[:int(DataX_Numpy.shape[0]*0.8)], indices[80:(DataX_Numpy.shape[0] - int(DataX_Numpy.shape[0]*0.8))] TrainX, TestX = DataX_Numpy[Train_idx,:], DataX_Numpy[Test_idx,:] TrainY, TestY = DataY_Numpy[Train_idx], DataY_Numpy[Test_idx] Using the pre-built Sklearn models We use the classifier model provided by the Python package Scikit Learn (sklearn) for training and testing. We use nine models in a series of examples, including nearest neighbor, linear SVM, RBF SVM, decision tree, random forest, neural network, Adaboost, Naive Bayes, and QDA. We sequentially introduce training data for adjustment and then test data for prediction. We compare the test data with the label content in the predicted results and use a confusion matrix to present the classification results for different label combinations.\nclassifier_names = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(TrainX, TrainY) Result = model.predict(TestX) mat = confusion_matrix(TestY, Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=Label, yticklabels=Label, ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(classifier_names[i], round(accuracy_score(Result, TestY), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() Among the classification results of these nine classification models, we found that RBF SVM can achieve a classification success rate of nearly 70%. This is only the result of using the original data without further processing and analysis. If readers are interested in the classifier, you can refer to relevant resources for more in-depth exploration, further improving the classifier’s ability to classify different data types.\nCase 2: Clustering of Air Quality Sensors In this case, we use the sensing data of the Taiwan EPA air quality monitoring station in Civil IoT Taiwan and analyze its historical data. The relationship is grouped so that each group’s stations have similar sensory data trends.\nData download and preprocessing We use the following codes to download all the sensing data of the Taiwan EPA air quality monitoring stations in 2021 from the historical database of the Civil IoT Taiwan Data Service Platform and decompress the downloaded file into /content directory.\n!wget -O 'EPA_OD_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FnkrDkv53nvbJf5ZyL5a6256m65ZOB5ris56uZL0VQQV9PRF8yMDIxLnppcA%3D%3D\" !unzip -q 'EPA_OD_2021.zip' \u0026\u0026 rm 'EPA_OD_2021.zip' !unzip -q '/content/EPA_OD_2021/EPA_OD_202112.zip' -d '/content' !rm -rf '/content/EPA_OD_2021' We first select the data of December 2021 and delete the unnecessary fields Pollutant, SiteId, Status, SO2_AVG. Then we change the data type of the sensing data to a floating point number to facilitate subsequent processing.\nDataframe = pd.read_csv(\"/content/EPA_OD_202112.csv\", parse_dates=['PublishTime']) Dataframe = Dataframe.drop(columns=[\"Pollutant\", \"SiteId\", \"Status\", \"SO2_AVG\"]) Numerical_ColumnNames = list(Dataframe.columns.values) for ColumnName in ['SiteName', 'County', 'PublishTime']: Numerical_ColumnNames.remove(ColumnName) for Numerical_ColumnName in Numerical_ColumnNames: Dataframe[Numerical_ColumnName] = pd.to_numeric(Dataframe[Numerical_ColumnName], errors='coerce').astype('float64') Dataframe = Dataframe.dropna() Dataframe.head() Due to the massive amount of data in one month, to shorten the execution time of the sample program, we extract the data of the five days from 2021-12-13 to 2021-12-17 as FiveDay_Dataframe, as in the example below, and use the Country and SiteName fields to merge the data. We then sort the data by when it was published.\nFiveDay_Dataframe = Dataframe.loc[(Dataframe['PublishTime'] \u003c= '2021-12-17 23:00:00') \u0026 (Dataframe['PublishTime'] \u003e= '2021-12-13 00:00:00')] FiveDay_Dataframe['CountyAndSiteName'] = FiveDay_Dataframe['County'] + FiveDay_Dataframe['SiteName'] FiveDay_Dataframe = FiveDay_Dataframe.drop(columns=[\"County\", \"SiteName\"]) FiveDay_Dataframe = FiveDay_Dataframe.sort_values(by=['CountyAndSiteName','PublishTime']) FiveDay_Dataframe = FiveDay_Dataframe.set_index(keys = ['CountyAndSiteName']) FiveDay_Dataframe Dynamic Time Warping (DTW) Next, we judge the “similarity” between the two sites and quantify it into a number. The primary similarity measurement method is to directly align the data of two measuring stations according to the perception time and then calculate the gap between the two air quality measurements. However, since air pollution may occur in different orders between sites and the duration of its effects is not necessarily the same, more flexibility in estimating the similarity between two sites is needed. Therefore, we used the dynamic time wrapping (DTW) method to measure the similarity. The smaller the DTW distance between the two stations, the higher their similarity.\nSite_TimeSeriesData = dict() for Site in np.unique(FiveDay_Dataframe.index.values): tmp = FiveDay_Dataframe[FiveDay_Dataframe.index == Site] tmp = tmp.groupby(['CountyAndSiteName', 'PublishTime'], as_index=False).mean() tmp = tmp.loc[:,~tmp.columns.isin(['CountyAndSiteName', 'PublishTime'])] Site_TimeSeriesData[Site] = tmp.to_numpy() DictKeys = Site_TimeSeriesData.keys() Sites_DTW = dict() for i, key1 in enumerate(DictKeys): for j, key2 in enumerate(DictKeys): if i \u003e= j: continue else: Sites_DTW[str(key1)+\" \"+str(key2)] = fastdtw(Site_TimeSeriesData[key1][:,:-4], Site_TimeSeriesData[key2][:,:-4], dist=euclidean)[0] Sites_DTW_keys = np.array(list(Sites_DTW.keys())) Site_DTW_Numpy = np.array([[value] for _, value in Sites_DTW.items()]) We plot the DTW distance between all sites in the figure below, where the DTW distance is small to large. For further processing, we need to start analyzing with clustering algorithms.\nfig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) ax.scatter(Site_DTW_Numpy[:,0], [1]*len(Sites_DTW.items()), s=0.05) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') K-Mean clustering We use the K-Means module in the sklearn package for data clustering. Since the clustering algorithm needs to set the number of clusters to be generated in advance, we first set it to 3. We clustered using the following code and plotted the results with the cluster number as Y-axis and the DTW similarity value to other data as X-axis.\nfrom sklearn.cluster import KMeans model = KMeans(n_clusters=3, random_state=0).fit([[value] for _, value in Sites_DTW.items()]) Result = model.labels_ for i in np.unique(Result): print(\"Number of Cluster{} : {}\".format(i,len(Result[Result==i]))) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): ax.scatter(Site_DTW_Numpy[Result==i][:,0],[i]*len(Site_DTW_Numpy[Result==i]), s=0.05) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Number of Cluster0 : 1165 Number of Cluster1 : 994 Number of Cluster2 : 542 The clustering results show that the K-Means algorithm divides the original data into three clusters, with 1,165, 994, and 542 data, respectively. To further understand the causes of these three groups, we continue to trace each possible reason for the formation of a group.\nCorrelation between data clusters and geographical locations We first assume that the change in air quality is regional, so we explore whether the results of data clustering are related to the geographical location of air quality stations. We first retrieve the GPS coordinates of the stations and compute the physical distances of the two geographical locations. Then, according to the results of data clustering, we conduct a simple statistical analysis of the physical distances of different clusters and draw them in the picture below.\nDist_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Dist_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Dist_for_Cluster.append(geopy.distance.geodesic(coord1, coord2).km) Dist_for_Cluster = np.array(Dist_for_Cluster) Dist_for_Clusters[i] = Dist_for_Cluster Dist_for_Clusters = np.array(Dist_for_Clusters) # for Dist_for_Cluster in Dist_for_Clusters: # print(np.mean(Dist_for_Cluster)) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): gtMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003enp.mean(Dist_for_Clusters[i])] ltMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003cnp.mean(Dist_for_Clusters[i])] print(\"Mean Distance between Sites for Cluster{} : {}\".format(i, np.mean(Dist_for_Clusters[i]))) print(\"In Cluster{} there are {:.2%} less than mean, and {:.2%} greater than mean.\\n\".format(i, len(ltMean)/len(Dist_for_Clusters[i]), len(gtMean)/len(Dist_for_Clusters[i]))) ax.scatter(gtMean, [i]*len(gtMean), s=0.05, color=\"orange\") ax.scatter(ltMean, [i]*len(ltMean), s=0.05, color=\"pink\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=0.5) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Mean Distance between Sites for Cluster0 : 84.34126465234523 In Cluster0 there are 60.09% less than mean, and 39.91% greater than mean. Mean Distance between Sites for Cluster1 : 180.26230465399215 In Cluster1 there are 54.53% less than mean, and 45.47% greater than mean. Mean Distance between Sites for Cluster2 : 234.89206124762546 In Cluster2 there are 39.48% less than mean, and 60.52% greater than mean. The analysis results show that the group with a higher DTW value (lower time series similarity) has a more considerable average distance between stations and vice versa. The similarity of the station data is related to the difference in geographical location, which should also support our hypothesis, confirming that the dispersion of air pollutants is indeed affected by geographical distance.\nCorrelation between data clusters and wind directions We then assume that the change in air quality is affected by the environmental wind field, so we explore whether the results of data clustering are related to the wind direction where the air quality station is located. We first retrieve the GPS coordinates of the station and convert the azimuth relationship between the two geographical locations. Then, according to the data clustering results, we calculated the correlation between the geographical location of azimuth and the wind direction on site. Finally, we conduct a simple statistical analysis of the obtained values and draw them in the figure below.\ndef get_bearing(lat1, long1, lat2, long2): dLon = (long2 - long1) x = math.cos(math.radians(lat2)) * math.sin(math.radians(dLon)) y = math.cos(math.radians(lat1)) * math.sin(math.radians(lat2)) - math.sin(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.cos(math.radians(dLon)) brng = np.arctan2(x,y) brng = np.degrees(brng) return brng def Check_Wind_Dirc(brng, wind_dirc): if brng \u003e 180: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng - 180 \u003c wind_dirc + 45) and (brng - 180 \u003e wind_dirc - 45)) else: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng + 180 \u003c wind_dirc + 45) and (brng + 180 \u003e wind_dirc - 45)) Brng_for_Clusters = [None]*len(np.unique(Result)) Boolean_WindRelated_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Brng_for_Cluster = [] Boolean_WindRelated_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Brng_Between_Site = get_bearing(coord1[0], coord1[1], coord2[0], coord2[1]) Brng_for_Cluster.append(Brng_Between_Site) MeanWindDirc1 = np.mean(Site_TimeSeriesData[Site1][:,-3]) MeanWindDirc2 = np.mean(Site_TimeSeriesData[Site2][:,-3]) Boolean_WindRelated_for_Cluster.append(Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc1) or Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc2)) Brng_for_Cluster = np.array(Brng_for_Cluster) Boolean_WindRelated_for_Cluster = np.array(Boolean_WindRelated_for_Cluster) Boolean_WindRelated_for_Clusters[i] = Boolean_WindRelated_for_Cluster Brng_for_Clusters[i] = Brng_for_Cluster Brng_for_Clusters = np.array(Brng_for_Clusters) Boolean_WindRelated_for_Clusters = np.array(Boolean_WindRelated_for_Clusters) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): print(\"Relevance for Cluster{} : {:.2%}\".format(i, len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True])/len(Dist_for_Clusters[i]))) ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True]), s=2, color=\"green\") ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False]), s=0.05, color=\"violet\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=2) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Relevance for Cluster0 : 54.08% Relevance for Cluster1 : 39.24% Relevance for Cluster2 : 22.69% The analysis results show that the smaller the DTW value, the higher the correlation between the inter-station azimuth and the wind direction, and vice versa. It can be seen that the similarity of the site data is indeed related to the wind direction of the ambient wind field, which also confirms our hypothesis and confirms that the diffusion of air pollutants is certainly affected by the wind direction of the ambient wind field.\nCase 3: Clustering and Classification Application of Water and Meteorological Data This case combines the application examples of data clustering and data classification. We use rain gauge data (Central Weather Bureau) and flood sensor data (Water Resource Agency) from Civil IoT Taiwan and use historical data analysis through data clustering to find the group of river water level stations most correlated with rainfall changes. We then use a data classification approach to predict whether a particular region will flood, given only rain gauge data.\nData download and preprocessing We use the following codes to download all the sensing data of the rain gauge (Central Weather Bureau) and flood sensors (Water Resource Agency) in 2021 from the historical database of the Civil IoT Taiwan Data Service Platform and decompress the downloaded file into /content directory.\n!wget -O 'Rain_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZLzIwMjEuemlw\" !wget -O 'Rain_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZL3JhaW5fc3RhdGlvbi5jc3Y%3D\" !unzip -q 'Rain_2021.zip' \u0026\u0026 rm 'Rain_2021.zip' !find '/content/2021' -name '*.zip' -exec unzip -q {} -d '/content/Rain_2021_csv' \\; !rm -rf '/content/2021' !wget -O 'Flood_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoLzIwMjEuemlw\" !wget -O 'Flood_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoL3N0YXRpb25f5rC05Yip572y77yI6IiH57ij5biC5pS%2F5bqc5ZCI5bu677yJX%2Ba3ueawtOaEn%2Ba4rOWZqC5jc3Y%3D\" !unzip -q 'Flood_2021.zip' \u0026\u0026 rm 'Flood_2021.zip' !find '/content/2021' -name '*_QC.zip' -exec unzip -q {} -d '/content/Flood_2021_csv' \\; !rm -rf '/content/2021' We first deal with the rain gauge data and delete the unnecessary fields MIN_10, HOUR_6, HOUR_12, and NOW. Then we remove the data after November and store the remaining records in Rain_df. We also import the information on rain gauges and keep it into Rain_Station_df. Since the amount of data processed in this step is huge, it will take a long time; please wait patiently.\ncsv_files = glob.glob(os.path.join(\"/content/Rain_2021_csv\", \"*.csv\")) csv_files.sort() Rain_df = pd.DataFrame() for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['obsTime']) tmp_df.drop(['MIN_10','HOUR_6', 'HOUR_12', 'NOW'], axis=1, inplace=True) try: tmp_df = tmp_df.loc[tmp_df['obsTime'].dt.minute == 00] Rain_df = pd.concat([Rain_df, tmp_df]) except: print(csv_file) continue Rain_df = Rain_df.loc[Rain_df['obsTime'] \u003c \"2021-11-01 00:00:00\"] num = Rain_df._get_numeric_data() num[num \u003c 0] = 0 Rain_df.dropna(inplace=True) Rain_df.sort_values(by=['station_id','obsTime'], inplace=True) Rain_Station_df = pd.read_csv('/content/Rain_Stataion.csv') Rain_df We also deal with the flood sensor data and remove the records after November. Then, we delete the records with missing values and store the remaining records in Flood_df. We also import the information on flood sensors and keep it into Flood_Station_df. The amount of data processed in this step is huge, and it will take a long time; please wait patiently.\ncsv_files = glob.glob(os.path.join(\"/content/Flood_2021_csv\", \"*_QC.csv\")) csv_files.sort() Flood_df = pd.DataFrame() for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['timestamp']) tmp_df = tmp_df.loc[(tmp_df['PQ_unit'] == 'cm')] Flood_df = pd.concat([Flood_df,tmp_df], axis=0, ignore_index=True) Flood_df = Flood_df.loc[Flood_df['timestamp'] \u003c \"2021-11-01 00:00:00\"] Flood_df.replace(-999.0,0.0, inplace=True) Flood_df.dropna(inplace=True) Flood_df.sort_values(by=['timestamp'], inplace=True) Flood_Station_df = pd.read_csv('/content/Flood_Stataion.csv') Flood_df Correlation of flood sensors and rain gauges Due to the large amount of flood sensor data, we select the flood sensor number 43b2aec1-69b0-437b-b2a2-27c76a3949e8 located in Yunlin County and store its data in the Flood_Site_df object as an example of subsequent processing.\nFlood_Site_df = Flood_df.loc[Flood_df['station_id'] == '43b2aec1-69b0-437b-b2a2-27c76a3949e8'] Flood_Site_df.head() We then compute the similarity between the rain gauge data and the selected flood sensor. We use Dynamic Time Warping (DTW) for our measurements. The smaller the value of DTW, the greater the similarity. To express the similarity more intuitively, we define the similarity in this example as the reciprocal of the DTW value and calculate the similarity between the selected flood sensor and all rain gauge data.\nFlood_Sensor_np = np.array([[v,v,v] for v in Flood_Site_df['value'].to_numpy()]) Site_dtw_Dist = dict() Rain_tmp_df = Rain_df.loc[(Rain_df['obsTime'].dt.hour == 1)] for Site in tqdm(np.unique(Rain_Station_df.loc[Rain_Station_df['city']=='雲林縣']['station_id'].to_numpy())): tmp_df = Rain_tmp_df.loc[(Rain_tmp_df['station_id'] == Site)] if tmp_df.empty: continue tmp_np = tmp_df[['RAIN','HOUR_3','HOUR_24']].to_numpy() Site_dtw_Dist[Site] = (1/fastdtw(Flood_Sensor_np, tmp_np, dist=euclidean)[0]) Site_dtw_Dist = dict(sorted(Site_dtw_Dist.items(), key=lambda item: item[1])) print(json.dumps(Site_dtw_Dist, indent=4, ensure_ascii=False)) { \"88K590\": 4.580649481044748e-05, \"C0K560\": 4.744655320519647e-05, \"C0K490\": 4.79216996101994e-05, \"C0K520\": 5.038234963332513e-05, \"C0K420\": 5.0674082994877385e-05, \"C0K250\": 5.1021366345465985e-05, \"C0K280\": 5.118406054309105e-05, \"A0K420\": 5.1515699268157996e-05, \"C0K400\": 5.178763059243615e-05, \"O1J810\": 5.2282255279259976e-05, \"C0K240\": 5.2470804312991397e-05, \"01J960\": 5.334885670256585e-05, \"C0K470\": 5.438256498969844e-05, \"81K580\": 5.45854441959214e-05, \"C0K410\": 5.5066753408217084e-05, \"A2K570\": 5.520214274022887e-05, \"01J970\": 5.529887546186233e-05, \"C0K480\": 5.5374254960644355e-05, \"C0K460\": 5.5657892623955056e-05, \"72K220\": 5.5690175197363816e-05, \"C0K510\": 5.5742273217039165e-05, \"C1K540\": 5.618025674136218e-05, \"C0K550\": 5.621240903075098e-05, \"C0K450\": 5.62197509062689e-05, \"C0K291\": 5.6380522616008906e-05, \"C0K330\": 5.638960953991442e-05, \"C0K530\": 5.6525582441919285e-05, \"C0K500\": 5.6825555408648244e-05, \"C0K440\": 5.692254536595e-05, \"C0K430\": 5.697351917955081e-05, \"01J930\": 5.7648109890427854e-05, \"C0K580\": 5.770344946580767e-05, \"C0K390\": 5.782553930260475e-05, \"01J100\": 5.7933240408734325e-05, \"01K060\": 5.8343415644572526e-05 } Data clustering to find highly correlated rain gauges We divide the rain gauges into three groups according to the similarity relationship through the clustering algorithm, and find out the group with the highest similarity and the codes of the rain gauges in this group. In our example, we found three clusters with 9, 23, and 3 rain gauges, and the second cluster had the time series data most similar to the flood sensor data.\ncluster_model = KMeans(n_clusters=3).fit([[value] for _, value in Site_dtw_Dist.items()]) Result = cluster_model.labels_ for i in np.unique(Result): print(\"Number of Cluster {} : {}\".format(i,len(Result[Result==i]))) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Site_DTW_Numpy = np.array([value for _, value in Site_dtw_Dist.items()]) Site_Name_Numpy = np.array([key for key, _ in Site_dtw_Dist.items()]) Mean_Dis_For_Cluster = [None] * len(np.unique(Result)) for i in np.unique(Result): Mean_Dis_For_Cluster[i] = (np.mean(Site_DTW_Numpy[Result==i])) ax.scatter(Site_DTW_Numpy[Result==i],[i]*len(Site_DTW_Numpy[Result==i]), s=10) print(\"Mean Distance of Cluster {} : {}\".format(i,Mean_Dis_For_Cluster[i])) ax.get_yaxis().set_visible(False) Best_Cluster = np.where(Mean_Dis_For_Cluster == np.amax(Mean_Dis_For_Cluster))[0] Best_Site = Site_Name_Numpy[Result == Best_Cluster] print(Best_Site) Number of Cluster0 : 23 Number of Cluster1 : 3 Number of Cluster2 : 9 Mean Similarity of Cluster0 : 5.6307994901628334e-05 Mean Similarity of Cluster1 : 4.7058249208614454e-05 Mean Similarity of Cluster2 : 5.1629678408018994e-05 ['C0K470' '81K580' 'C0K410' 'A2K570' '01J970' 'C0K480' 'C0K460' '72K220' 'C0K510' 'C1K540' 'C0K550' 'C0K450' 'C0K291' 'C0K330' 'C0K530' 'C0K500' 'C0K440' 'C0K430' '01J930' 'C0K580' 'C0K390' '01J100' '01K060'] To better understand the correlation between the flood sensor and the 23 rain gauges, we plotted the sensing data of this selected flood sensor in chronological order.\ntmp= Flood_Site_df['value'].to_numpy() fig= plt.figure(figsize=(6, 3), dpi=150) ax= fig.add_subplot(1,1,1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) ax.set_xlabel('Time sequence') ax.set_ylabel('Water level') We then plotted hourly rainfall data chronologically from the 23 rain gauges in the most similar group into 23 graphs. The figures also show that when the value of the flood sensor is high, the value of the rain gauge also increases. The changing trends of the two are indeed very similar, which is in line with our common sense expectations.\nfig = plt.figure(figsize=(8, 2*(len(Best_Site)//4+1)), dpi=150) for i, Site in enumerate(Best_Site): tmp = Rain_df.loc[Rain_df['station_id']==Site]['RAIN'] ax = fig.add_subplot(len(Best_Site)//4+1,4,i+1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) Data classification to predict flooding by rainfall data Next, we use the flood data recorded by the selected flood sensors as labels and use the best similarity group of rain gauges to build a simple classifier to predict whether flooding occurs at the location of the flood sensor. We divide the original data from January 2021 to October 2021 into the training data (the first seven months) and the test data (the next two months). Then we mark the data with a value greater than 0 in the flood sensor as a flood event, and data with a value of 0 is marked as having no flood events. We store the sorted data in the training data Train_DataSet object.\nNote: In this example, based on the principle of ease of use, the most lenient standard is adopted, and events with a water level greater than 0 are identified as flood events. But in fact, there are stricter regulations for determining flood events. For proper use, it is recommended to carry out identification following relevant laws and regulations.\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=10*len(Flooding)).values Train_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(0) In the same way, we mark the data with a value greater than 0 in the flood sensor from August 2021 to October 2021 as a flood event, and the data with a value equal to 0 as a non-flood event. We store the sorted data in the test data Test_DataSet object.\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=2*len(Flooding)).values Test_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(0) We use the classifier model provided by the Python package Scikit Learn (sklearn) for training and testing. We use nine models in a series of examples, including nearest neighbor, linear SVM, RBF SVM, decision tree, random forest, neural network, Adaboost, Naive Bayes, and QDA. We sequentially introduce training data for adjustment and then test data for prediction. We compare the test data with the label content in the predicted results and use a confusion matrix to present the classification results for different label combinations.\nnames = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(Train_DataSet['x'], Train_DataSet['y']) Result = model.predict(Test_DataSet['x']) mat = confusion_matrix(Test_DataSet['y'], Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=[\"不淹水\",\"淹水\"], yticklabels=[\"不淹水\",\"淹水\"], ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(names[i], round(accuracy_score(Result, Test_DataSet['y']), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() In this case, we found that the nearest neighbor method can achieve a classification success rate of nearly 7.3%. This is just the result of using raw data without further processing and analysis. If we do more research on the data itself, we still have the potential to continue to improve the classification success rate. Readers interested in classifiers can refer to related resources for more in-depth exploration, which will further enhance the ability of classifiers to classify different types of data.\nReferences Ibrahim Saidi, Your First Machine Learning Project in Python, Medium, Jan, 2022, (https://ibrahimsaidi.com.au/your-first-machine-learning-project-in-python-e3b90170ae41) Esmaeil Alizadeh, An Illustrative Introduction to Dynamic Time Warping, Medium, Oct. 2020, (https://towardsdatascience.com/an-illustrative-introduction-to-dynamic-time-warping-36aa98513b98) Jason Brownlee, 10 Clustering Algorithms With Python, Machine Learning Mastery, Aug. 2020, (https://machinelearningmastery.com/clustering-algorithms-with-python/) Alexandra Amidon, How to Apply K-means Clustering to Time Series Data, TOwards Data Science, July 2020, (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) scikit-learn Tutorials (https://scikit-learn.org/stable/tutorial/index.html) scikit-learn Classifier comparison (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) FastDTW - A Python implementation of FastDTW, (https://github.com/slaypni/fastdtw) Nearest Neighbors - Wikipedia, (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) SVM Classifier - Wikipedia, (https://en.wikipedia.org/wiki/Support-vector_machine) Decision Tree - Wikipedia, (https://en.wikipedia.org/wiki/Decision_tree) Random Forest - Wikipedia, (https://en.wikipedia.org/wiki/Random_forest) K-Means - Wikipedia, (https://en.wikipedia.org/wiki/K-means_clustering) DBSCAN - Wikipedia, (https://en.wikipedia.org/wiki/DBSCAN) Hierarchical Clustering - Wikipedia, (https://en.wikipedia.org/wiki/Hierarchical_clustering) BIRCH - Wikipedia, (https://en.wikipedia.org/wiki/BIRCH) Confusion matrix - Wikipedia, (https://en.wikipedia.org/wiki/Confusion_matrix#Confusion_matrices_with_more_than_two_categories) ",
    "description": "We use air quality and water level data, combined with weather observations, using machine learning for data classification and data grouping. We demonstrate the standard process of machine learning and introduce how to further predict data through data classification and how to further explore data through data grouping.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "6.1. Machine Learning Preliminaries",
    "uri": "/en/ch6/ch6.1/"
  },
  {
    "content": " Table Of Contents Goal QGIS Basic Operation Data Preparation GeoJSON Output Data Filter and Change Colors Export Thematic Maps Example 2: Distribution of Emergency Shelters Conclusion References QGIS is a free geographic information system. In addition to presenting the data collected by users in the form of geographic data, users can also process, analyze and integrate geospatial data through QGIS, and draw thematic maps. In this article, we will use QGIS to assist in the analysis and presentation of PM2.5 data obtained from Civil IoT Taiwan, and output the results as a thematic map for interpretation after the analysis is complete. We also demonstrate how to combine the disaster prevention data of Civil IoT Taiwan to draw a distribution map of disaster prevention shelters through the QGIS system, allowing citizens to easily query the nearest disaster shelters.\nNote Note: The QGIS version used in this article is 3.16.8, but the functions used in this article are the basic functions of the software. If you use another version of the software, you should still be able to run it normally.\nGoal Import data into QGIS Basic geographic and spatial data analysis in QGIS Export thematic maps from QGIS QGIS Basic Operation After entering the QGIS software, you can see the following operation interface. In addition to the data frame in the middle area, the upper part is the standard toolbar, which provides various basic operation tools and functions. There are two sub-areas on the left, which are the data catalog window and layers; on the right is the analysis toolbar, which provides various analysis tools.\nData Preparation Data source: Civil IoT Taiwan - Historical Data (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器)\nIn this section, we will first describe how to import data into QGIS. Since some data will be split into multiple different data tables during the storage process, we should pay special attention to whether this is the case with the data at hand when analyzing, and recombine the data into the original single data table. Here’s how to recombine the data:\nImport data\nFirst, we need to import the csv file directly downloaded from Civil IoT Taiwan Historical Data into QGIS. Since there are Chinese characters in the data, garbled characters will be displayed when importing, so the import method is Layer (at the top menu) \u003e Add Layer \u003e Add Delimited Text Layer, and the import interface is as follows: Join data\nSince the original data divides PM2.5 and station coordinates (latitude and longitude) into two files, it is necessary to join PM2.5 and latitude and longitude coordinates first. The join method is as follows, right-click the file to be joined \u003e Properties \u003e joins \u003e click the + sign to enter Add Vector Join, as shown below There are four options after entering:\nJoin Layer: the layer you want to join Join field: corresponding to the Target field, which is the reference for Join (similar to Primary key) Target field: corresponding to the Join field, which is the reference for Join (similar to Foreign key) Joined field: the fields you want to join GeoJSON Output Then we use the built-in functions of QGIS to convert the original csv data into GeoJSON files, which is a geographical data representation in JSON format. The operation procedure is to click Processing \u003e Toolbox \u003e Create points from table.\nPlease note that after clicking, select the Table to be imported, select lon in X, enter lat in Y, and specify WGS 84 (latitude and longitude coordinates) in Target CRS, and then enter the name of the file to be output, as shown below.\nThen select the file format you want to export, and click “Save”.\nData Filter and Change Colors Next, we demonstrate how to use QGIS to filter the required stations, and let the color of the stations change with the PM2.5 value.\nUse Intersection to filter the required stations by county\nBefore screening, you need to download the shp files of municipalities, counties and cities from the government data open platform, and then import the shp files of the county and city boundaries into QGIS, as shown below: Note Note: In the Civil IoT Taiwan project, National Chi Nan University is responsible for the deployment of micro air quality sensors in Changhua and Nantou counties and cities. Therefore, in the data obtained this time, there is no Changhua and Nantou. material.\nThen we click on the icon for “Select Feature”, then click on “Country of Country” and select the desired county. Selected counties will be displayed in yellow. Here we take New Taipei City as an example, as shown in the following figure: We then look for the “Intersection” function in “Processing Toolbox”, and after clicking, the following interface will appear, in which there are three input options, namely:\nInput Layer Overlay Layer Intersection (Output) Next, please put the PM2.5 layer in the “Input Layer”, put the county-city boundary layer in the “Overlay Layer” and check “Select features only”, which means that only the stations that intersect with the New Taipei City selected in the previous step are filtered out. Next, enter the name of the file to be exported in the “Intersection”. The supported export file format options are the same as the previously selected options.\nThen, the following results will be obtained: Display different colors according to the value of PM2.5\nThen we right click on the PM2.5 layer \u003e Properties \u003e Symbology to see the dot color settings. The color setting steps for each PM2.5 station are as follows:\nChange the top original setting from “No Symbols” to “Graduated” as follows Select PM25 in the “Value” part Click the “Classify” button at the bottom Set the number of colors in “Classes” (Note: It is recommended to set the number of categories should not be too many) Go to “Color ramp” to set the color of each value Clock “OK” When everything is set, you will get the following image, where the color of the dots changes with the PM 2.5 value, and the Layer on the right shows the PM 2.5 value represented by the different colors. Note Note: The new version of QGIS (after 3.16) already has the basemap of OpenStreetMap, you can click XYZ Tiles -\u003e OpenStreetMap in the figure below to add the OSM basemap.\nExport Thematic Maps After completing the above settings, the next step is to output the QGIS project as a JPG image. After we click Project \u003e New Print Layout, the Layout name setting will pop up. After the setting is completed, the following screen will appear:\nClick “Add map” on the left, and select the range on the drawing area to add the PM 2.5 map, as shown below\nNext, click “Add Legend” on the left and select a range to import the label, while the “Item Properties” on the right can be used to change the font size, color, etc. in the label. Finally, we put the title, scale bar, and compass to complete the thematic map.\nFinally, click Layout \u003e Export as Image in the upper left corner to export the thematic map as an image file.\nExample 2: Distribution of Emergency Shelters Data source: https://data.gov.tw/dataset/73242\nIn the government’s public information, Taiwan’s emergency shelters have been organized into electronic files for the convenience of citizens to download and use. In this example, we’ll use this data to describe how to find the closest shelter to home via QGIS.\nWe first obtain the shelter information from the above URL, and then load the information according to the method mentioned above, as shown below:\nDue to the large number of shelters in Taiwan, this article only analyzes the shelters in Taipei City, and other counties and cities can also be analyzed in the same way. Readers are welcome to try it for themselves. We first use the above intersection method to find the shelters in Taipei City, and then use the Voronoi Polygons on the side toolbar to draw the Voronoi diagram, as shown below:\nFill in the layer of the shelters in Voronoi Polygons and press “Run”\nAccording to the characteristics of Voronoi Diagram, we can know the location of the closest shelter to our house, as shown below:\nAfter the analysis is completed, the analysis results can be produced into a thematic map according to the previous method.\nConclusion In this chapter, we introduced how to import data into QGIS, and how to use the analysis tools in QGIS to investigate the data. Finally, we introduce the method of data exporting, which can make the analyzed data into thematic maps for interpretation. Of course, there are still many functions in QGIS that are not covered in this chapter. If you are interested in QGIS, you can refer to additional resources below.\nReferences QGIS: A Free and Open Source Geographic Information System (https://qgis.org/) QGIS Tutorials and Tips (https://www.qgistutorials.com/en/) QGIS Documentation (https://www.qgis.org/en/docs/index.html) ",
    "description": "We introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.1. QGIS Application",
    "uri": "/en/ch7/ch7.1/"
  },
  {
    "content": "This website provides data application descriptions and examples based on various open data from Civil IoT Taiwan. Please refer to the original website description for the authorization requirements of different available data used on this website. For the Python language packages and software packages used in the program examples provided on this website, please refer to the relevant webpage instructions of each package and software for authorization. The content of articles offered on this site is licensed under CC-BY (https://creativecommons.org/licenses/by/3.0/).\nThe organization of this website project is as follows:\nOrganizer: Science \u0026 Technology Policy Research and Information Center, National Applied Research Laboratories Co-organizer: Institute of Information Science, Academia Sinica Host: Ling-Jyh Chen Advisors: Chih-Chieh Hung; National Chung Hsing University Li-Pen Wang; National Taiwan University Jen-Wei Huang; National Cheng Kung University Wei-Jia Huang; LASS Chih-Hao Liu; National Science and Technology Center for Disaster Reduction Tsui-Ping Hung; Taipei Municipal Yu Cheng Senior High School Huei-Jun Kao; Taipei Municipal Nan-Gang High School Authors: Hung-Ying Chen, Yu-Shen Cheng, Ming-Kuang Chung, Jen-Wei Huang, Sky Hung, Huei-Jun Kao, Quen Luo, Yu-Chi Peng, Tze-Yu Sheng, Cheng-Jia Wu Reviewers: Chia-Kai Liu; DSP, Inc. Wuulong Hsu; LASS Hsin-Cheng Hsieh; National Central University Yu-Hung Liu; Cianjhen Senior High School, Kaohsiung Chien-Hua Ke; National Keelung Senior High School, Keelung Ting-Yen Hung; The Affiliated Senior High School of National Taiwan Normal University ",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "About",
    "uri": "/en/about/"
  },
  {
    "content": " Table Of Contents Programming Language - Python Development Platform – Google Colab References Programming Language - Python The subject of this material is the data application of Civil IoT Taiwan. The articles will use Python, a programming language commonly used in the data science community, as the primary programming language and adopt an easy-to-understand demonstration approach to guide readers step-by-step into the content. The readers can obtain first-hand experience with Civil IoT Taiwan data applications and become able to draw inferences from other data science topics in the future.\nOverall, the Python programming language quickly became the most popular programming language in the data science community in a short period, mainly due to the following three advantages:\nLower barriers to learning: Compared with other text-based programming languages (such as C and Java), Python has fewer special symbols and looks closer to the typical English articles in daily life. Suppose you master Python syntax and program execution logic, coupled with the ability of English words. In that case, it is easy to understand the semantics of Python code, so the threshold for learning Python is very low! A wide variety of libraries: After more than 30 years of development, the Python language has grown into an ecosystem. In addition to the basic syntax, various packages can be added, which can be advanced and transformed into multiple tools suitable for solving different problems. By customizing a Python toolkit (pyCIOT) in this project, the learning ceiling can be raised faster to understand how to more conveniently apply Civil IoT Taiwan data. More suitable for self-study: With Kit, code reading comprehension is more important than code writing ability! If you want to control an existing kit, you must understand the manuals, line them up and piece together the code that solves the problem. Therefore, problem-solving will be “stacking programs based on existing program masters, rather than writing Python code from scratch.” Consequently, it is more suitable to use the self-learning mode to develop code reading and comprehension skills. After you are familiar with the logic of Python, learning to read code will have the opportunity to be as fun as reading a storybook. Due to the above advantages of the Python language, the Python language has become the most commonly used programming language in data science and the first programming language used by many programming learners. Therefore, in addition to various Python language teaching books on the market, many practical learning resources can also be found online. It is worth further exploration and learning for readers interested in Python.\nFree Courses Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) Free e-Book Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) Development Platform – Google Colab Unlike C language, which can be compiled into executable files in advance, Python itself is an interpreted language, that is, it is translated into machine language for computer execution before execution. In other words, it is a literal translation when executed, in situations similar to everyday life. In this way, it is as if translators are helping us to translate and communicate in a language acceptable to foreigners. When we speak a sentence, the translator directly helps us translate the sentence. On the contrary, when the foreigner speaks, the staff will help us translate the foreigner’s words so that we can understand.\nDue to this feature of the interpreted language, in addition to the program editor officially provided by Python, many other editors have different functions. Based on the similarity between Python code and natural language, some people have proposed making the Python editor into a notebook-like mode, an editor that can mix natural language articles and Python code on one page, the most famous of which is Jupyter.\nBased on Jupyter, Google moved the Python interpretation language to the Internet cloud. If you apply for a Google account, you can install the free Colab application on Google Drive and use the browser directly to enjoy the Python program editing function. Due to its simplicity and rich functionality, it has become the most used development platform for Python users.\nOverall, Google Colab has four significant advantages:\nPre-installed packages: The Google Colab development platform has pre-installed most of the Python packages, and users can use them directly, avoiding their installation and even eliminating the problem of version conflicts caused by different packages during the installation process, significantly reducing the entry threshold for using Python to develop programs. Cloud storage: With Google’s own cloud storage space, Google Colab can store program notes during development in the cloud space. As long as it is accessed through the network, it can be seamlessly connected even on different computers, solving the problems of data storage, backup, and portability. Collaboration: Google Colab provides network sharing and online collaboration features, allowing users to share program notes with other users through cloud storage, and allowing users to invite other users to browse, annotate, and even edit their program notes, speeding up team collaboration. Free GPU and TPU resources: With Google’s rich cloud computing resources, Google Colab provides GPU and TPU processors, allowing users to use high-end processors to execute their personal program notes, which is conducive to large-capacity program development needs. For related learning resources of Google Colab, please refer to the following links:\nGetting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) References Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "A brief introduction of the programming language Python and the development platform Google Colab used in the materials",
    "tags": [
      "Introduction"
    ],
    "title": "2.2. Material Tools",
    "uri": "/en/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents Data access under temporal conditions Data access under spatial conditions Case study: Is the air quality here worse than there? Import data Remove invalid data Calculate distance Pandas package Results References This article will access the data of the Civil IoT Taiwan project from the perspective of time and space, and carry out a simple implementation with the theme of air quality monitoring. It will covers the following topics:\nthe usage of the datetime, math, numpy, and pandas packages the processing of JSON format data data processing using Pandas DataFrame Data access under temporal conditions When using get_data() in pyCIOT, data can be obtained according to the start time and end time. The format is passed in the time_range as a dictionary (Dict), which are start, end and num_of_data respectively.\nstart and end refer to the start and end time of data collection, in ISO8601 or Datetime format. num_of_data will control the number of data acquisitions not to exceed this number. If the data in the range exceeds num_of_data, it will be collected at intervals, so that the time interval between data and data tends to be the same.\nTaking air quality data as an example, the acquired data can go back one day at most, so when the end variable is set greater than one day, no data will be acquired. In addition, since the update frequency of each sensor in the Civil IoT Taiwan project is different, the number of data sets per day for different sensors will also be different. For details, please refer to: https://ci.taiwan.gov.tw/dsp/dataset_air.aspx\nfrom datetime import datetime, timedelta end_date = datetime.now() # current datetime isodate_end = end_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format start_date = datetime.now() + timedelta(days = -1) # yesterday isodate_start = start_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format time = { \"start\": isodate_start, \"end\": isodate_end, \"num_of_data\": 15 } data = Air().get_data(\"OBS:EPA_IoT\", stationIds=[\"11613429495\"], time_range=time) data The data will be stored in data in List format, and stored separately according to different properties. The data of temperature, relative humidity and PM2.5 will be stored in the ‘values’ list under the corresponding name respectively, and the time of each data record will be marked and displayed in ISO8601.\n[{'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 30.7}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T12:54:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:53:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 11.9}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 12.15}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 12.2}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 12.22}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 12.31}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 12.19}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 12.26}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 12.17}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 12.04}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 11.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 11.67}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 11.56}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 11.56}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}}] Data access under spatial conditions In pyCIOT, there are also methods to get specific data based on the region. If we take the latitude and longitude of a specific location as the center, we can use the radius distance to form a search range (circle) to get the station ID and sensing value of the specific space.\nThe data format of a specific area is also passed in as a dictionary (Dict), where the latitude, longitude and radius are “latitude”, “longitude” and “distance” respectively. The filtering functions of a specific area and a specific time can be used at the same time. When searching for a specific area, the stations to be observed can also be put into the “stationIds”, and the stations outside the area can be removed by the way.\nloc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # 半徑(km) } c = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) c[0] { 'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': { 'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [ { 'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 35.84}] },{ 'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 59.5}] },{ 'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 11.09}] } ], 'location': { 'latitude': 24.998769, 'longitude': 121.512717, 'address': None } } The above are the basic methods commonly used to obtain pyCIOT station data, using time and space as the filtering criteria, and applicable to all data including location and timestamp types. To demonstrate, we give some simple examples and implement them using these pyCIOT packages.\nCase study: Is the air quality here worse than there? Project codes: OBS:EPA_IoT (low-cost air quality stations by EPA） Target location: Nanshijiao MRT Station Exit 1 (GPS coordinates: 24.990550, 121.507532) Region of interest: Zhonghe District, New Taipei City Import data First, we need to get all the information about the target location and the region of interest. We can use the method of “data access under spatial conditions” to set the latitude and longitude at Exit 1 of Nanshijiao MRT Station, and set the distance to three kilometers. Then, we simply use Air().get_data() to obtain the data:\nloc = { \"latitude\": 24.990550, \"longitude\": 121.507532, \"distance\": 3.0 # (km) } EPA_IoT_zhonghe_data_raw = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) print(\"len:\", len(EPA_IoT_zhonghe_data_raw)) EPA_IoT_zhonghe_data_raw[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [{'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 94.84}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 3.81}]}, {'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 25.72}]}], 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}} Remove invalid data Of the in-scope sites, not every station is still running smoothly. In order to remove potentially problematic sites, we observe the data characteristics of invalid stations and find that all three data (temperature, humidity, PM2.5 concentration) are 0. So we just need to pick and delete this data before we can move on to the next step.\n# Data cleaning EPA_IoT_zhonghe_data = [] for datajson in EPA_IoT_zhonghe_data_raw: # 確認資料存在 if \"data\" not in datajson: continue; # 將格式轉換為 Temperature, Relative_Humidity 和 PM2_5 for rawdata_array in datajson['data']: if(rawdata_array['name'] == 'Temperature'): datajson['Temperature'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'Relative humidity'): datajson['Relative_Humidity'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'PM2.5'): datajson['PM2_5'] = rawdata_array['values'][0]['value'] datajson.pop('data') # 確認所有資料皆為有效，同時去除無資料之檢測站 if \"Relative_Humidity\" not in datajson.keys(): continue if \"PM2_5\" not in datajson.keys(): continue if \"Temperature\" not in datajson.keys(): continue if(datajson['Relative_Humidity'] == 0 and datajson['PM2_5'] == 0 and datajson['Temperature'] == 0): continue EPA_IoT_zhonghe_data.append(datajson) print(\"len:\", len(EPA_IoT_zhonghe_data)) EPA_IoT_zhonghe_data[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24} Calculate distance Assuming that there is no error in the data of each station, the data of the station closest to the target position is the data to be compared. To find the nearest station, we need to calculate the distance between each station and the target location.\nWe can use the point-to-point distance formula to calculate and sort to find the closest station to the target location. But here we use the standard Haversine formula to calculate the spherical distance between two points on Earth. The following is the implementation in the WGS84 coordinate system:\nimport math def LLs2Dist(lat1, lon1, lat2, lon2): R = 6371 dLat = (lat2 - lat1) * math.pi / 180.0 dLon = (lon2 - lon1) * math.pi / 180.0 a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) * math.sin(dLon / 2) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) dist = R * c return dist for data in EPA_IoT_zhonghe_data: data['distance'] = LLs2Dist(data['location']['latitude'], data['location']['longitude'], 24.990550, 121.507532)# (24.990550, 121.507532) EPA_IoT_zhonghe_data[0] {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24, 'distance': 1.052754763080127} Pandas package Pandas is a Python package of commonly used data manipulation and analysis. DataFrame format is used to store two-dimensional or multi-column data format, which is very suitable for data analysis. We convert the processed data into a DataFrame, pick out the required fields, and sort them according to the previously calculated distance from small to large.\nimport pandas as pd df = pd.json_normalize(EPA_IoT_zhonghe_data) #Results contain the required data df EPA_IoT_zhonghe_data_raw = df[['distance', 'PM2_5', 'Temperature', 'Relative_Humidity', 'properties.stationID', 'location.latitude', 'location.longitude', 'properties.areaType']] EPA_IoT_zhonghe_data_raw = EPA_IoT_zhonghe_data_raw.sort_values(by=['distance', 'PM2_5'], ascending=True) EPA_IoT_zhonghe_data_raw Results To know if the air quality at the target location is better than the area of interest, we can roughly use the distribution of air quality across all stations. You can use tools such as the numpy package, a common data science processing library in Python, or directly calculate the mean and standard deviation to get the answer.\nimport numpy as np zhonghe_target = EPA_IoT_zhonghe_data_raw.iloc[0,1] zhonghe_ave = np.mean(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) zhonghe_std = np.std(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) result = (zhonghe_target-zhonghe_ave)/zhonghe_std print('Mean:', zhonghe_ave, 'std:', zhonghe_std) print('PM2.5 of the neareat station:', zhonghe_target) print('The target is ', result, 'standard deviations from the mean.\\n') if(result\u003e0): print('Result: The air quality at the target location is worse.') else: print('Result: The air quality at the target location is better.') Mean: 6.71 std: 3.18 PM2.5 of the neareat station:7.38 The target is 0.21 standard deviations from the mean. Result: The air quality at the target location is worse. References Python pyCIOT package (https://pypi.org/project/pyCIOT/) pandas - Python Data Analysis Library (https://pandas.pydata.org/) 10 minutes to pandas — pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) NumPy (https://numpy.org/) NumPy quickstart (https://numpy.org/doc/stable/user/quickstart.html) Haversine formula - Wikipedia (https://en.wikipedia.org/wiki/Haversine_formula) ",
    "description": "We introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.",
    "tags": [
      "Python",
      "API",
      "Air"
    ],
    "title": "3.2. Data Access under Spatial or Temporal Conditions",
    "uri": "/en/ch3/ch3.2/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Preprocessing Stationary Evaluation Data Forecast ARIMA SARIMAX auto_arima Prophet LSTM Holt-Winter Comparison References The previous chapter has introduced various methods of processing time series data, including visual presentation of data, decomposition of time series data, etc. In this chapter, we will further extract the characteristics of these data and use various predictive models to find the law of the data and predict the future.\nGoal Stationary evaluation of time series data Comparison of different forecasting models The practice of time series data forecasting Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and pmdarima, which need to be installed by :\n!pip install --upgrade pip !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 !pip install pmdarima After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import numpy as np import pandas as pd import pmdarima as pm import statsmodels.api as sm import matplotlib.pyplot as plt import os, zipfile from dateutil import parser as datetime_parser from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.tsa.stattools import adfuller, kpss from kats.consts import TimeSeriesData, TimeSeriesIterator from kats.detectors.outlier import OutlierDetector from kats.models.prophet import ProphetModel, ProphetParams from kats.models.lstm import LSTMModel, LSTMParams from kats.models.holtwinters import HoltWintersParams, HoltWintersModel Data Access The topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.info()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Preprocessing We first resample the data according to the method introduced in Section 4.1 and take the hourly average (air_hour), daily average (air_day), and monthly average (air_month) of the data, respectively.\nair_hour = air.resample('H').mean() air_day = air.resample('D').mean() air_month = air.resample('M').mean() Then we remove the outliers in the air_hour data according to the outlier detection method introduced in Section 4.1 and fill in the missing data with the Forward fill method.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') # remove the outliers outlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outliers_removed = outlierDetection.remover(interpolate=False) air_hour_df = outliers_removed.to_dataframe() air_hour_df.rename(columns={'time': 'timestamp', 'y_0': 'PM25'}, inplace=True) air_hour_df.set_index('timestamp', inplace=True) air_hour = air_hour_df air_hour = air_hour.resample('H').mean() # fill in the missing data with the Forward fill method air_hour.ffill(inplace=True) Stationary Evaluation Before proceeding to predict the data, we first check the stationarity of the data. We select the period we want to detect (for example, 2020-06-10 ~ 2020-06-17) and store the data of this period in the data variable.\ndata = air_hour.loc['2020-06-10':'2020-06-17'] Then we calculate these data’s mean (mean) and variation (var) and plot them.\nnmp = data.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() It can be seen from the figure that the mean does not change much, but the variance varies greatly. We say that such data are poorly stationary; conversely, if the data is stationary, the change in its mean and variance will have nothing to do with time.\nIn other words, if the data distribution has a particular trend over time, it has no stationarity. If the data distribution does not change over time, while the mean and variance remain fixed, it has stationarity. The information on stationarity helps find a suitable model and predict future values.\nThere are at least two common ways to check whether data is stationary:\nAugmented Dickey Fuller (ADF) test: Using the unit root test，the data are stationary if the p-value \u003c 0.05. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: Contrary to the ADF test, if p-value \u003c 0.05, the data is not stationary. # ADF Test result = adfuller(data.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -2.7026194088541704 p-value: 0.07358609270498144 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.620177 p-value: 0.020802 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 If we take the sample data we use as an example, the p-value obtained by the ADF test is 0.073, so the information is not stationary. To achieve stationarity, we next differentiate the data, subtract the i-1th data from the i-th data, and use the obtained results to test again.\nIn the data format of the dataframe, we can directly use data.diff() to differentiate the data and name the differentiated data as data_diff.\ndata_diff = data.diff() data_diff PM25 timestamp\t2020-06-10 00:00:00\tNaN 2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 We can see that the first data is Nan because the first data cannot be subtracted from the previous data, so we have to discard the first data.\ndata_diff = data_diff[1:] data_diff PM25 timestamp\t2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 2020-06-10 05:00:00\t-1.300000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 Then we plot the data to observe the relationship between the mean and variance of the data after differentiation over time.\nnmp = data_diff.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() From the above results, we find that the change in the mean is still small, while the change in the variance becomes smaller. We then repeat the above stationarity evaluation steps:\n# PM25 # ADF Test result = adfuller(data_diff.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data_diff.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -13.350457196046884 p-value: 5.682260865619701e-25 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.114105 p-value: 0.100000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 After the test, the p-value of the ADF test is 5.68e-25, which shows that the data after a difference is stationary, and this result will be used in the subsequent prediction model in the following.\nData Forecast After data preprocessing, we demonstrate using different prediction models to predict time series data. We will use ARIMA, SARIMAX, auto_arima, Prophet, LSTM, and Holt-Winter models.\nARIMA The ARIMA model is an extended version of the ARMA model, so we first introduce the ARMA model and split the ARMA model into two parts, namely:\nAutoregressive model (AR): Use a parameter p and make a linear combination of the previous p historical values to predict the current value. Moving average model (MA): Use a parameter q and make a linear combination of the previous q prediction errors using the AR model to predict the current value. The ARIMA model uses one more parameter, d, than the ARMA model. If the data is not stationary, it needs to be differentiated, and the parameter d represents the number of times to be differentiated.\nBelow we use air quality data to conduct an exercise. First, we plot the data to select the piece of data to use:\nair_hour.loc['2020-06-01':'2020-06-30']['PM25'].plot(figsize=(12, 8)) We select a piece of data that we want to use and divide the data into two parts:\nTrain data: used to train the model and find the most suitable parameters. Test data: used to evaluate the model’s accuracy in data prediction. In our following example, we set the length of the test data to be 48 hours (train_len=-48) and the training data to be all the data minus the last 48 hours.\ndata_arima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_arima.iloc[:train_len] test = data_arima.iloc[train_len:] We first evaluate the stationarity of this piece of data. Then we determine the value of the d parameter by the number of differentiations required.\n# Run Dicky-Fuller test result = adfuller(train) # Print test statistic print('The test stastics:', result[0]) # Print p-value print(\"The p-value:\",result[1]) The test stastics: -3.1129543556288826 The p-value: 0.025609243615341074 Since the p-value is already smaller than 0.05, we can continue exploring the parameters p and q in the ARIMA model without differentiations (d=0). We take a simple method to combine the possible combinations of p and q, respectively, and determine the parameter values by evaluating the quality of the resulting models.\nWe can use the AIC or BIC method to evaluate whether the model fits the training data. Generally speaking, the smaller the judged value, the better the effect of the model. For example, we first limit the range of p and q between 0 and 2 so that there are nine possible combinations. Then, we check the values of AIC and BIC, respectively, and use the combination of p and q with the smallest value as the decision value of the parameters.\nwarnings.filterwarnings('ignore') order_aic_bic =[] # Loop over p values from 0-2 for p in range(3): # Loop over q values from 0-2 for q in range(3): try: # create and fit ARMA(p,q) model model = sm.tsa.statespace.SARIMAX(train['PM25'], order=(p, 0, q)) results = model.fit() # Print order and results order_aic_bic.append((p, q, results.aic, results.bic)) except: print(p, q, None, None) # Make DataFrame of model order and AIC/BIC scores order_df = pd.DataFrame(order_aic_bic, columns=['p', 'q', 'aic','bic']) # lets sort them by AIC and BIC # Sort by AIC print(\"Sorted by AIC \") # print(\"\\n\") print(order_df.sort_values('aic').reset_index(drop=True)) # Sort by BIC print(\"Sorted by BIC \") # print(\"\\n\") print(order_df.sort_values('bic').reset_index(drop=True)) Sorted by AIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 Sorted by BIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 We found that when (p,q) = (1,0), the values of AIC and BIC are the smallest, representing the best configuration of the model. Therefore, we set the three parameters p, d, and q to 1, 0, and 0, respectively, and then started training the model.\n# Instantiate model object model = ARIMA(train, order=(1,0,0)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ============================================================================== Dep. Variable: PM25 No. Observations: 72 Model: ARIMA(1, 0, 0) Log Likelihood -168.853 Date: Fri, 26 Aug 2022 AIC 343.706 Time: 05:01:13 BIC 350.536 Sample: 06-17-2020 HQIC 346.425 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ const 6.3774 1.959 3.255 0.001 2.537 10.218 ar.L1 0.7792 0.047 16.584 0.000 0.687 0.871 sigma2 6.2934 0.746 8.438 0.000 4.832 7.755 =================================================================================== Ljung-Box (L1) (Q): 0.08 Jarque-Bera (JB): 76.49 Prob(Q): 0.77 Prob(JB): 0.00 Heteroskedasticity (H): 2.30 Skew: 1.44 Prob(H) (two-sided): 0.05 Kurtosis: 7.15 =================================================================================== We then use the test data to make predictions and evaluate the accuracy of the predictions. From the resulting graph, we can find that the curve of the data prediction result is too smooth, which is very different from the actual value. If you observe the changing trend of the overall data, you will find that the raw data itself fluctuates regularly, but ARIMA can only predict the trend of the data. If you want to predict the data’s value accurately, there is still a considerable gap in the results.\ndata_arima['forecast'] = results.predict(start=24*5-48, end=24*5) data_arima[['PM25', 'forecast']].plot(figsize=(12, 8)) SARIMAX data_sarimax = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_sarimax.iloc[:train_len] test = data_sarimax.iloc[train_len:] We next introduce the SARIMAX model. The SARIMAX model has seven parameters, p, d, q, P, D, Q, s. These parameters can be divided into two groups: the first group is order=(p, d, q), and they are the same as the parameters of the ARIMA model; the other group is seasonal_order=(P, D, Q, s), and they are the periodic AR model parameters, the periodic differentiation times, the periodic MA model parameters, and the periodic length.\n參數 說明 p AR 模型參數 d 達到平穩性所需要的差分次數 q MA 模型參數 P 週期性的 AR 模型參數 D 週期上達到平穩性所需要的差分次數 Q 週期性的 MA 模型參數 s 週期長度 Since the previous observations demonstrated that these data generally have a periodic change of about 24 hours, we set s=24 and use the following commands to build the model.\n# Instantiate model object model = SARIMAX(train, order=(1,0,0), seasonal_order=(0, 1, 0, 24)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ========================================================================================== Dep. Variable: PM25 No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:26 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== Next, we make predictions using the test data and visualize the prediction results. Although the SARIMA model’s prediction results still have room to improve, they are already much better than the ARIMA model.\ndata_sarimax['forecast'] = results.predict(start=24*5-48, end=24*5) data_sarimax[['PM25', 'forecast']].plot(figsize=(12, 8)) auto_arima We use the pmdarima Python package, which is similar to the auto.arima model in R. The package can automatically find the most suitable ARIMA model parameters, increasing users’ convenience when using ARIMA models. The pmdarima.ARIMA object in the pmdarima package currently contains three models: ARMA, ARIMA, and SARIMAX. When using the pmdarima.auto_arima method, as long as the parameters p, q, P, and Q ranges are provided, the most suitable parameter combination is found within the specified range.\nNext, we will implement how to use pmdarima.auto_arima, and first divide the data set into training data and test data:\ndata_autoarima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_autoarima.iloc[:train_len] test = data_autoarima.iloc[train_len:] For the four parameters p, q, P, Q, we use start and max to specify the corresponding ranges. We also set the periodic parameter seasonal to True and the periodic variable m to 24 hours. Then we can directly get the best model parameter combination and model fitting results.\nresults = pm.auto_arima(train,start_p=0, d=0, start_q=0, max_p=5, max_d=5, max_q=5, start_P=0, D=1, start_Q=0, max_P=5, max_D=5, max_Q=5, m=24, seasonal=True, error_action='warn', trace = True, supress_warnings=True, stepwise = True, random_state=20, n_fits = 20) print(results.summary()) Performing stepwise search to minimize aic ARIMA(0,0,0)(0,1,0)[24] intercept : AIC=268.023, Time=0.04 sec ARIMA(1,0,0)(1,1,0)[24] intercept : AIC=247.639, Time=0.85 sec ARIMA(0,0,1)(0,1,1)[24] intercept : AIC=250.711, Time=0.79 sec ARIMA(0,0,0)(0,1,0)[24] : AIC=271.305, Time=0.04 sec ARIMA(1,0,0)(0,1,0)[24] intercept : AIC=247.106, Time=0.09 sec ARIMA(1,0,0)(0,1,1)[24] intercept : AIC=247.668, Time=0.45 sec ARIMA(1,0,0)(1,1,1)[24] intercept : AIC=inf, Time=2.63 sec ARIMA(2,0,0)(0,1,0)[24] intercept : AIC=249.013, Time=0.15 sec ARIMA(1,0,1)(0,1,0)[24] intercept : AIC=248.924, Time=0.21 sec ARIMA(0,0,1)(0,1,0)[24] intercept : AIC=250.901, Time=0.11 sec ARIMA(2,0,1)(0,1,0)[24] intercept : AIC=250.579, Time=0.30 sec ARIMA(1,0,0)(0,1,0)[24] : AIC=246.926, Time=0.06 sec ARIMA(1,0,0)(1,1,0)[24] : AIC=247.866, Time=0.28 sec ARIMA(1,0,0)(0,1,1)[24] : AIC=247.933, Time=0.31 sec ARIMA(1,0,0)(1,1,1)[24] : AIC=inf, Time=2.35 sec ARIMA(2,0,0)(0,1,0)[24] : AIC=248.910, Time=0.08 sec ARIMA(1,0,1)(0,1,0)[24] : AIC=248.893, Time=0.09 sec ARIMA(0,0,1)(0,1,0)[24] : AIC=252.779, Time=0.08 sec ARIMA(2,0,1)(0,1,0)[24] : AIC=250.561, Time=0.17 sec Best model: ARIMA(1,0,0)(0,1,0)[24] Total fit time: 9.122 seconds SARIMAX Results ========================================================================================== Dep. Variable: y No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:37 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== Finally, we use the best model found for data prediction, and plot the prediction results and test data on the same graph in the form of an overlay. Since the best model found this time is the SARIMAX model just introduced, the results of both predictors are roughly the same.\nresults.predict(n_periods=10) 2020-06-20 00:00:00 10.371336 2020-06-20 01:00:00 13.142043 2020-06-20 02:00:00 13.505843 2020-06-20 03:00:00 9.506395 2020-06-20 04:00:00 7.450378 2020-06-20 05:00:00 7.782850 2020-06-20 06:00:00 7.633757 2020-06-20 07:00:00 5.200781 2020-06-20 08:00:00 3.634188 2020-06-20 09:00:00 3.946824 Freq: H, dtype: float64 data_autoarima['forecast']= pd.DataFrame(results.predict(n_periods=48), index=test.index) data_autoarima[['PM25', 'forecast']].plot(figsize=(12, 8)) Prophet Next, we use the Prophet model provided in the kats suite for data prediction. This model is proposed by Facebook’s data science team, and it is good at predicting periodic time series data and can tolerate missing data, data shift, and outliers.\nWe first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_prophet = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_prophet.iloc[:train_len] test = data_prophet.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) We then use ProphetParams to set the Prophet model’s parameters and the training data and parameters to initialize the ProphetModel. Then we use the fit method to build the model and use the predict method to predict the data, and then we can get the final prediction result.\n# Specify parameters params = ProphetParams(seasonality_mode=\"multiplicative\") # Create a model instance m = ProphetModel(trainData, params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq=\"H\") data_prophet['forecast'] = fcst[['time','fcst']].set_index('time') fcst time\tfcst\tfcst_lower fcst_upper 0\t2020-06-20 00:00:00\t14.705192\t12.268361\t17.042476 1\t2020-06-20 01:00:00\t15.089580\t12.625568\t17.573396 2\t2020-06-20 02:00:00\t14.921077\t12.459802\t17.411335 3\t2020-06-20 03:00:00\t13.846131\t11.444988\t16.200284 4\t2020-06-20 04:00:00\t12.278140\t9.863531\t14.858334 5\t2020-06-20 05:00:00\t10.934739\t8.372450\t13.501025 6\t2020-06-20 06:00:00\t10.126712\t7.654647\t12.658054 7\t2020-06-20 07:00:00\t9.535067\t7.034313\t11.762639 8\t2020-06-20 08:00:00\t8.661877\t6.255147\t11.132732 9\t2020-06-20 09:00:00\t7.424133\t5.055052\t9.770750 10\t2020-06-20 10:00:00\t6.229786\t3.640543\t8.625856 11\t2020-06-20 11:00:00\t5.464764\t3.039011\t7.939283 12\t2020-06-20 12:00:00\t4.998005\t2.692023\t7.550191 13\t2020-06-20 13:00:00\t4.334771\t1.961382\t6.506875 14\t2020-06-20 14:00:00\t3.349172\t1.059836\t5.768178 15\t2020-06-20 15:00:00\t2.819902\t0.399350\t5.226658 16\t2020-06-20 16:00:00\t4.060070\t1.556264\t6.322976 17\t2020-06-20 17:00:00\t7.792830\t5.331987\t10.237182 18\t2020-06-20 18:00:00\t13.257767\t10.873149\t15.542380 19\t2020-06-20 19:00:00\t18.466805\t15.895210\t20.874602 20\t2020-06-20 20:00:00\t21.535994\t19.150397\t23.960260 21\t2020-06-20 21:00:00\t22.005943\t19.509141\t24.691836 22\t2020-06-20 22:00:00\t21.014449\t18.610361\t23.661906 23\t2020-06-20 23:00:00\t20.191905\t17.600568\t22.868388 24\t2020-06-21 00:00:00\t20.286952\t17.734177\t22.905280 25\t2020-06-21 01:00:00\t20.728067\t18.235829\t23.235212 26\t2020-06-21 02:00:00\t20.411124\t17.755181\t22.777073 27\t2020-06-21 03:00:00\t18.863739\t16.261775\t21.315573 28\t2020-06-21 04:00:00\t16.661351\t13.905466\t19.374726 29\t2020-06-21 05:00:00\t14.781150\t12.401465\t17.499478 30\t2020-06-21 06:00:00\t13.637436\t11.206142\t16.239831 31\t2020-06-21 07:00:00\t12.793609\t9.940829\t15.319559 32\t2020-06-21 08:00:00\t11.580455\t9.059603\t14.261605 33\t2020-06-21 09:00:00\t9.891025\t7.230943\t12.471543 34\t2020-06-21 10:00:00\t8.271552\t5.840853\t10.677227 35\t2020-06-21 11:00:00\t7.231671\t4.829449\t9.733231 36\t2020-06-21 12:00:00\t6.592515\t4.108251\t9.107216 37\t2020-06-21 13:00:00\t5.699548\t3.288052\t8.019402 38\t2020-06-21 14:00:00\t4.389985\t1.848621\t6.825121 39\t2020-06-21 15:00:00\t3.685033\t1.196467\t6.150064 40\t2020-06-21 16:00:00\t5.289956\t2.907623\t8.012851 41\t2020-06-21 17:00:00\t10.124029\t7.397842\t12.676256 42\t2020-06-21 18:00:00\t17.174959\t14.670539\t19.856592 43\t2020-06-21 19:00:00\t23.856724\t21.102924\t26.712359 44\t2020-06-21 20:00:00\t27.746195\t24.636118\t30.673178 45\t2020-06-21 21:00:00\t28.276321\t25.175013\t31.543197 46\t2020-06-21 22:00:00\t26.932054\t23.690073\t29.882014 47\t2020-06-21 23:00:00\t25.811943\t22.960132\t28.912079 data_prophet timestamp\tPM25\tforecast 2020-06-17 00:00:00\t6.300000\tNaN 2020-06-17 01:00:00\t11.444444\tNaN 2020-06-17 02:00:00\t6.777778\tNaN 2020-06-17 03:00:00\t4.875000\tNaN 2020-06-17 04:00:00\t5.444444\tNaN ...\t...\t... 2020-06-21 19:00:00\t18.777778\t23.856724 2020-06-21 20:00:00\t21.400000\t27.746195 2020-06-21 21:00:00\t11.222222\t28.276321 2020-06-21 22:00:00\t9.800000\t26.932054 2020-06-21 23:00:00\t8.100000\t25.811943 We use the built-in drawing method of ProphetModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range, and overall the data prediction results are satisfactory.\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() LSTM Next, we introduce the Long Short-Term Memory (LSTM) model for data prediction. The LSTM model is a predictive model suitable for continuous data because it will generate different long-term and short-term memories for data at different times and use it to predict the final result. Currently, the LSTM model is provided in the kats package, so we can directly use the syntax similar to using the Prophet model.\nWe first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_lstm = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_lstm.iloc[:train_len] test = data_lstm.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) Then we select the parameters of the LSTM model in order, namely the number of training times (num_epochs), the time length of data read in at one time (time_window), and the number of neural network layers related to long-term and short-term memory (hidden_size). Then you can directly perform model training and data prediction.\nparams = LSTMParams( hidden_size=10, # number of hidden layers time_window=24, num_epochs=30 ) m = LSTMModel(trainData, params) m.fit() fcst = m.predict(steps=48, freq=\"H\") data_lstm['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst\tfcst_lower\tfcst_upper 0\t2020-06-20 00:00:00\t11.905971\t11.310672\t12.501269 1\t2020-06-20 01:00:00\t10.804338\t10.264121\t11.344554 2\t2020-06-20 02:00:00\t9.740741\t9.253704\t10.227778 3\t2020-06-20 03:00:00\t8.696406\t8.261586\t9.131226 4\t2020-06-20 04:00:00\t7.656923\t7.274077\t8.039769 5\t2020-06-20 05:00:00\t6.608442\t6.278019\t6.938864 6\t2020-06-20 06:00:00\t5.543790\t5.266600\t5.820979 7\t2020-06-20 07:00:00\t4.469023\t4.245572\t4.692474 8\t2020-06-20 08:00:00\t3.408312\t3.237897\t3.578728 9\t2020-06-20 09:00:00\t2.411980\t2.291381\t2.532578 10\t2020-06-20 10:00:00\t1.564808\t1.486567\t1.643048 11\t2020-06-20 11:00:00\t0.982147\t0.933040\t1.031255 12\t2020-06-20 12:00:00\t0.792612\t0.752981\t0.832242 13\t2020-06-20 13:00:00\t1.105420\t1.050149\t1.160691 14\t2020-06-20 14:00:00\t1.979013\t1.880062\t2.077964 15\t2020-06-20 15:00:00\t3.408440\t3.238018\t3.578862 16\t2020-06-20 16:00:00\t5.337892\t5.070997\t5.604786 17\t2020-06-20 17:00:00\t7.659332\t7.276365\t8.042299 18\t2020-06-20 18:00:00\t10.104147\t9.598940\t10.609355 19\t2020-06-20 19:00:00\t12.047168\t11.444809\t12.649526 20\t2020-06-20 20:00:00\t12.880240\t12.236228\t13.524252 21\t2020-06-20 21:00:00\t12.748750\t12.111312\t13.386187 22\t2020-06-20 22:00:00\t12.128366\t11.521947\t12.734784 23\t2020-06-20 23:00:00\t11.311866\t10.746273\t11.877459 24\t2020-06-21 00:00:00\t10.419082\t9.898128\t10.940036 25\t2020-06-21 01:00:00\t9.494399\t9.019679\t9.969119 26\t2020-06-21 02:00:00\t8.551890\t8.124296\t8.979485 27\t2020-06-21 03:00:00\t7.592260\t7.212647\t7.971873 28\t2020-06-21 04:00:00\t6.613075\t6.282421\t6.943729 29\t2020-06-21 05:00:00\t5.614669\t5.333936\t5.895402 30\t2020-06-21 06:00:00\t4.605963\t4.375664\t4.836261 31\t2020-06-21 07:00:00\t3.611552\t3.430974\t3.792129 32\t2020-06-21 08:00:00\t2.679572\t2.545593\t2.813550 33\t2020-06-21 09:00:00\t1.887442\t1.793070\t1.981814 34\t2020-06-21 10:00:00\t1.340268\t1.273255\t1.407282 35\t2020-06-21 11:00:00\t1.156494\t1.098669\t1.214318 36\t2020-06-21 12:00:00\t1.440240\t1.368228\t1.512252 37\t2020-06-21 13:00:00\t2.251431\t2.138859\t2.364002 38\t2020-06-21 14:00:00\t3.592712\t3.413076\t3.772347 39\t2020-06-21 15:00:00\t5.415959\t5.145161\t5.686757 40\t2020-06-21 16:00:00\t7.613187\t7.232528\t7.993847 41\t2020-06-21 17:00:00\t9.918564\t9.422636\t10.414493 42\t2020-06-21 18:00:00\t11.755348\t11.167580\t12.343115 43\t2020-06-21 19:00:00\t12.576593\t11.947764\t13.205423 44\t2020-06-21 20:00:00\t12.489052\t11.864599\t13.113504 45\t2020-06-21 21:00:00\t11.915885\t11.320090\t12.511679 46\t2020-06-21 22:00:00\t11.133274\t10.576610\t11.689938 47\t2020-06-21 23:00:00\t10.264495\t9.751270\t10.777719 We also use the built-in drawing method of LSTMModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range, but overall, the data prediction result (blue curve) is slightly lower than the test data (black curve).\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() Holt-Winter We also use the Holt-Winter model provided by the kats package, a method that uses moving averages to assign weights to historical data for data forecasting. We first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_hw = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_hw.iloc[:train_len] test = data_hw.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) Then we need to set the parameters of the Holt-Winter model, which are to select whether to use addition or multiplication to decompose the time series data (the following example uses multiplication, mul), and the length of the period (the following example uses 24 hours). Then we can perform model training and data prediction.\nwarnings.simplefilter(action='ignore') # Specify parameters params = HoltWintersParams( trend=\"mul\", seasonal=\"mul\", seasonal_periods=24, ) # Create a model instance m = HoltWintersModel( data=trainData, params=params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq='H') data_hw['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst 72\t2020-06-20 00:00:00\t14.140232 73\t2020-06-20 01:00:00\t14.571588 74\t2020-06-20 02:00:00\t12.797056 75\t2020-06-20 03:00:00\t10.061594 76\t2020-06-20 04:00:00\t9.927476 77\t2020-06-20 05:00:00\t8.732691 78\t2020-06-20 06:00:00\t10.257460 79\t2020-06-20 07:00:00\t8.169070 80\t2020-06-20 08:00:00\t6.005400 81\t2020-06-20 09:00:00\t5.038056 82\t2020-06-20 10:00:00\t6.391835 83\t2020-06-20 11:00:00\t5.435677 84\t2020-06-20 12:00:00\t3.536135 85\t2020-06-20 13:00:00\t2.725477 86\t2020-06-20 14:00:00\t2.588198 87\t2020-06-20 15:00:00\t2.967987 88\t2020-06-20 16:00:00\t3.329448 89\t2020-06-20 17:00:00\t4.409821 90\t2020-06-20 18:00:00\t10.295263 91\t2020-06-20 19:00:00\t10.587033 92\t2020-06-20 20:00:00\t14.061718 93\t2020-06-20 21:00:00\t18.597275 94\t2020-06-20 22:00:00\t12.040684 95\t2020-06-20 23:00:00\t12.124081 96\t2020-06-21 00:00:00\t13.522973 97\t2020-06-21 01:00:00\t13.935499 98\t2020-06-21 02:00:00\t12.238431 99\t2020-06-21 03:00:00\t9.622379 100\t2020-06-21 04:00:00\t9.494116 101\t2020-06-21 05:00:00\t8.351486 102\t2020-06-21 06:00:00\t9.809694 103\t2020-06-21 07:00:00\t7.812468 104\t2020-06-21 08:00:00\t5.743248 105\t2020-06-21 09:00:00\t4.818132 106\t2020-06-21 10:00:00\t6.112815 107\t2020-06-21 11:00:00\t5.198396 108\t2020-06-21 12:00:00\t3.381773 109\t2020-06-21 13:00:00\t2.606503 110\t2020-06-21 14:00:00\t2.475216 111\t2020-06-21 15:00:00\t2.838426 112\t2020-06-21 16:00:00\t3.184109 113\t2020-06-21 17:00:00\t4.217320 114\t2020-06-21 18:00:00\t9.845847 115\t2020-06-21 19:00:00\t10.124881 116\t2020-06-21 20:00:00\t13.447887 117\t2020-06-21 21:00:00\t17.785455 118\t2020-06-21 22:00:00\t11.515076 119\t2020-06-21 23:00:00\t11.594832 We also use the built-in drawing method of HoltWintersModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range. Still, overall, the data prediction result (blue curve) responds slightly slower to the rising slope than the test data (black curve).\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') # ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() Comparison Finally, to facilitate observation and comparison, we will draw the prediction results of the six models introduced in the figure below simultaneously (Note: You must first run all the codes of the above prediction models to see the results of these six prediction models). We can observe and compare the prediction accuracy of the six models under different time intervals and curve change characteristics, which is convenient for users to decide on the final model selection and possible future applications.\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8)) data_arima[['PM25', 'forecast']].plot(ax=axes[0, 0], title='ARIMA') data_sarimax[['PM25', 'forecast']].plot(ax=axes[1, 0], title='SARIMAX') data_autoarima[['PM25', 'forecast']].plot(ax=axes[2, 0], title='auto_arima') data_prophet[['PM25', 'forecast']].plot(ax=axes[0, 1], title='Prophet') data_lstm[['PM25', 'forecast']].plot(ax=axes[1, 1], title='LSTM') data_hw[['PM25', 'forecast']].plot(ax=axes[2, 1], title='Holt-Winter') fig.tight_layout(pad=1, w_pad=2, h_pad=5) References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Rob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice, 3rd edition (https://otexts.com/fpp3/) Stationarity, NIST Engineering Statistics Handbook (https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm) Unit root test - Wikipedia (https://en.wikipedia.org/wiki/Unit_root_test) Akaike information criterion (AIC) - Wikipedia (https://en.wikipedia.org/wiki/Akaike_information_criterion) Bayesian information criterion (BIC) - Wikipedia (https://en.wikipedia.org/wiki/Bayesian_information_criterion) ARIMA models (https://otexts.com/fpp2/arima.html) SARIMAX: Introduction (https://www.statsmodels.org/stable/examples/notebooks/generated/statespace_sarimax_stata.html) Prophet: Forecasting at scale (https://facebook.github.io/prophet/) Long short-term memory (LSTM) – Wikipedia (https://en.wikipedia.org/wiki/Long_short-term_memory) Time Series Forecasting with ARIMA Models In Python [Part 1] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48?gi=264dc7630363) Time Series Forecasting with ARIMA Models In Python [Part 2] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-2-91a30d10efb0) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057) Kats - Time Series Forecasting By Facebook | by Himanshu Sharma | MLearning.ai | Medium (https://medium.com/mlearning-ai/kats-time-series-forecasting-by-facebook-a2741794d814) ",
    "description": "We use the sensing data of the Civil IoT Taiwan Data Service Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare the prediction results of different data prediction models. We use graphics to present the data and discuss the significance of the data prediction of the dataset at different time resolutions in the real field, as well as possible derived applications.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.2. Time Series Data Forecast",
    "uri": "/en/ch4/ch4.2/"
  },
  {
    "content": "\nTable Of Contents Voronoi diagram Convex hull Clustering Kernel density Spatial interpolation Inverse Distance Weighting Kriging Nearest neighbor Interpolation Contour Profile References Microsensors that are widespread in the environment help us obtain environmental information, making decisions and taking actions accordingly. Therefore, it’s fundamental for us to clearly understand the spatial relationships between stations when analyzing data of stations. Apart from that the location of stations itself may form certain geometric structures or spatial clusters, we can also estimate the values where there’s no stations according to the location of stations and their value differences. Thus, we will have a picture of value distribution that’s more comprehensive, within which we may explore the correlations between values and environmental factors. In this section, with data of flooding sensors and groundwater table observation stations in different counties provided by the Water Resources Agency (MOEA), we can practice some simple spatial analyses.\nVoronoi diagram First of all, we may need to clarify the service/defense area of individual stations, within which we further ask if data of certain stations can represent certain areas. In this case, Voronoi Diagram can help us look for the area. The principle of Voronoi Diagram is that by building vertical and equally divided lines between two nearby stations and integrating them, a polygon can be made. The center of each polygon is the station, of which the data represent values within this area. In this section, we can try to practice creating Voronoi Diagrams with data of flooding sensors in Chiayi City and Chiayi County. Thus, we can have a rough understanding about the sphere of influence of flooding sensors.\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install pykrige !pip install pykrige #install elevation !pip install elevation #install affine rasterio !pip install affine rasterio #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # downlaod the county boundary shpfile from open database !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # # get flood sensors' data by pyCIOT wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") flood_list = wa + wa2 county = gpd.read_file('/content/shp/COUNTY_MOI_1090820.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_flood, basemap, how='intersection') from scipy.spatial import Voronoi, voronoi_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) inputp = intersected_data[['lon','lat']] basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); vor = Voronoi(inputp) voronoi_plot_2d(vor,ax = ax,show_vertices=False,) plt.show() Besides, we can describe the service/defense areas of stations with Delaunay Triangulation, with which we choose one station as the center and search for two nearest dots to create a triangular service area. If we consider the triangular service area as a homogeneous sphere, in which data of sensors can be replaced with the average values obtained from the three nodal stations.\nOverall, the two algorithms can help us understand the spatial distribution of sensors and the spatial structure it constructs as graphics.\nfrom scipy.spatial import Delaunay, delaunay_plot_2d import numpy as np fig, ax = plt.subplots(figsize=(6, 10)) #input should be array inputp = np.array(inputp) tri = Delaunay(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); delaunay_plot_2d(tri,ax=ax) plt.show() Convex hull The algorithm of Convex Hull aims at selecting stations located at marginal areas from a group of stations, and form a polygon with smallest side lengths while containing all points. Therefore, we can locate the clustering range among many stations, with which more calculation can be done. To compute the algorithm of Convex Hull, we basically have to arrange the order of stations according to their x coordinates; if the x coordinates are identical, we can use y coordinates for arrangement to find out the peripheral endpoints to form polygons. (Of course, there are many other methods applying similar concepts.) With such an algorithm, we can evaluate the effective inspection areas of stations. Accordingly, we can examine the covering area of flooding sensors with data of their distribution in Chiayi City and Chiayi County.\nfrom scipy.spatial import ConvexHull, convex_hull_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) hull = ConvexHull(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); convex_hull_plot_2d(hull,ax=ax) plt.tight_layout() Clustering As explained above, the nearer the stations are, the more similar the interfering factors in surrounding environments are. So, with K-means algorithm serving as a partitioning method, we can partition stations into clusters to explore the relationship between sensor data and environmental factors.\nAfter we decide to partition our observations into n clusters, K-means clustering will randomly select n points as centers to search for their nearby neighbors; by measuring the linear distance between observation points and their centers, K-means clustering partitions the observation points and calculates the average of each cluster. Repeating the procedure above, K-means clustering aims to minimize average distances between all observation points and their centers, and the partitioning will be finished once the distance becomes shortest.\n# get the groundwater station data through pyCIOT count = 733 num = 0 water_level = pd.DataFrame([]) while(num\u003c=count): url_level = \"https://sta.ci.taiwan.gov.tw/STA_WaterResource_v2/v1.0/Datastreams?$skip=\"+str(num)+\"\u0026$filter=%28%28Thing%2Fproperties%2Fauthority_type+eq+%27%E6%B0%B4%E5%88%A9%E7%BD%B2%27%29+and+substringof%28%27Datastream_Category_type%3D%E5%9C%B0%E4%B8%8B%E6%B0%B4%E4%BD%8D%E7%AB%99%27%2CDatastreams%2Fdescription%29%29\u0026$expand=Thing,Thing%28%24expand%3DLocations%29,Observations%28%24top%3D1%3B%24orderby%3DphenomenonTime+desc%3B%24top%3D1%29\u0026$count=true\" ssl._create_default_https_context = ssl._create_unverified_context r_l = urllib.request.urlopen(url_level) string_l = r_l.read().decode('utf-8') jf_level = json.loads(string_l) station = pd.DataFrame(jf_level['value']).filter(items=['Thing','observedArea','Observations']) station['lat']=station['observedArea'] for i in range(len(station)): station['Thing'][i] = station['Thing'][i]['properties']['stationName'] if pd.isnull(station['observedArea'][i]): station['lat'][i]=-1 station['observedArea'][i]=-1 else: station['lat'][i]=station['lat'][i]['coordinates'][1] station['observedArea'][i]=station['observedArea'][i]['coordinates'][0] if len(station['Observations'][i])!=0: station['Observations'][i] = station['Observations'][i][0]['result'] else: station['Observations'][i] = -1 station = station.rename(columns={\"Thing\": \"name\", 'observedArea': 'lon'}) if num ==0 : water_level = station else: water_level = pd.concat([water_level, station]) num+=100 result_df = water_level.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) station = station[station.lon!=-1] gdf_level = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") # intersect with county boundary basemap = county.loc[county['COUNTYNAME'].isin([\"雲林縣\"])] basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_level, basemap, how='intersection') from sklearn.cluster import KMeans from scipy.spatial import ConvexHull import folium clusterp = intersected_data[[\"name\",\"lon\", 'lat', 'Observations']] # 1. identify the clusters by kmeans #1.1 pre-processing X = clusterp.iloc[:, 1:3].values # ddecide the number groups by elbow method wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1, 11), wcss) plt.title('The Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show() # 1.2 applying K-Means model kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42) y_kmeans = kmeans.fit_predict(X) # 1.3 map data back to df clusterp['cluster'] = y_kmeans+1 # to step up to group 1 to 4 # 2. display the result on maps m = folium.Map(location=[clusterp['lat'].mean(), clusterp['lon'].mean()], tiles='CartoDB positron', zoom_start=7) # create the layer by kmeans layer1 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup1\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer1) layer2 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup2\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer2) layer3 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup3\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer3) layer4 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup4\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer4) # create symbology my_symbol_css_class= \"\"\" \u003cstyle\u003e .fa-g1:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g4 '; } .fa-g1bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g4 '; } \u003c/style\u003e \"\"\" # add the symbology code to the map m.get_root().html.add_child(folium.Element(my_symbol_css_class)) for index, row in clusterp.iterrows(): if row['cluster'] == 1: color='black' fa_symbol = 'fa-g1' lay = layer1 elif row['cluster'] == 2: color='purple' fa_symbol = 'fa-g2' lay = layer2 elif row['cluster'] == 3: color='orange' fa_symbol = 'fa-g3' lay = layer3 elif row['cluster'] == 4: color='blue' fa_symbol = 'fa-g4' lay = layer4 folium.Marker( location=[row['lat'], row['lon']], title = row['name']+ 'group:{}'.format(str(row[\"cluster\"])), popup = row['name']+ 'group:{},value:{}'.format(str(row[\"cluster\"]),str(row['Observations'])), icon= folium.Icon(color=color, icon=fa_symbol, prefix='fa')).add_to(lay) # display the result on maps layer_list = [layer1,layer2,layer3,layer4] color_list = ['black','purple','orange','blue'] for g in clusterp['cluster'].unique(): latlon_cut =clusterp[clusterp['cluster']==g].iloc[:, 1:3] hull = ConvexHull(latlon_cut.values) Lat = latlon_cut.values[hull.vertices,0] Long = latlon_cut.values[hull.vertices,1] cluster = pd.DataFrame({'lat':Lat,'lon':Long }) area = list(zip(cluster['lat'],cluster['lon'])) list_index = g-1 lay_cluster = layer_list[list_index ] folium.Polygon(locations=area, color=color_list[list_index], weight=2, fill=True, fill_opacity=0.1, opacity=0.8).add_to(lay_cluster) folium.LayerControl(collapsed=False,position= 'bottomright').add_to(m) print(m) m.save('River_clustering.html') Kernel density The concept of density allows us to describe the intensity of clustering, and it’s traditionally defined by the formula d = N/A, where d is density, N is number of observations, and A is area. However, the formula above is easily influenced by the area, which makes it possible that the same station/case numbers in towns that have different area sizes result in different densities. Therefore, we might have difficulty calculating the intensity of clustering events correctly.\nIn order to avoid calculation differences of areas, we describe the clustering intensity with the concept of kernel density, which takes observation points as centers and a fixed radius to select neighbors, and finally replaces the original observation values with the total values obtained from all neighbors. Such method standardizes the “area” in the density formula and thus obtains a more comprehensive density distribution, which helps us understand the distribution intensity of events.\nbasemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] basemap = basemap.set_crs(4326,allow_override=True) gdf = gpd.overlay(gdf_level, basemap, how='intersection') # selecting the polygon's geometry field to filter out points that # are not overlaid import plotly.express as px fig = px.density_mapbox(gdf, lat='lat', lon='lon', z='Observations', radius=25, center=dict(lat=23.5, lon=120.5), zoom=8, mapbox_style=\"stamen-terrain\") fig.show() Spatial interpolation The implementation of microsensors is similar to our spatial samplings, with which we can estimate the population distribution with statistical analyses. Since it’s impossible to spread observation stations all over earth’s surface, we are faced with conditions in which some places are rich in data while others aren’t. In such a scenario, spatial interpolation is a statistical method that helps us estimate regions without data, and to further obtain a whole picture of our study area.\nBefore conducting spatial interpolation, we need to clarify two concepts: deterministic model and stochastic model. In a deterministic model, if we know the distribution rule of certain spatial phenomena, we can estimate the value of unknown areas with correlation parameters. Take the house number in Taiwan for example. House numbers in Taiwan are arranged by odd and even numbers respectively. So, suppose we wonder the number of certain house, and we know the previous one is 6 while the next one is 10, we can therefore presume the house in the middle (which is the one we’re interested in) is 8. As for a stochastic model, it assumes that the reality is rather complicated, and we can only construct suitable estimation models with probability and variety of variances while accepting the uncertainty.\nInverse Distance Weighting With Inverse Distance Weighting, we construct an estimation model with value deviations and intervals of known observation points. Generally speaking, if the deviation of two points is 10 and the interval is 100 meter, then the deviation per 10 meter should be 1 theoretically. But, since there may not be a linear relationship in the deviation distributions, the principle of IDW emphasizes on the first law of geography: nearby things are closer than others, and estimates value deviations of two points. By obtaining the reciprocal of the cube of the product of deviation and distance, IDW allows us to estimate values of certain positions. So, the shorter the distance is, the larger the weighting is, and vice versa.\nimport numpy as np import matplotlib.pyplot as plt from scipy.interpolate import Rbf def distance_matrix(x0, y0, x1, y1): obs = np.vstack((x0, y0)).T interp = np.vstack((x1, y1)).T # build distance matrix d0 = np.subtract.outer(obs[:,0], interp[:,0]) d1 = np.subtract.outer(obs[:,1], interp[:,1]) print(d0.dtype,d1.dtype) return np.hypot(d0, d1) def simple_idw(x, y, z, xi, yi, pows): dist = distance_matrix(x,y, xi,yi) # the IDW weight is 1 / distance weights = 1.0 / dist # weight is 1 weights /= weights.sum(axis=0) # set the Z value zi = np.dot(weights.T, z) return zi fig, ax = plt.subplots(figsize=(6, 4)) ax.set_aspect('equal') pows = 2 nx, ny = 100, 100 xmin, xmax = 119.8, 121.2 ymin, ymax = 23, 24 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] x = interpolatep['lon'] y = interpolatep['lat'] z = interpolatep['Observations'] x = x.astype(\"float64\") y = y.astype(\"float64\") z = z.astype(\"float64\") xi = np.linspace(xmin,xmax, nx) yi = np.linspace(ymin, ymax, ny) xi, yi = np.meshgrid(xi, yi) xi, yi = xi.flatten(), yi.flatten() # 計算 IDW grid = simple_idw(x,y,z,xi,yi,pows) grid = grid.reshape((ny, nx)) grid = grid.astype(\"float64\") plt.imshow(grid, extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='lightgray'); ax.scatter(x, y, marker=\".\", color='orange', s=z,label=\"input point\") plt.colorbar() plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.title('IDW') plt.show() Kriging The principle of Kriging is to build a semi-variogram with the value and position of known observed points, and we can partition the observations based on the semi-variogram to obtain several regionalized variables for value estimation. Similar to IDW, Kriging also utilizes values and distances of known points to estimate nearby unknown points. Notice that in Kriging, we divide observations into groups by distances, and the estimation formulas will be adjusted according to distances.\n# set the data extend and resolution import numpy as np resolution = 0.1 # cell size in meters gridx = np.arange(119.8, 121.2, resolution) gridy = np.arange(23, 24, resolution) # set the raster to polygon import itertools from shapely.geometry import Polygon def pixel2poly(x, y, z, resolution): \"\"\" x: x coords of cell y: y coords of cell z: matrix of values for each (x,y) resolution: spatial resolution of each cell \"\"\" polygons = [] values = [] half_res = resolution / 2 for i, j in itertools.product(range(len(x)), range(len(y))): minx, maxx = x[i] - half_res, x[i] + half_res miny, maxy = y[j] - half_res, y[j] + half_res polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)])) if isinstance(z, (int, float)): values.append(z) else: values.append(z[j, i]) return polygons, values # calculate ithe pykrige package from pykrige.ok import OrdinaryKriging krig = OrdinaryKriging(x=gdf[\"lon\"], y=gdf[\"lat\"], z=gdf['Observations'], variogram_model=\"spherical\", pseudo_inv=True) z, ss = krig.execute(\"grid\", gridx, gridy) plt.imshow(z); # dispaly by plotly import plotly.express as px polygons, values = pixel2poly(gridx, gridy, z, resolution) water_model = (gpd.GeoDataFrame({\"water_modelled\": values}, geometry=polygons, crs=\"EPSG:4326\") .to_crs(\"EPSG:4326\") ) fig = px.choropleth_mapbox(water_model, geojson=water_model.geometry, locations=water_model.index, color=\"water_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5, center={\"lat\": 24, \"lon\": 121}, zoom=6, mapbox_style=\"carto-positron\") fig.update_layout(margin=dict(l=0, r=0, t=30, b=10)) fig.update_traces(marker_line_width=0) Nearest neighbor Interpolation Nearest Neighbor Interpolation is actually quite simple. If we want to know the value of a certain position, we only need to find the nearest station with data, thus taking it in replacement. The design of this method basically follows the principle of “nearby things are more similar,” and it’s often applied in image processing and cases of enlargement.\nfrom scipy.interpolate import NearestNDInterpolator import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(6, 4)) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") X = np.linspace(min(xd), max(xd)) Y = np.linspace(min(yd), max(yd)) X, Y = np.meshgrid(X, Y) # 2D grid for interpolation interp = NearestNDInterpolator(list(zip(xd, yd)), zd) Z = interp(X, Y) im = ax.pcolormesh(X, Y, Z, shading='auto') basemap.plot(ax=ax, facecolor='none', edgecolor='gray'); sns.scatterplot(x='lon', y='lat', data=interpolatep,label=\"input point\") plt.legend() plt.colorbar(im) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.show() Contour Generally speaking, after conducting spatial interpolation to the position and values of stations, we will obtain comprehensive grid data. How can we analyze the grid data? First, the easiest method is to draw a line connecting nearby points by the values and positions of the grid, which is similar to drawing contours on changing landforms, so that we can consider values on the same line as equivalent.\nfrom osgeo import gdal import numpy as np import matplotlib import matplotlib.pyplot as plt import elevation # 利用 Matplotlib 中的 'contourf' 函式來畫 fig, ax = plt.subplots(figsize=(6, 10)) X = np.linspace(xmin, xmax) Y = np.linspace(ymin, ymax) krig = OrdinaryKriging(x=interpolatep['lon'], y=interpolatep['lat'], z=interpolatep['Observations'], variogram_model=\"spherical\") z, ss = krig.execute(\"grid\", X, Y) im = ax.contourf(z, cmap = \"viridis\", levels = list(range(-30, 30, 10)),extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='black'); plt.title(\"Elevation Contours Taiwan\") plt.show() Profile Contours help us attain value distribution and ranges, and geographical profiling is another method for understanding value distribution. By drawing a straight line between two points, geographical profiling helps us to retrieve corresponding values according to positions of lines for estimation. This method allows us to inspect value variation between two points. In some studies of air quality, scientists evaluate changes of PM2.5 on both sides of roads with geographical profiling.\ndef export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename): '''Export and save a kernel density raster.''' # 取得解析度 xres = (max_x - min_x) / len(XX) yres = (max_y - min_y) / len(YY) # 取得 bound 等資訊 transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres) # 輸出為 raster with rasterio.open( filename, mode = \"w\", driver = \"GTiff\", height = Z.shape[0], width = Z.shape[1], count = 1, dtype = Z.dtype, crs = proj, transform = transform, ) as new_dataset: new_dataset.write(Z, 1) from pykrige.ok import OrdinaryKriging from affine import Affine import rasterio import math start_cor = [119.9,23.2] end_cor = [120.1,23.9] npoints=100 X = np.linspace(xmin, xmax, npoints) Y = np.linspace(ymin, ymax, npoints) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") krig = OrdinaryKriging(x=xd, y=yd, z=zd, variogram_model=\"spherical\") zr, ss = krig.execute(\"grid\", X, Y) # 輸出 raster export_kde_raster(Z = zr, XX = X, YY = Y, min_x = xmin, max_x = xmax, min_y = ymin, max_y = ymax, proj = 4326, filename = \"kriging_result.tif\") kriging = rasterio.open(\"kriging_result.tif\",mode='r') dist = math.sqrt((end_cor[0]-start_cor[0])**2+(end_cor[1]-start_cor[1])**2)*111 npoints=500 lat = np.linspace(start_cor[1], end_cor[1],npoints) lon = np.linspace(start_cor[0], end_cor[0],npoints) distarray = np.linspace(0, dist,npoints) np.append(distarray, dist) df = pd.DataFrame({'Latitude': lat, 'Longtitude':lon,'h_distance':distarray}) df['Observations']=0 gdf_pcs = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longtitude, df.Latitude)) gdf_pcs.crs = {'init':'epsg:4326'} for index, row in gdf_pcs.iterrows(): rows, cols = kriging.index(row['geometry'].x,row['geometry'].y) kri_data = kriging.read(1) df['Observations'].loc[index] = kri_data[rows, cols] profile = df[['h_distance','Observations']] profile.plot(x='h_distance',y='Observations') kriging.close() References Geopandas https://ithelp.ithome.com.tw/articles/10202336 scipy.spatial (https://docs.scipy.org/doc/scipy/reference/spatial.html) scipy.interpolate (https://docs.scipy.org/doc/scipy/reference/interpolate.html) pykrige (https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/api.html#krigging-algorithms) ",
    "description": "We use Civil IoT Taiwan's sensor data to introduce more advanced geospatial analysis. Using the GPS location coordinates in the site information, we first use the package to find the largest convex polygon (Convex Hull) to frame the geographical area covered by the sensor. Then we apply the Voronoi Diagram package to draw the distribution status of each sensor on the map according to the sensor and crops out the area of influence of each sensor. We adopt the spatial interpolation method for the space between sensors, applying different spatial interpolation algorithms. We then populate the spatial map with values based on the sensor values and generate a corresponding image output.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "5.2. Geospatial Analysis",
    "uri": "/en/ch5/ch5.2/"
  },
  {
    "content": "\nTable Of Contents Anomaly Detection Framework Types of abnormal events Possible causes of abnormal events Case Study Package Installation and Importing Initialization and Data Access Find nearby sensors Time slicing every five minutes Average the sensing values of nearby sensors every time slice Abnormal event judgements Implementation of the Malfunction Detection module Implementation of the Real-time Emission Detection module Implementation of the Device Ranking module References Anomaly Detection Framework Multiple large-scale micro air quality monitoring systems have been successfully deployed in different countries and cities. However, one of these micro sensors’ main challenges is ensuring data quality and detecting possible anomalies in real-time. In 2018, the research team of the Network Research Laboratory of the Institute of Information Science, Academia Sinica, Taiwan, proposed an anomaly detection framework that can be used in real environments called the Anomaly Detection Framework (ADF).\nThis anomaly detection framework consists of four modules:\nTime-Sliced Anomaly Detection (TSAD): It can detect abnormal sensor data in space or time in real-time and output the results to other modules for further analysis. Real-time Emission Detection (RED): It can detect potential regional pollution events in real-time through the detection results of TSAD. Sensor reliability evaluation module (Device Ranking, DR): It can accumulate the detection results of TSAD and evaluate the reliability of each miniature sensor device Abnormal use of machine detection module (Malfunction Detection, MD): It can accumulate the detection results of TSAD, and through data analysis, it can identify micro-sensors that may be used abnormally, such as machines installed indoors, placed in continuous Machines next to sources of sexual pollution, etc. Types of abnormal events In the ADF framework, the TSAD module will judge the abnormal event of time or space every time the micro air quality sensor receives new sensing data. We will use the micro air quality sensor as an example to illustrate :\nTime-type abnormal events: We assume that air diffusion is uniform and slow, so the value change of the same micro air quality sensor in a short period should be extremely gentle. There are drastic changes in a short period, which means that abnormal events may occur in the time dimension. Spatial abnormal events: We can assume that the outdoor air will spread evenly in geographical space, so the sensing value of the micro-air sensor should be similar to the surrounding sensors. Suppose there is a considerable difference between the sensing value of one particular sensor and its neighboring at the same time. In that case, an abnormal event may occur in the space where the sensor is located. Possible causes of abnormal events There are many possible causes for the abnormal events described above. Common ones are:\nAbnormal installation environment: The sensor is installed in a specific environment, so it cannot show the overall environmental phenomenon, such as installed next to a temple, in a barbecue shop, or in other indoor places without ventilation. Machine failure or installation error: For example, when the sensor is installed, the direction of the air intake is wrong, or the fan of the sensor is fouled so that the operation is not smooth. A temporary source of contamination occurs: For example, someone smoking, a fire, or emitting pollutants right next to the sensor. Case Study In this article, we will take the Civil IoT Taiwan project air quality data as an example. We will use some campus micro air quality sensors installed in Kaohsiung City for analysis and introduce how to use the ADF detection framework to find potential sensors that may be installed indoors or located near pollution sources. We will also rank the sensors depending on their trustworthiness.\nPackage Installation and Importing In this article, we will use the pandas, numpy, plotly, and geopy packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. We can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import numpy as np import plotly.express as px from geopy.distance import geodesic Initialization and Data Access In this case, we will use some of the micro air quality sensors deployed in Kaohsiung under the Civil IoT Taiwan Project for analysis. We consider the following spatial and temporal ranges of the data:\nSpatial range: latitude in the range of 22.631231 - 22.584989, and longitude in the range of 120.263422 - 120.346764 Temporal range: 2022.10.15 - 2022.10.28 Note that you can go to the Civil IoT Taiwan Data Service Platform to download the raw data of micro air quality sensors on campus. However, for simplicity, we pre-download the data and make it available as allLoc.csv for the examples described in this article.\nWe first load the data file and preview the contents of the data:\nDF = pd.read_csv(\"https://LearnCIOT.github.io/data/allLoc.csv\") DF.head() Then we fetch the GPS geolocation coordinates of each sensor in the data file. Since the GPS coordinates of these sensors will not change, we average each sensor’s longitude and latitude data in the data file as the geographic coordinates of the sensor.\ndfId = DF[[\"device_id\",\"lon\",\"lat\"]].groupby(\"device_id\").mean().reset_index() print(dfId.head()) device_id lon lat 0 74DA38F207DE 120.340 22.603 1 74DA38F20A10 120.311 22.631 2 74DA38F20B20 120.304 22.626 3 74DA38F20B80 120.350 22.599 4 74DA38F20BB6 120.324 22.600 We plot the sensor locations on a map to get an overview of the geographic distribution of sensors in the data file.\nfig_map = px.scatter_mapbox(dfId, lat=\"lat\", lon=\"lon\", color_continuous_scale=px.colors.cyclical.IceFire, zoom=9, mapbox_style=\"carto-positron\") fig_map.show() Find nearby sensors Since the campus micro air quality sensors are fixedly installed, their GPS location coordinates will not change. To save the computing time for subsequent data analysis, we first calculate each sensor’s “neighbor” list in a unified manner. In our case, we define that two tiny sensors become neighbors if their mutual distance is less than or equal to 3 kilometers.\nWe first write a function countDis to calculate the physical kilometer distance between the two input GPS geographic coordinates.\ndef countDis(deviceA, deviceB): return geodesic((deviceA[\"lat\"], deviceB['lon']), (deviceB[\"lat\"], deviceB['lon'])).km Then we convert the sensor list of raw data from DataFrame data type to Dictionary data type and calculate the distance between any two sensors. As long as the distance between the two is less than 3km, we store each other in the neighbor sensor list dicNeighbor.\n# set the maximum distance of two neighbors DISTENCE = 3 ## convert Dataframe -\u003e Dictionary # {iD: {'lon': 0.0, 'lat': 0.0}, ...} dictId = dfId.set_index(\"device_id\").to_dict(\"index\") ## obtain the list of sensor device_id listId = dfId[\"device_id\"].to_list() ## initialize dicNeighbor # {iD: []} dicNeighbor = {iD: [] for iD in listId} ## use countDis to calculate distance of every two sensors # The two sensors are deem to be neighbors of each other if their distance is less than 3km # Time complexity: N! for x in range(len(listId)): for y in range(x+1, len(listId)): if ( countDis( dictId[listId[x]], dictId[listId[y]]) \u003c DISTENCE ): dicNeighbor[listId[x]].append( listId[y] ) dicNeighbor[listId[y]].append( listId[x] ) Time slicing every five minutes Since each sensor in the original data is not synchronized in time, it is proposed in the ADF framework to divide the sensing data at intervals of each unit of time to obtain the time slice of the overall sensing result. We first merge the date and time fields in the original data and store them in the datetime data type of the Python language to form a new datetime field, and then delete unnecessary fields such as date, time, RH, temperature, lat, and lon.\n# combine the 'date' and 'time' columns to a new column 'datetime' DF[\"datetime\"] = DF[\"date\"] + \" \" + DF[\"time\"] # remove some non-necessary columns DF.drop(columns=[\"date\",\"time\", \"RH\",\"temperature\",\"lat\",\"lon\"], inplace=True) # convert the new 'datetime' column to datetime data type DF['datetime'] = pd.to_datetime(DF.datetime) Since the data frequency of the campus micro air quality sensors is about 5 minutes, we set the unit time FREQ of the time slice to 5 minutes and calculate the average value of the sensing values returned by each sensor every 5 minutes. To ensure the correctness of the data, we have also done additional verification and deleted the data with negative PM2.5 sensing values.\nFREQ = '5min' dfMean = DF.groupby(['device_id', pd.Grouper(key = 'datetime', freq = FREQ)]).agg('mean') # remove invalid records (i.e., PM2.5 \u003c 0) dfMean = dfMean[ dfMean['PM2.5'] \u003e= 0 ] print(dfMean.head()) PM2.5 device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 2022-10-15 00:05:00 37.0 2022-10-15 00:10:00 37.0 2022-10-15 00:20:00 36.0 2022-10-15 00:25:00 36.0 Average the sensing values of nearby sensors every time slice To calculate the average sensing value of the neighboring sensors of a specific sensor on a particular time slice, we write the cal_mean function, which can return the number and average sensing values of the neighboring sensors according to the input device number iD and time stamp dt.\ndef cal_mean(iD, dt): neighborPM25 = dfMean[ (dfMean.index.get_level_values('datetime') == dt) \u0026 (dfMean.index.get_level_values(0).isin(dicNeighbor[iD])) ][\"PM2.5\"] avg = neighborPM25.mean() neighbor_num = neighborPM25.count() return avg, neighbor_num Then, for each timestamp of each sensor in dfMean, we calculate the number of neighboring sensors and the average sensing value and store them in two new fields, avg and neighbor_num, respectively. Note that we use the syntax of zip and apply it to bring the values of DataFrame into the function for operation:\nWe use the zip syntax for packaging the two parameter values returned by apply_func. We use the syntax of apply to match the rules of DataFrame and receive the return value of apply_func. def apply_func(x): return cal_mean(x.name[0], x.name[1]) dfMean['avg'], dfMean['neighbor_num'] = zip(*dfMean.apply(apply_func, axis=1)) print(dfMean.head()) PM2.5 avg neighbor_num device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 2022-10-15 00:05:00 37.0 19.888889 9 2022-10-15 00:10:00 37.0 16.500000 12 2022-10-15 00:20:00 36.0 16.750000 8 2022-10-15 00:25:00 36.0 17.000000 11 Abnormal event judgements Through observation, we found that the so-called “abnormal event” refers to the sensing value of a sensor being too far from the reasonable value in our mind. This reasonable value may be the sensing value of the adjacent sensor (spatial type anomaly), the previous sensing value of the same sensor (time type anomaly), or a reasonable estimate based on other information sources. At the same time, we also found that the so-called “too far from the reasonable value” is a very vague term, and its specific value varies significantly with the size of the sensing value.\nTherefore, we first divide the distribution of dfMean['avg'] values into nine intervals and calculate the standard deviation of the PM2.5 sensing value in each interval. Then, we use these values as the threshold values for sensing data judgment, as shown in the table below. For example, when the original value is 10 (ug/m3), if the average value of the surrounding sensors is higher than 10+6.6 or lower than 10-6.6, we will determine that this sensing value is an abnormal event.\nRaw value (ug/m3) threshold 0-11 6.6 12-23 6.6 24-35 9.35 36-41 13.5 42-47 17.0 48-58 23.0 59-64 27.5 65-70 33.5 71+ 91.5 According to this table, we write the following function THRESHOLD to return the corresponding threshold value according to the input sensing value, and store this threshold value in the new column PM_thr of dfMEAN.\ndef THRESHOLD(value): if value\u003c12: return 6.6 elif value\u003c24: return 6.6 elif value\u003c36: return 9.35 elif value\u003c42: return 13.5 elif value\u003c48: return 17.0 elif value\u003c54: return 23.0 elif value\u003c59: return 27.5 elif value\u003c65: return 33.5 elif value\u003c71: return 40.5 else: return 91.5 dfMean['PM_thr'] = dfMean['PM2.5'].apply(THRESHOLD) Since the last record time of the original data is 2022-10-28 23:45, we set the judgment time as 2022-10-29. Then, we created a new column, day, to represent the difference between each record in the original data and the judgment time of the data. We preview the current state of the dfMEAN data table below.\nTARGET_DATE = \"2022-10-29\" dfMean = dfMean.assign(days = lambda x: ( (pd.to_datetime(TARGET_DATE + \" 23:59:59\") - x.index.get_level_values('datetime')).days ) ) print(dfMean.head()) PM2.5 avg neighbor_num PM_thr days device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 13.5 14 2022-10-15 00:05:00 37.0 19.888889 9 13.5 14 2022-10-15 00:10:00 37.0 16.500000 12 13.5 14 2022-10-15 00:20:00 36.0 16.750000 8 13.5 14 2022-10-15 00:25:00 36.0 17.000000 11 13.5 14 Implementation of the Malfunction Detection module In the following example, we implement the Malfunction Detection module in ADF. The core concept is that if we compare the PM2.5 value of a certain micro-sensor with other sensors within 3 kilometers around, A machine is considered an indoor machine (labeled indoor) if its sensed value is below the average value (avg) of neighboring sensors minus the acceptable threshold (PM_thr). A machine is considered to be installed next to a pollution source (labeled emission) if its sensed value is higher than the average value (avg) of neighboring sensors plus an acceptable threshold (PM_thr). To avoid misjudgment due to insufficient neighboring sensors, we only take cases with more than two other sensors in the neighboring area.\nMINIMUM_NEIGHBORS = 2 dfMean[\"indoor\"] = ((dfMean['avg'] - dfMean['PM2.5']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean[\"emission\"] = ((dfMean['PM2.5'] - dfMean['avg']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean We can observe from the results that indoor and emission judgment results are likely to be different due to different daily air quality conditions. Thus, to obtain a more convincing judgment result, we consider the three time lengths of the past one day, seven days, and 14 days to calculate the ratios of each sensor’s judgment results (indoor or emission). At the same time, to avoid the misjudgment caused by the external environment from affecting the final judgment result, we forcibly modified the calculation result, and we changed the value with a ratio of less than 1/3 to 0.\n# initialize dictIndoor = {iD: [] for iD in listId} dictEmission = {iD: [] for iD in listId} for iD in listId: dfId = dfMean.loc[iD] for day in [1, 7, 14]: indoor = (dfId[ dfId['days'] \u003c= day]['indoor'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictIndoor[iD].append( indoor if indoor \u003e 0.333 else 0 ) emission = (dfId[ dfId['days'] \u003c= day]['emission'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictEmission[iD].append( emission if emission \u003e 0.333 else 0 ) We then print out the contents of dictIndoor and dictEmission, respectively, and observe the judged results.\ndictIndoor {'74DA38F207DE': [0, 0, 0], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0.995, 0.86, 0.816], '74DA38F20B80': [0.995, 0.872, 0.867], '74DA38F20BB6': [0, 0, 0], '74DA38F20C16': [0.989, 0.535, 0], '74DA38F20D7C': [0, 0, 0], '74DA38F20D8A': [0, 0, 0], '74DA38F20DCE': [0, 0, 0], '74DA38F20DD0': [0.984, 0.871, 0.865], '74DA38F20DD8': [0, 0.368, 0.369], '74DA38F20DDC': [0, 0, 0], '74DA38F20DE0': [0, 0, 0], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0, 0, 0], '74DA38F20E42': [0.99, 0.866, 0.87], '74DA38F20E44': [0, 0, 0], '74DA38F20F0C': [0.979, 0.872, 0.876], '74DA38F20F2C': [0, 0, 0], '74DA38F210FE': [0.99, 0.847, 0.864]} dictEmission {'74DA38F207DE': [0.92, 0.737, 0.735], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0, 0, 0], '74DA38F20B80': [0, 0, 0], '74DA38F20BB6': [0.492, 0.339, 0], '74DA38F20C16': [0, 0, 0], '74DA38F20D7C': [0.553, 0.342, 0.337], '74DA38F20D8A': [0.672, 0.457, 0.388], '74DA38F20DCE': [0.786, 0.556, 0.516], '74DA38F20DD0': [0, 0, 0], '74DA38F20DD8': [0, 0, 0], '74DA38F20DDC': [0.345, 0, 0], '74DA38F20DE0': [0.601, 0.503, 0.492], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0.938, 0.75, 0.75], '74DA38F20E42': [0, 0, 0], '74DA38F20E44': [0.938, 0.69, 0.6], '74DA38F20F0C': [0, 0, 0], '74DA38F20F2C': [0.744, 0.575, 0.544], '74DA38F210FE': [0, 0, 0]} From the above results, it can be found that with the length of the past reference time, there will be some differences in the indoor and emission judgment results. Since different lengths of time represent different reference meanings, we adopt a weighting method, assigning weight to 1-day reference time A, weighting to 7-day reference time B, and weighting to 14-day reference time 1-A-B.\nAt the same time, we consider working about 8 hours out of 24 hours in a day, 40 hours out of 168 hours in seven days, and 80 hours out of 336 hours in 14 days. Therefore, if a sensor is judged as indoor or emission type, its weighted proportion should be greater than or equal to MD_thresh, and\nMD_thresh = (8.0/24.0)*A+(40.0/168.0)B+(80.0/336.0)(1-A-B)\nIn our example, we assume A=0.2 and B=0.3, and we can obtain the weighted indoor and emission sensor list through the following codes.\nA=0.2 B=0.3 MD_thresh=(8.0/24.0)*A+(40.0/168.0)*B+(80.0/336.0)*(1-A-B) listIndoorDevice = [] listEmissionDevice = [] for iD in listId: rate1 = A*dictIndoor[iD][0] + B*dictIndoor[iD][1] + (1-A-B)*dictIndoor[iD][2] if rate1 \u003e MD_thresh: listIndoorDevice.append( (iD, rate1) ) rate2 = A*dictEmission[iD][0] + B*dictEmission[iD][1] + (1-A-B)*dictEmission[iD][2] if rate2 \u003e MD_thresh: listEmissionDevice.append( (iD, rate2) ) We then print out the contents of listIndoorDevice and listEmissionDevice, respectively, and observe the results of the weighted judgment.\nlistIndoorDevice [('74DA38F20B20', 0.865), ('74DA38F20B80', 0.8941), ('74DA38F20C16', 0.3583), ('74DA38F20DD0', 0.8906), ('74DA38F20DD8', 0.2949), ('74DA38F20E42', 0.8928), ('74DA38F20F0C', 0.8954), ('74DA38F210FE', 0.8841)] listEmissionDevice [('74DA38F207DE', 0.7726), ('74DA38F20D7C', 0.38170000000000004), ('74DA38F20D8A', 0.4655), ('74DA38F20DCE', 0.5820000000000001), ('74DA38F20DE0', 0.5171), ('74DA38F20E0E', 0.7876), ('74DA38F20E44', 0.6945999999999999), ('74DA38F20F2C', 0.5933)] Implementation of the Real-time Emission Detection module For real-time emission detection (RED), we assume that if the latest sensing value of a tiny sensor is 1/5 more than the previous sensing value, the surrounding environment has undergone drastic changes, which is very likely due to the emission of air pollution around. Since the change of 1/5 is still very slight in the actual concentration change of PM2.5 when the sensing value is small, we exclude the situation that the sensing value is less than 20 to avoid the error caused by the sensor, leading to misjudgment of real-time pollution detection.\ndfMean['red'] = False for iD in listId: dfId = dfMean.loc[iD] p_index = '' p_row = [] for index, row in dfId.iterrows(): red = False if p_index: diff = row['PM2.5'] - p_row['PM2.5'] if p_row['PM2.5']\u003e20 and diff\u003ep_row['PM2.5']/5: red = True dfMean.loc[pd.IndexSlice[iD, index.strftime('%Y-%m-%d %H:%M:%S')], pd.IndexSlice['red']] = red p_index = index p_row = row dfMean Implementation of the Device Ranking module Finally, we summarize the judgment results of the RED and MD modules and conduct a reliability evaluation (Device Ranking, DR) for the microsensor. The main concepts are:\nIf it is often judged that the sensing data of the sensor is abnormal in time or space, it indicates that there may be potential problems in the hardware or environment of the sensor, which needs to be further clarified. If the sensing data of the sensor is seldom judged to be abnormal, it means that the sensing value of the sensor is highly consistent with the values of the surrounding sensors, so the reliability is high. We calculate the total number of time (red=True) or space (indoor=True or emission=True) anomalies for each sensor per day based on all the sensor data per day. Then we calculate its proportion of all data items in a day as sensor information Basis for reliability assessment.\ndevice_rank = pd.DataFrame() for iD in listId: dfId = dfMean.loc[iD] abnormal = {} num = {} for index, row in dfId.iterrows(): d = index.strftime('%Y-%m-%d') if d not in abnormal: abnormal[d] = 0 if d not in num: num[d] = 0 num[d] = num[d] + 1 if row['indoor'] or row['emission'] or row['red']: abnormal[d] = abnormal[d] + 1 for d in num: device_rank = device_rank.append(pd.DataFrame({'device_id': [iD], 'date': [d], 'rank': [1 - abnormal[d]/num[d]]})) device_rank.set_index(['device_id','date']) References Ling-Jyh Chen, Yao-Hua Ho, Hsin-Hung Hsieh, Shih-Ting Huang, Hu-Cheng Lee, and Sachit Mahajan. ADF: an Anomaly Detection Framework for Large-scale PM2.5 Sensing Systems. IEEE Internet of Things Journal, volume 5, issue 2, pp. 559-570, April, 2018. (https://dx.doi.org/10.1109/JIOT.2017.2766085) ",
    "description": "We use air quality data to demonstrate the anomaly detection framework commonly used in Taiwan's micro air quality sensing data. We learn by doing, step by step, from data preparation and feature extraction to data analysis, statistics, and induction. The readers will experience how to gradually achieve advanced and practical data application services by superimposing basic data analysis methods.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "6.2. Anomaly Detection",
    "uri": "/en/ch6/ch6.2/"
  },
  {
    "content": " Table Of Contents Goal Data Source Tableau Basic Operation Data Import Worksheet Introduction Tableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Time Series Graph Tableau Example 2: Disaster Notification Dashboard Disaster data format conversion Dashboard size Add worksheets to a dashboard Add interactions Interacting multiple worksheet Additional information Story Conclusion References Data visualization is a method of expressing data in a graphical way, which can help us to have a better understanding of the data. When visualizing data, different types of graphs are suitable for different types of data. For example, when presenting data with latitude and longitude coordinates, map-type charts are mostly used; when presenting time series data, line charts or histograms can be used. However, many real-world data often have more than one characteristic. For example, the sensor data of Civil IoT Taiwan has both latitude and longitude coordinates and time series characteristics. Therefore, when presenting these data, it is often necessary to use more than one kind of chart to present the insight of the data. In this chapter, we will introduce a very popular software, Tableau, to help us present the above-mentioned complex data types.\nTableau is an easy-to-use visual analysis platform. In addition to quickly creating various charts, its dashboard function makes it easier for users to present and integrate different data charts. In addition, dashboards allow users to interact with graphs, which not only helps users understand data more easily and quickly, but also makes data analysis easier. Tableau was originally a paid commercial software, but it also provides a free version of Tableau Public for everyone to use. Note, however, that when using the free version of Tableau, all work produced with it must be made public.\nNote Note: The Tableau Public version used in this article is Tableau Desktop Public Edition (2022.2.2 (20222.22.0916.1526) 64-bit), but the functions used in this article are the basic functions of the software. If you use other versions of the software, you should still be able to run it normally.\nGoal Import data into Tableau for data visualization Create data charts using Worksheets Create interactive charts and reports using dashboards and stories in Tableau Public Data Source Civil IoT Taiwan Historical Data: Academia Sinica - Micro Air Quality Sensors (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器) Civil IoT Taiwan Historical Data: National Fire Agency - Disaster Notifications (https://history.colife.org.tw/#/?cd=%2F災害示警與災情通報%2F消防署_災情通報) Tableau Basic Operation Data Import Tableau can read text files (csv) and spatial information files (shp, geojson). To import data, click Data \u003e Add Data Source to select the data file to be imported.\nDepending on the type of input file, we provide different examples of operations below:\nExamples using geospatial data format (shp、geojson)\nFirst, please click the data source in the lower left corner, and then you can see the data fields imported now. ![Tableau screenshot](figures/7-2-3-2.png) Then click the symbol above the field name to change the properties of that field.\nFinally, we assign the latitude and longitude coordinates to geographic roles for subsequent work.\nExamples using test data format (CSV)\nAfter importing the PM 2.5 records and station locations into Tableau, we establish the relationship between the data tables:\nNext we establish the link between the PM 2.5 station and the station coordinates. We first click the station data twice to enter the connection canvas, and drag another data table to the connection canvas.\nFinally, we click the link icon in the middle to set the link type and operation logic.\nWhen the setting is completed, you can see that the Station id in the lower right corner is from the Location data table, so that the PM 2.5 data and the station location data are linked together.\nWorksheet Introduction A worksheet is where Tableau can use to visualize data. In addition to plotting data into traditional graphs such as pie, bar, and line graphs, it can also be used to map geographic information. Next we will show how to draw different shapes.\nAfter processing the data, click the new worksheet in the lower left, and you can see the interface below after entering the worksheet.\nThere is a “Show” button in the upper right corner of the worksheet. After clicking, you will see different types of charts (the system will automatically determine the charts that can be drawn, and those that cannot be drawn will be highlighted), and then you can click the desired type to change the chart.\nTableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Before starting to draw, we need to change the latitude and longitude from a measure to a dimension. For a detailed explanation of the measure and dimension, please refer to the reference materials. The detailed operation process is shown in the following animation:\nThen we drag the latitude and longitude to the position of the column and row, and drag the PM 2.5 value to the marked position. Then we change the mark to color to generate the distribution map of PM 2.5 in Taiwan, as shown in the following animation.\nWe can click on the color to change the color of the dots, and the filter above can filter the PM 2.5 value for a specific condition. If you click PM 2.5 \u003e Metric, you can select the mean, sum, or median value of PM 2.5. The following will demonstrate how to display the PM 2.5 value for New Taipei City.\nDrag “Site Name” to filter Click on wildcards, click “Include” and enter New Taipei City Click “OK” Time Series Graph We drag “Timestamp” to the column and “Site Name” and PM 2.5 value to the row. Then we click “Timestamp” to set the time interval. For example, we set the interval to 1 hour in the image below. Note that, similar to plotting the spatial distribution, the filter function can also be used to filter the stations to be displayed. In the animation below, we demonstrate how to display the monitoring values of the Kaohsiung PM2.5 station throughout the day.\nTableau Example 2: Disaster Notification Dashboard Dashboards can combine different worksheets (charts) to present richer information and allow users to interact with thedata. In the following example, we will introduce how to create a simple dashboard using the rainstorm disaster data provided by the Civil IoT Taiwan project.\nDisaster data format conversion We use the 823 flood that occurred on 2018/08/23 as an example. Since the original data format is an xml file, we first use the following URL to convert the original data to a csv file: https://www.convertcsv.com/xml-to-csv.htm (animated below)\nDashboard size After clicking Dashboard in the Tableau menu, you can set the size of the dashboard in the tool list on the left.\nAdd worksheets to a dashboard Then you can see the previously created worksheet in the tool list on the left, you can add it to the dashboard by dragging and dropping the worksheet to the empty space of the dashboard.\nAfter Tableau receives the data of the worksheet, it will automatically read the information of the content and automatically generate the corresponding initial chart.\nThe worksheet you just dragged in has a fixed size and cannot be changed at will. There is a downward arrow at the top right of the worksheet, and you can change the size of the graph by clicking and selecting “Float”.\nAdd interactions Next, we demonstrate how to provide an interactive interface that allows users to select the point in time in the disaster data that they want to observe.\nWe start by clicking the down arrow in the upper right corner of the sheet, then selecting Filters, then Case Time.\nAfter clicking, you will see a selection field pop up in the worksheet where you can check the date. At this time, the user only needs to check the date they want to observe, and then they can see the disaster situation on that day.\nIf we want to change the date selection method, we can also click the down key at the top right of the selection field to change the style of the selection field.\nFor example, we can change the selection method from the original list method to the slider method as follows.\nInteracting multiple worksheet The above interaction buttons can only be used for one worksheet. If you want multiple worksheets on the dashboard to share the same interactive button at the same time, you can refer to the following methods:\nAccording to the above method, first create an interactive button field;\nClick the down button at the top right of the interactive button field, select Apply to worksheet and click the selected worksheet;\nSelect the worksheet to apply to complete the setting.\nAdditional information If you need to add other information on the dashboard, you can find an object column at the bottom left of the toolbar, which contains text, pictures and other objects. Just drag and drop the required objects to the top of the dashboard to display them in the dashboard, and then add text, pictures and other objects.\nStory A story is a combination of multiple dashboards or sheets used to create a slideshow-like display. Adding a story to a dashboard or sheet is the same as adding a sheet from a dashboard, just drag the sheet or dashboard created on the right to the empty space of the story.\nIf you need to add a new story page, you can click New Story at the top to add a new blank page, or copy an existing page to become a new page.\nFinally, to make the text information better fit its content, we can modify the title of the page by double-clicking the box above the text page.\nConclusion In this chapter, we briefly introduce the basic operations of Tableau, and use Tableau to design presentations/charts that interact with users. However, Tableau’s functions are far more than these, and there are many classic examples created using Tableau on the Internet. If you are interested in Tableau, you can refer to additional resources below.\nReferences Tableau Public (https://public.tableau.com/) Tableau - About data field roles and types (https://help.tableau.com/current/pro/desktop/en-us/datafields_typesandroles.htm) Get Started with Tableau (https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-home.htm) Tableau Tutorial — Learn Data Visualization Using Tableau (https://medium.com/edureka/tableau-tutorial-71ef4c122e55) YouTube: Tableau For Data Science 2022 (https://www.youtube.com/watch?v=Wh4sCCZjOwo) YouTube: Tableau Online Training (https://www.youtube.com/watch?v=ttCDqyfrcEc) ",
    "description": "We introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.2. Tableau Application",
    "uri": "/en/ch7/ch7.2/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access and Preprocessing Data Clustering Fast Fourier Transform Wavelet Transform References Cluster analysis is a commonly used data processing method in data science, and its primary purpose is to find similar clusters in the data. After cluster analysis, data with similar attributes are clustered together for ease of use, and researchers can perform deeper analysis and processing on data with similar characteristics. In Civil IoT Taiwan, since the data of each sensor is time series data, to properly group a large number of sensors for more in-depth data analysis, we introduce standard feature extraction methods and for clustering time series data.\nGoal Learn to use Fast Fourier Transform (FFT) and Wavelet Transform to extract features of time series data Use unsupervised learning method to cluster time series data Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, and pywt packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use one additional package that Colab does not have pre-installed, tslearn, which need to be installed by :\n!pip install --upgrade pip !pip install tslearn After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport numpy as np import pandas as pd import pywt import os, zipfile import matplotlib.pyplot as plt from datetime import datetime, timedelta from numpy.fft import fft, ifft from pywt import cwt from tslearn.clustering import TimeSeriesKMeans Data Access and Preprocessing Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the 2021 data archive of “Academia Sinica - Micro Air Quality Sensors” in 2021 from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\n!mkdir Air CSV_Air !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" !unzip Air/2021.zip -d Air At the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily files in 2021/08 and store the content of the decompressed csv files in the air_month dataframe.\nfolder = 'Air/2021/202108' extension_zip = 'zip' extension_csv = 'csv' for item in os.listdir(folder): if item.endswith(extension_zip): file_name = f'{folder}/{item}' zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(folder) zip_ref.close() air_month = pd.DataFrame() for item in os.listdir(folder): if item.endswith(extension_csv): file_name = f'{folder}/{item}' df = pd.read_csv(file_name, parse_dates=['timestamp']) air_month = air_month.append(df) air_month.set_index('timestamp', inplace=True) air_month.sort_values(by='timestamp', inplace=True) At present, the format of air_month does not meet our needs. It needs to be sorted into a data format with site data as columns and time data as columns. Thus, we first find out the number of distinct sites in the data, and store the site information in a sequence.\nid_list = air_month['device_id'].to_numpy() id_uniques = np.unique(id_list) id_uniques array(['08BEAC07D3E2', '08BEAC09FF12', '08BEAC09FF22', ..., '74DA38F7C648', '74DA38F7C64A', '74DA38F7C64C'], dtype=object) Then we save the data of each site as a column and put them all in the air dataframe. Finally, we delete all downloaded data and unpacked data to save cloud storage space.\nair = pd.DataFrame() for i in range(len(id_uniques)): # print('device_id==\"' + id_uniques[i] + '\"') query = air_month.query('device_id==\"' + id_uniques[i] + '\"') query.sort_values(by='timestamp', inplace=True) query_mean = query.resample('H').mean() query_mean.rename(columns={'PM25': id_uniques[i]}, inplace=True) air = pd.concat([air, query_mean], axis=1) !rm -rf Air We can quickly view the contents of air using the following syntax.\nair.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 745 entries, 2021-08-01 00:00:00 to 2021-09-01 00:00:00 Freq: H Columns: 1142 entries, 08BEAC07D3E2 to 74DA38F7C64C dtypes: float64(1142) memory usage: 6.5 MB 08BEAC07D3E2 08BEAC09FF12 08BEAC09FF22 08BEAC09FF2A \\ timestamp 2021-08-01 00:00:00 2.272727 1.250000 16.818182 12.100000 2021-08-01 01:00:00 1.909091 1.285714 13.181818 14.545455 2021-08-01 02:00:00 2.000000 1.125000 12.727273 16.600000 2021-08-01 03:00:00 2.083333 3.000000 11.800000 12.090909 2021-08-01 04:00:00 2.000000 2.600000 10.090909 8.545455 08BEAC09FF34 08BEAC09FF38 08BEAC09FF42 08BEAC09FF44 \\ timestamp 2021-08-01 00:00:00 2.727273 0.000000 1.181818 NaN 2021-08-01 01:00:00 2.000000 0.545455 0.909091 NaN 2021-08-01 02:00:00 4.090909 1.583333 0.636364 NaN 2021-08-01 03:00:00 0.545455 1.454545 1.181818 NaN 2021-08-01 04:00:00 1.363636 1.363636 2.454545 NaN 08BEAC09FF46 08BEAC09FF48 ... 74DA38F7C62A \\ timestamp ... 2021-08-01 00:00:00 2.636364 2.545455 ... 6.777778 2021-08-01 01:00:00 1.636364 2.272727 ... 7.800000 2021-08-01 02:00:00 1.400000 3.100000 ... 7.300000 2021-08-01 03:00:00 2.181818 4.000000 ... 12.000000 2021-08-01 04:00:00 1.909091 2.100000 ... 9.000000 74DA38F7C630 74DA38F7C632 74DA38F7C634 74DA38F7C63C \\ timestamp 2021-08-01 00:00:00 9.800000 13.200000 5.0 6.200000 2021-08-01 01:00:00 13.000000 15.700000 5.2 6.800000 2021-08-01 02:00:00 12.800000 19.300000 5.0 7.300000 2021-08-01 03:00:00 8.444444 15.200000 5.1 6.777778 2021-08-01 04:00:00 6.500000 10.222222 4.9 6.100000 74DA38F7C63E 74DA38F7C640 74DA38F7C648 74DA38F7C64A \\ timestamp 2021-08-01 00:00:00 NaN 7.500000 5.000000 9.000000 2021-08-01 01:00:00 NaN 10.200000 4.900000 6.600000 2021-08-01 02:00:00 NaN 10.500000 3.666667 7.600000 2021-08-01 03:00:00 NaN 8.500000 8.400000 8.000000 2021-08-01 04:00:00 NaN 5.571429 6.200000 5.666667 74DA38F7C64C timestamp 2021-08-01 00:00:00 7.600000 2021-08-01 01:00:00 7.700000 2021-08-01 02:00:00 7.888889 2021-08-01 03:00:00 6.400000 2021-08-01 04:00:00 5.000000 Next, we delete the part of the data that has missing values (the value is Nan) and draw the data into a graph to observe the data distribution.\nair = air[:-1] air_clean = air.dropna(1, how='any') air_clean.plot(figsize=(20, 15), legend=None) Since the instantaneous value of the sensor in the original data is prone to sudden and dramatic changes due to environmental changes, we use a moving average method to average the sensing values every ten times so that the processed data can be smoother and more stable. It can also better reflect the overall trend around the sensor and facilitate the following cluster analysis.\nair_clean = air_clean.rolling(window=10, min_periods=1).mean() air_clean.plot(figsize=(20, 15), legend=None) Data Clustering Since the current data representation in the dataframe is to put each station’s data in a separate column, and each row stores the sensor values of all stations at a specific time point, this format is more suitable for time-series data processing and analysis. Still, it is not ideal for data grouping. Therefore, we need to exchange the columns and columns of the data first; that is, we need to transpose the existing data before proceeding to the data grouping.\nData clustering is a prevalent method in data science. Among many data clustering methods, we choose the KMeans method (TimeSeriesKMeans) provided by the tslearn package. In machine learning, this type of method is classified as an “unsupervised learning” method because, in the process of clustering, there is no specific standard to organize the data into a particular cluster, but only the similarity between the data is used to determine the clustering. Thus, it is beneficial for finding outliers or performing predictions.\nThe operation of the KMeans clustering method is roughly divided into the following steps:\nDetermine the value of k, that is, the final number of clusters; Randomly select k records as the center points of the initial k clusters (also called “cluster heads”); Calculate the distance from each record to each cluster head according to the distance formula, and select the nearest cluster head to attribute the record to this group; Recalculate its new cluster head for each cluster, and repeat the above steps until the cluster heads of k clusters no longer change. We first set k=10, and use TimeSeriesKMeans to divide the data into 10 clusters (0~9) as follows:\nair_transpose = air_clean.transpose() model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) # n_cluster:分群數量, max_iter: 分群的步驟最多重複幾次 pre = model.fit(air_transpose) pre.labels_ array([3, 3, 6, 3, 3, 3, 3, 6, 9, 6, 3, 6, 3, 3, 3, 3, 3, 1, 3, 3, 6, 3, 3, 3, 3, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 9, 3, 3, 3, 3, 3, 6, 3, 2, 6, 3, 2, 3, 3, 3, 6, 9, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 6, 3, 3, 3, 3, 3, 6, 3, 6, 3, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 6, 9, 6, 7, 3, 2, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 6, 6, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 6, 3, 3, 3, 0, 6, 0, 6, 3, 0, 3, 3, 3, 6, 6, 6, 6, 3, 3, 6, 3, 3, 9, 6, 3, 6, 3, 6, 6, 3, 2, 3, 3, 3, 6, 9, 6, 6, 3, 3, 6, 3, 0, 3, 3, 6, 6, 6, 3, 0, 0, 6, 3, 6, 6, 6, 3, 6, 6, 3, 0, 3, 3, 0, 3, 3, 6, 3, 6, 6, 6, 3, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 9, 9, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 9, 0, 9, 0, 0, 0, 9, 9, 0, 9, 0, 0, 0, 5, 0, 0, 0, 0, 9, 9, 0, 1, 0, 9, 2, 9, 9, 0, 0, 5, 0, 9, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 2, 3, 3, 0, 3, 0, 0, 0, 6, 3, 0, 8, 9, 3, 9, 9, 0, 5, 0, 3, 0, 9, 9, 5, 5, 5, 3, 5, 9, 3, 0, 0, 3, 0, 0, 0, 3, 5, 5, 3, 0, 5, 6, 5, 5, 5, 0, 9, 0, 6, 6, 6, 3, 0, 3, 5, 5, 9, 6, 3, 0, 0, 0, 0, 3, 5, 0, 5, 5, 0, 3, 3, 9, 5, 5, 9, 5, 9, 9, 9, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5]) The result of clustering is that a small number of sensors may form a cluster by themselves due to the particularity of the data. These sensors can be regarded as deviated stations with no value for further analysis. Therefore, our next step is first to count whether each separated cluster has only one internal sensor and discard the cluster. For example, in this example, we will discard clusters with only one sensor in them, as follows:\n# build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_clean.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() clusters_dropped [7, 4, 8] Finally, we display the remaining clusters graphically, and it can be found that the data in each cluster show a very high degree of similarity. In contrast, the data in different clusters show apparent differences. To quantify data similarity in clusters, we define a new variable quality. The value of this variable is equal to the average value of the correlation between each internal data and other data, and the lower the quality value, the more similar the internal data of this cluster.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_clean[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_clean[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() So far, we have demonstrated how to use air quality sensor data to cluster the data based on the characteristic information in the data. However, since the original data may contain noise interference, it will also affect the clustering results. To reduce noise interference in raw data, we introduce two commonly used advanced methods: Fourier transform, and wavelet transform. These two methods can extract advanced time series data features, so they can more effectively improve the effectiveness of clustering.\nFast Fourier Transform Fourier transform is a commonly used signal analysis method, which can convert the original data from the time domain to the frequency domain, which is convenient for further feature extraction and data analysis. It is commonly used in engineering and mathematics and in studying sound and time series data. Since the standard Fourier transform involves complicated mathematical operations, it is complex and time-consuming to implement. Therefore, a fast Fourier transform method to discretize the original data was developed later, which can significantly reduce the computational complexity of the Fourier transform and is especially suitable for large amounts of data. Therefore, in our next topic, we will use the Fast Fourier Transform method to extract the data features required for data grouping.\nWe first observe the temporal variation of a single station (’’08BEAC07D3E2’’) by plotting.\nair_clean['08BEAC07D3E2'].plot() Then we use the fast Fourier transform tool fft in the NumPy package to convert the station’s data input and observe the data distribution after conversion to the frequency domain by drawing.\nX = fft(air_clean['08BEAC07D3E2']) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] plt.figure(figsize = (12, 6)) plt.plot(f_oneside, np.abs(X[:n_oneside]), 'b') plt.xlabel('Freq (Hz)') plt.ylabel('FFT Amplitude |X(freq)|') plt.show() Then we extend the same transform step to all sensors and store the fast Fourier transform results for all sensors in the air_fft variable.\nair_fft = pd.DataFrame() col_names = list(air_clean.columns) for name in col_names: X = fft(air_clean[name]) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] air_fft[name] = np.abs(X[:n_oneside]) print(air_fft.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 12019.941563 11255.762431 13241.408211 12111.740447 11798.584351 1 3275.369377 2640.372699 1501.672853 3096.493565 3103.006928 2 1109.613670 1201.362571 257.659947 1085.384353 1128.373457 3 2415.899146 1631.128345 888.838822 2281.597031 2301.936400 4 1130.973327 446.032078 411.940722 1206.512460 1042.054041 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 12441.845754 11874.390693 12259.096742 3553.255916 13531.649701 ... 1 3262.357287 2999.042917 1459.720167 1109.764942 646.846038 ... 2 1075.877632 1005.445596 478.569869 368.572815 1163.425916 ... 3 2448.646527 2318.870954 956.029693 272.486798 553.409732 ... 4 1087.461354 1172.755489 437.920193 471.824176 703.557830 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 11502.841221 10589.689400 11220.068048 10120.220198 11117.146124 1 2064.762890 1407.105290 1938.647888 1126.088084 2422.787262 2 2163.528535 1669.014077 2054.586664 1759.326882 1782.523300 3 1564.407983 1157.759192 1253.849261 1244.799149 1519.477057 4 1484.232397 1177.909914 1318.704021 1106.349846 1373.167639 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 11243.094213 26.858333 9408.414826 11228.246949 8931.871618 1 2097.343959 15.020106 1667.485473 1687.251179 1395.239491 2 1806.524987 10.659603 1585.987276 1851.628868 1527.925427 3 1521.392873 6.021244 1217.547879 1240.173667 1022.239794 4 1393.469185 3.361938 1161.975844 1350.756178 1051.434944 [5 rows x 398 columns] Then we use the same method to divide the sensor data transformed into the frequency domain into 10 clusters using TimeSeriesKMeans, and delete the clusters with only a single sensor after clustering. In this example, there will be 9 clusters left, and we print the cluster code to which each sensor belongs.\nfft_transpose = air_fft.transpose() model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) pre = model.fit(fft_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_fft.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 0 3 08BEAC09FF42 7 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 0 8 08BEAC09FF8C 2 9 08BEAC09FF9C 8 Finally, we plot the sensor data for the nine clusters individually. It can be found that the data of sensors in each cluster are very similar in the frequency domain, and the frequency domain difference of sensors in different clusters is also more significant than that of sensors in the same cluster.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) for idx, cluster_number in enumerate(clusters_final): x_corr = air_fft[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_fft[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() Wavelet Transform Besides Fourier transform, wavelet transform is another commonly used method to convert raw data from the time domain to the frequency domain. The wavelet transform can provide more observation angles for frequency domain data than the Fourier transform. Therefore, recently, wavelet transform has been widely used in the analysis of video, audio, time series, and other related data and has achieved good results.\nWe use the pywt package for wavelet transform data processing. Since the wavelet transform will use the mother wavelet to extract the features in the time series data, we first use the following syntax to check the name of the mother wavelet that can be used.\nwavlist = pywt.wavelist(kind=\"continuous\") print(wavlist) ['cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'cmor', 'fbsp', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'shan'] Unlike the Fourier transform, which can only see changes in frequency, the wavelet transform can scale a finite mother wavelet to extract features from a data segment. When choosing the mother wavelet, you can draw it and observe it before deciding. If we take the morl mother wavelet as an example, we can use the following syntax to construct the graph.\nwav = pywt.ContinuousWavelet(\"morl\") scale = 1 int_psi, x = pywt.integrate_wavelet(wav, precision=10) int_psi /= np.abs(int_psi).max() wav_filter = int_psi[::-1] nt = len(wav_filter) t = np.linspace(-nt // 2, nt // 2, nt) plt.plot(t, wav_filter.real) plt.ylim([-1, 1]) plt.xlabel(\"time (samples)\") We first set the parameters of the wavelet transform and the mother wavelet as morl. For the selected sensor, we set the zoom range of the morl wavelet to 1~31 times and carried out wavelet transformation and plotting.\nF = 1 #Samples per hour hours = 744 nos = np.int(F*hours) #No of samples in 31 days x = air_clean['08BEAC09FF2A'] scales = np.arange(1, 31, 1) coef, freqs = cwt(x, scales, 'morl') # scalogram plt.figure(figsize=(15, 10)) plt.imshow(abs(coef), extent=[0, 744, 30, 1], interpolation='bilinear', cmap='viridis', aspect='auto', vmax=abs(coef).max(), vmin=abs(coef).min()) plt.gca().invert_yaxis() plt.yticks(np.arange(1, 31, 1)) plt.xticks(np.arange(0, nos/F, nos/(20*F))) plt.ylabel(\"scales\") plt.xlabel(\"hour\") plt.colorbar() plt.show() In general, we can find that the larger the scale in the picture, the more yellow-green the color, indicating that the extracted features are closer to the mother wavelet and vice versa. The characteristics of these comparison results will be used for subsequent data grouping.\nHowever, after wavelet transform, each site’s data are transformed into two-dimensional feature values. Before starting to group the data, we need to splice the fields of the original two-dimensional data into one dimension and store it in the air_cwt variable.\nair_cwt = pd.DataFrame() scales = np.arange(28, 31, 2) col_names = list(air_clean.columns) for name in col_names: coef, freqs = cwt(air_clean[name], scales, 'morl') air_cwt[name] = np.abs(coef.reshape(coef.shape[0]*coef.shape[1])) print(air_cwt.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 0.778745 6.336664 2.137342 1.849035 0.778745 1 0.929951 8.348031 2.977576 1.829540 0.958915 2 1.190476 11.476048 3.223773 1.832544 1.292037 3 1.227021 12.890554 4.488244 1.634717 1.338655 4 1.252126 14.103178 5.715656 1.430744 1.376562 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 0.555941 0.334225 8.110581 2.287378 0.409913 ... 1 0.596532 0.201897 8.774271 1.706411 0.311242 ... 2 0.771641 0.294242 8.327808 1.018296 2.754100 ... 3 0.713931 0.065011 8.669036 0.249029 3.278048 ... 4 0.670639 0.193083 8.795376 0.624988 3.628597 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 1.875873 1.090635 0.284901 1.111999 1.049112 1 0.643478 1.446061 0.420701 0.974641 0.478931 2 1.101801 2.404254 1.389941 1.190141 0.511454 3 2.370436 2.510728 1.927692 0.703424 1.054164 4 3.563873 2.387982 2.463458 0.098485 1.448078 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 1.107551 0.000918 0.080841 0.859855 0.415574 1 0.245802 0.005389 1.053260 1.762642 0.190458 2 1.162331 0.009754 2.667145 3.210326 0.516525 3 1.993138 0.014496 3.669712 3.889207 0.666492 4 2.672666 0.020433 4.482730 4.311888 0.677992 [5 rows x 398 columns] Due to the process of converting two-dimensional data into one-dimensional data, the number of characteristic values in each data will increase, significantly increasing the complexity of subsequent data calculations. Therefore, we only take the top 100 characteristic values to represent each sensor and apply the KMeans method for data grouping.\nSince wavelet transform can obtain more subtle data features, we preset 20 clusters in the data clustering process (readers can test different cluster number settings by themselves and observe what will happen to the results). In addition, we will also check whether there are small clusters with only a single sensor in the grouping results and remove them to prevent a few sensors with special conditions from affecting the overall data grouping results.\nair_cwt_less = air_cwt.iloc[:, 0:101] cwt_transpose = air_cwt_less.transpose() model = TimeSeriesKMeans(n_clusters=20, metric=\"dtw\", max_iter=10, verbose=1, n_jobs=-1) pre = model.fit(cwt_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_cwt.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 6 3 08BEAC09FF42 10 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 6 8 08BEAC09FF8C 15 9 08BEAC09FF9C 19 In our example, after the above procedure, there are 14 clusters left at the end. We draw the sensor raw data in each cluster one by one, and we can find that the consistency of the sensor data in the same cluster is more obvious. At the same time , the differences between the different clusters are more detailed and clear than when using the raw data or the Fourier transform before.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_cwt[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_cwt[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Time Series Clustering — Deriving Trends and Archetypes from Sequential Data | by Denyse | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) How to Apply K-means Clustering to Time Series Data | by Alexandra Amidon | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) Understanding K-means Clustering: Hands-On with SciKit-Learn | by Carla Martins | May, 2022 | Towards AI (https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81) Fast Fourier Transform. How to implement the Fast Fourier… | by Cory Maklin | Towards Data Science (https://towardsdatascience.com/fast-fourier-transform-937926e591cb) PyWavelets/pywt: PyWavelets - Wavelet Transforms in Python (https://github.com/PyWavelets/pywt) ",
    "description": "We introduce advanced data grouping analysis. We first present two time-series feature extraction methods, Fourier transform and wavelet transform, and briefly explain the similarities and differences between the two transform methods. We introduce two different time series comparison methods, Euclidean distance and dynamic time warping (DTW), and apply existing clustering algorithms accordingly. We then discuss data clustering with different temporal resolutions, as well as their representation and potential applications in the real world.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "4.3. Time Series Data Clustering",
    "uri": "/en/ch4/ch4.3/"
  },
  {
    "content": " Table Of Contents Package Installation and Importing Initialization and Data Access Data Preprocessing Calibration Model Establishment and Validation Best Calibration Model of the Day Calibration Results References In this article, we will take the air quality data in Civil IoT Taiwan as an example to introduce how to allow two levels of air quality sensing data to be systematically and dynamically calibrated through data science methods. At the same time, with the results of data fusion, different deployment projects can work together to create a more comprehensive air quality sensing result. We use the following two air quality sensing systems:\nEnvironmental Protection Agency Air Quality Monitoring Station: In traditional air quality monitoring, extremely professional, large-scale, and high-cost monitoring stations are mainly used. It cannot be deployed in every community due to high deployment and maintenance costs. Instead, local environmental agencies will select specific locations for deployment and maintenance. According to the announcement on the Taiwan Environmental Protection Agency website, as of July 2022, the number of official monitoring stations in Taiwan is 81. Micro Air Quality Sensor: Compared with the traditional professional station, the micro air quality sensor adopts low-cost sensors, transmits data through the network, and builds a denser air quality sensor in the way of the Internet of Things network. This technology has low erection costs and provides more flexible installation conditions and expanded coverage. At the same time, this technology has the characteristics of convenient installation and maintenance, which satisfies the needs of large-scale real-time air quality monitoring systems. It can achieve the frequency of uploading data once a minute and allows users to respond immediately to sudden pollution incidents, further reducing the loss. Of course, we cannot expect low-cost micro air quality sensors to have the high accuracy of professional monitoring stations. How to improve its accuracy becomes another problem to be solved. Therefore, in the following content, we will demonstrate how to improve the accuracy of the sensing results of the micro air quality sensors to reach the level of the EPA air quality monitoring stations using data science approaches. The results can facilitate the integration of different sensing systems and further data applications in the future.\nPackage Installation and Importing In this article, we will use the pandas, numpy, datetime, sklearn, scipy, and joblib packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. Thus, we can directly use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import numpy as np from datetime import datetime, timedelta from sklearn import linear_model, svm, tree from sklearn import metrics as sk_metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split, cross_validate from sklearn.feature_selection import f_regression import scipy.stats as stats import joblib Initialization and Data Access In this example, we will use the Wanhua station of the EPA air quality station (wanhua), and two airboxes of Academia Sinica - Micro Air Quality Sensors in Civil IoT Taiwan, which are co-located with the EPA Wanhua station (the device IDs are 08BEAC028A52 and 08BEAC028690 respectively) as examples, and evaluate the use of three training models, i.e., Linear Regression, Random Forest Regression and Support Vector Regression (SVR). We will consider the PM2.5 concentration, temperature, relative humidity, and timestamp features of the data, combined with historical data for three different lengths of time (3 days, 5 days, 8 days), and conduct a series of explorations.\nFor convenience, we first make the following initial program setup based on these assumptions.\nSITE= \"wanhua\" EPA= \"EPA-Wanhua\" AIRBOXS= ['08BEAC028A52', '08BEAC028690'] DAYS= [8, 5, 3] METHODS= ['LinearRegression', 'RandomForestRegressor', 'SVR'] METHOD_SW= { 'LinearRegression':'LinearR', 'RandomForestRegressor':'RFR', 'SVR':'SVR' } METHOD_FUNTION= {'LinearRegression':linear_model.LinearRegression(), 'RandomForestRegressor': RandomForestRegressor(n_estimators= 300, random_state= 36), 'SVR': svm.SVR(C=20) } FIELD_SW= {'s_d0':'PM25', 'pm25':'PM25', 'PM2_5':\"PM25\", 'pm2_5':\"PM25\", 's_h0':\"HUM\", 's_t0':'TEM'} FEATURES_METHOD= {'PHTR':[\"PM25\", \"HR\", \"TEM\", \"HUM\"], 'PH':['PM25','HR'], 'PT':['PM25','TEM'], 'PR':['PM25', 'HUM'], 'P':['PM25'], 'PHR':[\"PM25\", \"HR\", \"HUM\"], 'PTR':[\"PM25\", \"TEM\", \"HUM\"], 'PHT':[\"PM25\", \"HR\", \"TEM\"] } Next we consider how much data needs to be downloaded for system calibration and data fusion. As shown in the figure below, assuming that we want to obtain the calibration model of the Nth day, the data of (N-1)’s day will be used as test data to evaluate the accuracy of the calibration model. So if we set the training data to be X days, then the historical data representing days N - 2 to N - (2+X) days will be used as training data. In our scenario, we set N to the current time (today) and the maximum possible X value is 8, so we need to prepare a total of ten days of historical data for the next operation.\nFor later use, we first specify the date to calibrate the model (Nth day: TODAY), the date of the test data (N-1 th day: TESTDATE), and the end date of the training data (N-2 th day: ENDDATE) with the following code .\nTODAY = datetime.today() TESTDATE = (TODAY - timedelta(days=1)).date() ENDDATE = (TODAY - timedelta(days=2)).date() In this example, we use the data access API provided by Academia Sinica - Micro Air Quality Sensors. According to the specified date and device code , the sensing data of the corresponding date and device can be downloaded in CSV file format. Note that due to the limitations of the data access API, the specified date can only be a date within the past 30 days from the date of download.\nhttps://pm25.lass-net.org/data/history-date.php?device_id=\u003cID\u003e\u0026date=\u003cYYY-MM-DD\u003e\u0026format=CSV For example, suppose we want to download the sensing data of EPA Wanhua Station on September 21, 2022, we can download it in the following ways:\nhttps://pm25.lass-net.org/data/history-date.php?device_id=EPA-Wanhua\u0026date=2022-09-21\u0026format=CSV Using this method, we write a Python function getDF, which can download the sensing data of the past 10 days for the given device code, and return the collected data in the form of a single DataFrame object.\ndef getDF(id): temp_list = [] for i in range(1,11): date = (TODAY - timedelta(days=i)).strftime(\"%Y-%m-%d\") URL = \"https://pm25.lass-net.org/data/history-date.php?device_id=\" + id + \"\u0026date=\" + date + \"\u0026format=CSV\" temp_DF = pd.read_csv( URL, index_col=0 ) temp_list.append( temp_DF ) print(\"ID: {id}, Date: {date}, Shape: {shape}\".format(id=id, date=date, shape=temp_DF.shape)) All_DF = pd.concat( temp_list ) return All_DF Next, we download the sensing data of the first airbox installed at the EPA Wanhua station and store it in the AirBox1_DF object:\n# First AirBox device AirBox1_DF = getDF(AIRBOXS[0]) AirBox1_DF.head() ID: 08BEAC028A52, Date: 2022-09-29, Shape: (208, 19) ID: 08BEAC028A52, Date: 2022-09-28, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-27, Shape: (225, 19) ID: 08BEAC028A52, Date: 2022-09-26, Shape: (230, 19) ID: 08BEAC028A52, Date: 2022-09-25, Shape: (231, 19) ID: 08BEAC028A52, Date: 2022-09-24, Shape: (232, 19) ID: 08BEAC028A52, Date: 2022-09-23, Shape: (223, 19) ID: 08BEAC028A52, Date: 2022-09-22, Shape: (220, 19) ID: 08BEAC028A52, Date: 2022-09-21, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-20, Shape: (215, 19) Using the same method, we download the sensing data of the second airbox installed at the EPA Wanhua station and the sensing data of the EPA Wanhua station, and stored them in the AirBox2_DF and EPA_DF objects, respectively.\n# Second AirBox device AirBox2_DF = getDF(AIRBOXS[1]) # EPA station EPA_DF = getDF(EPA) Data Preprocessing Since our current data contains many unneeded fields, in order to avoid taking up too much memory space, we first simplify the data we use, leaving only what is needed.\nCol_need = [\"timestamp\", \"s_d0\", \"s_t0\", \"s_h0\"] AirBox1_DF_need = AirBox1_DF[Col_need] print(AirBox1_DF_need.head()) AirBox2_DF_need = AirBox2_DF[Col_need] print(AirBox2_DF_need.head()) Col_need = [\"time\", \"date\", \"pm2_5\"] EPA_DF_need = EPA_DF[Col_need] print(EPA_DF_need.head()) del AirBox1_DF del AirBox2_DF del EPA_DF timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:03:28Z 9.0 29.75 71.0 1 2022-09-30T00:33:46Z 11.0 31.36 67.0 2 2022-09-30T00:39:51Z 10.0 31.50 67.0 3 2022-09-30T00:45:58Z 12.0 31.50 66.0 4 2022-09-30T00:52:05Z 12.0 31.86 66.0 timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:00:31Z 9.0 29.36 -53.0 1 2022-09-30T00:07:17Z 9.0 29.50 -52.0 2 2022-09-30T00:23:47Z 10.0 30.25 -45.0 3 2022-09-30T00:34:24Z 10.0 31.11 -36.0 4 2022-09-30T00:40:31Z 11.0 31.25 -35.0 time date pm2_5 index 0 00:00:00 2022-09-30 9.0 1 01:00:00 2022-09-30 10.0 2 02:00:00 2022-09-30 16.0 3 03:00:00 2022-09-30 19.0 4 04:00:00 2022-09-30 20.0 Next, in order to unify the data fields, we merge the original date and time fields of the EPA station data to generate a new timestamp field.\nEPA_DF_need['timestamp'] = pd.to_datetime( EPA_DF_need[\"date\"] + \"T\" + EPA_DF_need[\"time\"], utc=True ) print(EPA_DF_need.head()) time date pm2_5 timestamp index 0 00:00:00 2022-09-30 9.0 2022-09-30 00:00:00+00:00 1 01:00:00 2022-09-30 10.0 2022-09-30 01:00:00+00:00 2 02:00:00 2022-09-30 16.0 2022-09-30 02:00:00+00:00 3 03:00:00 2022-09-30 19.0 2022-09-30 03:00:00+00:00 4 04:00:00 2022-09-30 20.0 2022-09-30 04:00:00+00:00 Due to the different temporal resolutions of the airbox and EPA station data, to align the data on both sides, we resample the airbox data from the original every five minutes to every hour.\ndef getHourly(DF): DF = DF.set_index( pd.DatetimeIndex(DF[\"timestamp\"]) ) DF_Hourly = DF.resample('H').mean() DF_Hourly.reset_index(inplace=True) return DF_Hourly AirBox1_DF_need_Hourly = getHourly( AirBox1_DF_need) AirBox2_DF_need_Hourly = getHourly( AirBox2_DF_need) EPA_DF_need_Hourly = getHourly( EPA_DF_need) # 可省略，原始數據已經是小時平均 del AirBox1_DF_need del AirBox2_DF_need del EPA_DF_need print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp s_d0 s_t0 s_h0 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp pm2_5 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 We also replace the data fields from both sources with easily identifiable field names for easy follow-up and identification.\nCol_rename = {\"s_d0\":\"PM25\", \"s_h0\":\"HUM\", \"s_t0\":\"TEM\"} AirBox1_DF_need_Hourly.rename(columns=Col_rename, inplace=True) AirBox2_DF_need_Hourly.rename(columns=Col_rename, inplace=True) Col_rename = {\"pm2_5\":\"EPA_PM25\"} EPA_DF_need_Hourly.rename(columns=Col_rename, inplace=True) print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp PM25 TEM HUM 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp EPA_PM25 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 Since the two airboxes have the same hardware and the same location, we treat them as the same data source and combine the data of the two airboxes to generate the AirBoxs_DF object. We then intersect the new data object with the EPA air quality station data according to the time field, producing a merged ALL_DF object.\nAirBoxs_DF = pd.concat([AirBox1_DF_need_Hourly, AirBox2_DF_need_Hourly]).reset_index(drop=True) All_DF = pd.merge( AirBoxs_DF, EPA_DF_need_Hourly, on=[\"timestamp\"], how=\"inner\" ) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 NaN 5 2022-09-21 02:00:00+00:00 9.000000 30.418889 -59.222222 NaN 6 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 7 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 8 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 9 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 We first exclude the presence of null values (NaN) in the data.\nAll_DF.dropna(how=\"any\", inplace=True) All_DF.reset_index(inplace=True, drop=True) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 Finally, since the daily hour value information will be used when building the model, we add an HR field to All_DF that contains the content of the hour value in the timestamp.\ndef return_HR(row): row['HR'] = int(row[ \"timestamp\" ].hour) return row All_DF = All_DF.apply(return_HR , axis=1) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 HR 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 1 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 1 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 3 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 3 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 4 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 4 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 5 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 5 Calibration Model Establishment and Validation After completing the data preparation, we next start building candidate calibration models. Since we discussed a total of 3 regression methods, 3 historical data lengths, and 8 feature combinations in this case, we will generate a total of 3 x 3 x 8 = 72 candidate models.\nFirst, we designed the SlideDay function to retrieve a specific length of historical data when building a calibration model. The function returns the input data Hourly_DF from enddate to the total day day data according to the input historical data length day.\ndef SlideDay( Hourly_DF, day, enddate ): startdate= enddate- timedelta( days= (day-1) ) time_mask= Hourly_DF[\"timestamp\"].between( pd.Timestamp(startdate, tz='utc'), pd.Timestamp(enddate, tz='utc') ) return Hourly_DF[ time_mask ] Next, we organize the sensing data Training_DF of the site station from enddate to day days before, and organize it into training data according to the feature feature combination. Then we apply the method regression method, so that the predicted value produced by the generated calibration model can approximate the data value of the EPA_PM25 field in the training data.\nWe calculate both the Mean Absolute Error (MAE) and the Mean Squared Error (MSE) of the corrected model itself. Among them, the mean absolute error (MAE) is the sum of the absolute values of the difference between the target value and the predicted value, which may reflect the actual situation of the predicted value error. The smaller the value, the better the performance. The mean squared error (MSE) is the mean of the square of the difference between the predicted value and the actual observed value. The smaller the MSE value, the better the accuracy of the prediction model in describing the experimental data.\ndef BuildModel( site, enddate, feature, day, method, Training_DF ): X_train = Training_DF[ FEATURES_METHOD[ feature ] ] Y_train = Training_DF[ \"EPA_PM25\" ] model_result = {} model_result[\"site\"], model_result[\"day\"], model_result[\"feature\"], model_result[\"method\"] = site, day, feature, method model_result[\"datapoints\"], model_result[\"modelname\"] = X_train.shape[0], (site + \"_\" + str(day) + \"_\" + METHOD_SW[method] + \"_\" + feature) model_result[\"date\"] = enddate.strftime( \"%Y-%m-%d\" ) # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) model_result['create_timestamp_utc'] = Now_Time ### training model ### print( \"[BuildR]-\\\"{method}\\\" with {day}/{feature}\".format(method=method, day=day, feature=feature) ) # fit lm = METHOD_FUNTION[ method ] lm.fit( X_train, Y_train ) # get score Y_pred = lm.predict( X_train ) model_result['Train_MSE'] = MSE = sk_metrics.mean_squared_error( Y_train, Y_pred ) model_result['Train_MAE'] = sk_metrics.mean_absolute_error( Y_train, Y_pred ) return model_result, lm In addition to evaluating the MAE and MSE of the training data when building the calibrated model, we also consider the prediction performance of the calibrated model on non-training data. Therefore, for the model lm, according to the feature data feature that it needs to bring in, as well as the test data, we generate the predicted value, and compare it with the EPA_PM25 field in the test data to calculate the MAE and MSE respectively. , as a reference for subsequent evaluation of the applicability of different calibration models.\ndef TestModel( site, feature, modelname, Testing_DF, lm ): X_test = Testing_DF[ FEATURES_METHOD[ feature ] ] Y_test = Testing_DF[ \"EPA_PM25\" ] # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) ### testing model ### # predict Y_pred = lm.predict( X_test ) # get score test_result = {} test_result[\"test_MSE\"] = round( sk_metrics.mean_squared_error( Y_test, Y_pred ), 3) test_result[\"test_MAE\"] = round( sk_metrics.mean_absolute_error( Y_test, Y_pred ), 3) return test_result Finally, we integrate the just completed SlideDay, BuildModel and TestModel, and complete a total of 72 calibration models. For each calibrated model, we compute MAE and MSE for training and testing data separately, and store all results in an AllResult_DF object.\nAllResult_list = [] for day in DAYS: for method in METHODS: for feature in FEATURES_METHOD: Training_DF = SlideDay(All_DF, day, ENDDATE)[ FEATURES_METHOD[feature] + [\"EPA_PM25\"] ] result, lm = BuildModel( SITE, TESTDATE, feature, day, method, Training_DF ) test_result = TestModel(SITE, feature, result[\"modelname\"], SlideDay(All_DF, 1, TESTDATE), lm) R_DF = pd.DataFrame.from_dict( [{ **result, **test_result }] ) AllResult_list.append( R_DF ) AllResult_DF = pd.concat(AllResult_list) AllResult_DF.head() [BuildR]-\"LinearRegression\" with 8/PHTR [BuildR]-\"LinearRegression\" with 8/PH [BuildR]-\"LinearRegression\" with 8/PT [BuildR]-\"LinearRegression\" with 8/PR [BuildR]-\"LinearRegression\" with 8/P [BuildR]-\"LinearRegression\" with 8/PHR [BuildR]-\"LinearRegression\" with 8/PTR [BuildR]-\"LinearRegression\" with 8/PHT [BuildR]-\"RandomForestRegressor\" with 8/PHTR [BuildR]-\"RandomForestRegressor\" with 8/PH [BuildR]-\"RandomForestRegressor\" with 8/PT [BuildR]-\"RandomForestRegressor\" with 8/PR [BuildR]-\"RandomForestRegressor\" with 8/P [BuildR]-\"RandomForestRegressor\" with 8/PHR [BuildR]-\"RandomForestRegressor\" with 8/PTR [BuildR]-\"RandomForestRegressor\" with 8/PHT [BuildR]-\"SVR\" with 8/PHTR [BuildR]-\"SVR\" with 8/PH [BuildR]-\"SVR\" with 8/PT [BuildR]-\"SVR\" with 8/PR [BuildR]-\"SVR\" with 8/P [BuildR]-\"SVR\" with 8/PHR [BuildR]-\"SVR\" with 8/PTR [BuildR]-\"SVR\" with 8/PHT [BuildR]-\"LinearRegression\" with 5/PHTR [BuildR]-\"LinearRegression\" with 5/PH [BuildR]-\"LinearRegression\" with 5/PT [BuildR]-\"LinearRegression\" with 5/PR [BuildR]-\"LinearRegression\" with 5/P [BuildR]-\"LinearRegression\" with 5/PHR [BuildR]-\"LinearRegression\" with 5/PTR [BuildR]-\"LinearRegression\" with 5/PHT [BuildR]-\"RandomForestRegressor\" with 5/PHTR [BuildR]-\"RandomForestRegressor\" with 5/PH [BuildR]-\"RandomForestRegressor\" with 5/PT [BuildR]-\"RandomForestRegressor\" with 5/PR [BuildR]-\"RandomForestRegressor\" with 5/P [BuildR]-\"RandomForestRegressor\" with 5/PHR [BuildR]-\"RandomForestRegressor\" with 5/PTR [BuildR]-\"RandomForestRegressor\" with 5/PHT [BuildR]-\"SVR\" with 5/PHTR [BuildR]-\"SVR\" with 5/PH [BuildR]-\"SVR\" with 5/PT [BuildR]-\"SVR\" with 5/PR [BuildR]-\"SVR\" with 5/P [BuildR]-\"SVR\" with 5/PHR [BuildR]-\"SVR\" with 5/PTR [BuildR]-\"SVR\" with 5/PHT [BuildR]-\"LinearRegression\" with 3/PHTR [BuildR]-\"LinearRegression\" with 3/PH [BuildR]-\"LinearRegression\" with 3/PT [BuildR]-\"LinearRegression\" with 3/PR [BuildR]-\"LinearRegression\" with 3/P [BuildR]-\"LinearRegression\" with 3/PHR [BuildR]-\"LinearRegression\" with 3/PTR [BuildR]-\"LinearRegression\" with 3/PHT [BuildR]-\"RandomForestRegressor\" with 3/PHTR [BuildR]-\"RandomForestRegressor\" with 3/PH [BuildR]-\"RandomForestRegressor\" with 3/PT [BuildR]-\"RandomForestRegressor\" with 3/PR [BuildR]-\"RandomForestRegressor\" with 3/P [BuildR]-\"RandomForestRegressor\" with 3/PHR [BuildR]-\"RandomForestRegressor\" with 3/PTR [BuildR]-\"RandomForestRegressor\" with 3/PHT [BuildR]-\"SVR\" with 3/PHTR [BuildR]-\"SVR\" with 3/PH [BuildR]-\"SVR\" with 3/PT [BuildR]-\"SVR\" with 3/PR [BuildR]-\"SVR\" with 3/P [BuildR]-\"SVR\" with 3/PHR [BuildR]-\"SVR\" with 3/PTR [BuildR]-\"SVR\" with 3/PHT Best Calibration Model of the Day When discussing the “best calibrated model of the day”, it must be admitted that the so-called “best model” is only available after the end of the day. That is to say, the MAE and MSE of different candidate models can only be obtained after collecting complete 24-hour data, and the best model can be obtained after comparative analysis. Therefore, it will not be possible to systematically generate a truly optimized calibration model before the end of today’s 24 hours.\nHowever, based on practical needs, we often need to have a calibration model that can be applied in the data production process. So, in practice, we usually assume that “yesterday’s best model will also perform well today” and use that as a way to correct that day’s data. For example, assuming we use MSE as the criterion for deciding the best model, we can get information about the best model using the following syntax:\nFIELD= \"test_MSE\" BEST= AllResult_DF[ AllResult_DF[FIELD]== AllResult_DF[FIELD].min() ] BEST Next, to adapt to today’s situation, we use yesterday’s best calibrated model parameters (historical data length, regression method, feature data combination), recalculate training and test data from today’s date range, and regenerate today’s calibrated model .\nBEST_DC= BEST.to_dict(orient=\"index\")[0] Training_DF= SlideDay(All_DF, BEST_DC[\"day\"], TESTDATE)[ FEATURES_METHOD[BEST_DC[\"feature\"]]+ [\"EPA_PM25\"] ] result, lm= BuildModel( SITE, TODAY, BEST_DC[\"feature\"], BEST_DC[\"day\"], BEST_DC[\"method\"], Training_DF ) result [BuildR]-\"SVR\" with 3/PHT {'site': 'wanhua', 'day': 3, 'feature': 'PHT', 'method': 'SVR', 'datapoints': 80, 'modelname': 'wanhua_3_SVR_PHT', 'date': '2022-09-30', 'create_timestamp_utc': '2022-09-30 11:19:48', 'Train_MSE': 3.91517342356589, 'Train_MAE': 1.42125724796098} From this example, we can see that the newly produced calibration model has a Train_MSE of 3.915, which is much higher than the original 2.179. However, since we have no way to know the real best model of the day before the end of the day, we can only apply yesterday’s best model if there is no other better choice. Finally, we export the .joblib file with the following syntax and release the model for sharing (please refer to the reference materials for details).\nmodel_dumpname= result[\"modelname\"]+ \".joblib\" MODEL_OUTPUT_PATH= \"\" try: joblib.dump( lm, MODEL_OUTPUT_PATH+ model_dumpname ) print( \"[BuildR]-dump {}\".format( MODEL_OUTPUT_PATH+model_dumpname ) ) except Exceptionas e: print( \"ERROR! [dump model] {}\".format( result[\"modelname\"] ) ) error_msg(e) [BuildR]-dump wanhua_3_SVR_PHT.joblib Calibration Results The joint calibration method described in this article has been officially applied to “Academia Sinica - Micro Air Quality Sensors” in Civil IoT Taiwan from 2020/5, and the daily output calibration model has been published on the Dynamic Calibration Model website. In the implementation, a total of 31 EPA air quality monitoring stations were selected to install two airboxes, and we considered 3 historical data lengths, 8 data feature combinations, and 7 regression methods (i.e. a total of 3 x 8 x 7 = 168 combinations) to generate the best daily calibration model for each of these 31 sites. Then, for the sensing data of each airbox, the best calibration model of the station closest to its geographical location is used as the application model of its data calibration, and its sensing data is generated and published. From the observation of the actual operation after the mechanism is launched, the data difference between the micro air quality sensor device and the EPA air quality monitoring station can be effectively reduced (as shown in the figure below). This achievement also established a good cooperation model for cross-system data integration of air quality monitoring.\nReferences Dynamic Calibration Model Status Report (https://pm25.lass-net.org/DCF/) scikit-learn: machine learning in Python (https://scikit-learn.org/stable/) Joblib: running Python functions as pipeline jobs (https://joblib.readthedocs.io/) Jason Brownlee, Save and Load Machine Learning Models in Python with scikit-learn, Machine Learning Mastery (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/) ",
    "description": "We use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "6.3. Joint Data Calibration",
    "uri": "/en/ch6/ch6.3/"
  },
  {
    "content": "\nTable Of Contents Package Installation and Importing Data Access Leafmap Basics Basic Data Presentation Cluster Data Presentation Change Leafmap Basemap Integrate OSM Resources Heatmap Presentation Split Window Presentation Leafmap for Web Applications Conclusion References In the previous chapters, we have demonstrated how to use programming languages to analyze data on geographic attributes, and we have also demonstrated how to use GIS software for simple geographic data analysis and presentation. Next, we will introduce how to use the Leafmap suite in Python language for GIS applications, and the Streamlit suite for website development. Finally, we will combine Leafmap and Streamlit to make a simple web GIS system by ourselves, and present the results of data processing and analysis through web pages.\nPackage Installation and Importing In this chapter, we will use packages such as pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder, and pyCIOT. Apart from pandas, our development platform Google Colab does not provide these packages, so we need to install them ourselves first. Since there are many packages installed this time, in order to avoid a large amount of information output after the command is executed, we have added the ‘-q’ parameter to each installation command, which can make the output of the screen more concise.\n!pip install -q geopandas !pip install -q leafmap !pip install -q ipyleaflet !pip install -q osmnx !pip install -q streamlit !pip install -q geocoder !pip install -q pyCIOT After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport pandas as pd import geopandas as gpd import leafmap import ipyleaflet import osmnx import geocoder import streamlit from pyCIOT.data import * Data Access In the examples in this article, we use several datasets on the Civil IoT Taiwan Data Service Platform, including air quality data from the EPA, and seismic monitoring station measurements from the National Earthquake Engineering Research Center and the Central Weather Bureau.\nFor the EPA air quality data, we use the pyCIOT package to obtain the latest measurement results of all EPA air quality stations, and convert the resulting JSON format data into a DataFrame through the json_normalize() method in the pandas package format. We only keep the station name, latitude, longitude and ozone (O3) concentration information for future use. The code for this part of data collection and processing is as follows:\nepa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) df_air['O3'] = 0 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] df_air = df_air[['name','location.latitude','location.longitude','O3']] df_air Then we extract the seismic monitoring station data from the National Earthquake Engineering Research Center and the Central Weather Bureau in a similar way, leaving only the station name, longitude and latitude information for subsequent operations. The code for this part of data collection and processing is as follows:\nquake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") df_quake = pd.json_normalize(quake_station) df_quake = df_quake[['name','location.latitude','location.longitude']] df_quake We have successfully demonstrated the reading examples of air quality data (air) and seismic data (quake). In the following discussion, we will use these data for the operation and application using the leafmap suite. The same methods can also be easily applied to other datasets on the Civil IoT Taiwan Data Service Platform. You are encouraged to try it yourself.\nLeafmap Basics Basic Data Presentation Using the data df_air and seismic data df_quake that we are currently processing, we first convert the format of these two data from the DataFrame format provided by the pandas package to the GeoDataFrame format provided by the geopandas package which supports geographic information attributes. We then use Leafmap’sadd_gdf() method to create a presentation layer for each dataset and add them to the map in one go.\ngdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") m1 From the map output by the program, we can see the names of the two data in the upper right corner of the map, which have been added to the map in the form of two layers. Users can click the layer to be queried to browse according to their own needs. However, when we want to browse the data of two layers at the same time, we will find that both layers are rendering the same icon, so there will be confusion on the map.\nTo solve this problem, we introduce another way of data presentation. We use the GeoData layer data format provided by the ipyleaflet suite and add the GeoData layer to the map using leafmap’s add_layer() method. For easy identification, we use the small blue circle icon to represent the data of the empty station, and the small red circle icon to represent the data of the seismic station.\ngeo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) m2 Cluster Data Presentation In some data applications, when there are too many data points on the map, it is not easy to observe. If this is the case, we can use clustering to present the data. i.e. when there are too many data points for a small area, we cluster the points together to show the number of points. When the user zooms in on the map, these originally clustered points are slowly pulled apart. When there is only one point left in a small area, the information of that point can be directly seen.\nLet’s take data from seismic stations as an example. Using leafmap’s add_points_from_xy() method, the data of df2 can be placed on the map in a clustered manner.\nm3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") m3 Change Leafmap Basemap Leafmap uses OpenStreetMap as the default basemap and provides over 100 other basemap options. Users can change the basemap according to their own preferences and needs. You can use the following syntax to learn which basemaps currently supported by leafmap:\nlayers = list(leafmap.basemaps.keys()) layers We select SATELLITE and Stamen.Terrain from these basemaps as demonstrations, and use the add_basemap() method of the leafmap package to add the basemap as a new layer. After adding, the leafmap preset will open all layers and stack them in the order of addition. You can select the layer you want to use through the layer menu in the upper right corner.\nm4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m4.add_gdf(gdf_air, layer_name=\"EPA Station\") m4.add_basemap(\"SATELLITE\") m4.add_basemap(\"Stamen.Terrain\") m4 In addition to using the basemap provided by leafmap, you can also use Google Map’s XYZ Tiles service to add layers of Google satellite imagery. The methods are as follows:\nm4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) m4 Integrate OSM Resources In addition to some built-in resources, Leafmap also integrates many external geographic information resources. Among them, OSM (OpenStreetMap) is a well-known and rich open source geographic information resource. Various resources provided by OSM can be found on the OSM website, with a complete list of properties.\nIn the following example, we use the add_osm_from_geocode() method provided by the leafmap package to demonstrate how to get the outline of a city and render it on the map. Taking Taichung City as an example, combined with the location information in the EPA air quality monitoring station data, we can clearly see which stations are in Taichung City.\ncity_name = \"Taichung, Taiwan\" m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) m5 Then we continue to use the add_osm_from_place() method provided by the leafmap package to further search for specific facilities in Taichung City and add them to the map layer. The following procedure takes factory facilities as an example and uses the land use data of OSM to find out the relevant factory locations and areas in Taichung City, which can be analyzed and explained in combination with the locations of EPA air quality monitoring stations. For more types of OSM facilities, you can refer to the complete properties list.\nm5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") m5 In addition, the leafmap package also provides a method for searching for OSM nearby facilities centered on a specific location, providing a very convenient function for analyzing and interpreting data. For example, in the following example, we use the add_osm_from_address() method to search for related religious facilities (attribute “amenity”: “place_of_worship”) within a 1,000-meter radius of Qingshui Station, Taichung; at the same time, we use the add_osm_from_point() method to search for relevant school facilities (attributes “amenity”: “school”) within 1,000 meters of the GPS coordinates (24.26365, 120.56917) of Taichung Qingshui Station. Finally, we overlay the results of these two queries on the existing map with different layers.\nm5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) m5 Heatmap Presentation A heatmap is a two-dimensional representation of event intensity through color changes. When matching a heatmap to a map, the state of event intensity can be expressed at different scales depending on the scale of the map used. It is a very common and powerful data representation tool. However, when drawing a heatmap, the user must confirm that the characteristics of the data are suitable for presentation by a heatmap, otherwise it is easy to be confused with the graphical data interpolation representations such as IDW and Kriging that we introduced in Chap 5. For example, we take the O3 concentration data of the EPA air quality data as an example, and draw the corresponding heat map as follows:\nm6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m6.add_layer(geo_data_air) m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) m6 There is nothing obvious about this image at first glance, but if we zoom in on the Taichung city area, we can see that the appearance of the heatmap has changed a lot, showing completely different results at different scales.\nThe above example is actually an example of misuse of the heatmap, because the O3 concentration data reflects the local O3 concentration. Due to the change of the map scale, its values cannot be directly accumulated or distributed to adjacent areas. Therefore, the O3 concentration data used in the example is not suitable for heatmap representation and should be plotted using the geographic interpolation method described in Chapter 5.\nTo show the real effect of the heatmap, we use the location data of the seismic stations instead and add a field num with a default value of 10. Then we use the code below to generate a heatmap of the status of Taiwan Seismic Monitoring Stations.\ndf_quake['num'] = 10 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m7.add_layer(geo_data_quake) m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) m7 Split Window Presentation In the process of data analysis and interpretation, it is often necessary to switch between different basemaps to obtain different geographic information. Therefore, the leafmap package provides the split_map() method, which can split the original map output into two submaps, each applying a different basemap. Its sample code is as follows:\nm8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m8.add_gdf(gdf_air, layer_name=\"EPA Station\") m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) m8 Leafmap for Web Applications In order to quickly share the processed map information, Leafmap suite also provides an integrated way of Streamlit suite, combining Leafmap’s GIS technical expertise with Streamlit’s web technical expertise to quickly build a Web GIS system. Below we demonstrate how it works through a simple example, you can extend and build your own Web GIS service according to this principle.\nIn the use of the Streamlit package, there are two steps to build a web system:\nPackage the Python program to be executed into a Streamlit object, and write the packaging process into the app.py file; and Execute app.py on the system. Since our operation process all use the Google Colab platform, in this platform we can directly write app.py into the temporary storage area with the special syntax %%writefile, and then Colab directly reads and runs the codes from the temporary storage area. Therefore, for the file writing part of step 1, we can proceed as follows:\n%%writefile app.py import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * contnet = \"\"\" Hello World! \"\"\" st.title('Streamlit Demo') st.write(\"## Leafmap Example\") st.markdown(contnet) epa_station = Air().get_data(src=\"OBS:EPA\") from pandas import json_normalize df_air = json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') with st.expander(\"See source code\"): with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") m.to_streamlit() For the second part, we use the following instructions:\n!streamlit run app.py \u0026 npx localtunnel --port 8501 After execution, an execution result similar to the following will appear:\nThen you can click on the URL after the string “your url is:”, and something similar to the following will appear in the browser\nFinally, we click “Click to Continue” to execute the Python code packaged in app.py. In this example, we can see the distribution map of EPA’s air quality monitoring stations presented by the leafmap package.\nConclusion In this article, we introduced the Leafmap package to render geographic data and integrate external resources, and demonstrated the combination of the Leagmap and Streamlit packages to build a simple web-based GIS service on the Google Colab platform. It should be noted that Leafmap also has many more advanced functions, which are not introduced in this article. You can refer to the following references for more in-depth and extensive learning.\nReferences Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "We introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.",
    "tags": [
      "Python",
      "Air",
      "Quake"
    ],
    "title": "7.3. Leafmap Applications",
    "uri": "/en/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/en/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Air",
    "uri": "/en/tags/air/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/en/tags/api/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Authors",
    "uri": "/en/authors/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/en/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/en/categories/"
  },
  {
    "content": "We love comments on our articles and would love to hear what you have to say respectfully. Remember to:\nFriendly, polite and relevant; No profanity, abusive, personal attacks or inappropriate content; If you harass people or make fun of tragedies, you will be blocked; No spam posts or posts trying to sell anything. The main rule here is simple: treat others with respect for yourself. Trolling and posting inappropriate content can get you banned, and we don’t want that.\nOur moderators do their best not to delete comments, and we believe that communities have individual, and often strong, opinions and perspectives. However, we reserve the right to remove comments for the above reasons as well as for the following reasons:\nAny violation of our Privacy Policy; Anything that is derogatory and/or derogatory in any perceivable way based on race, religion, colour, national origin, disability, sexual orientation; Any other comments that are harmful to the community. Our moderators try to catch inappropriate content, but we also want our community to help us police it. If you see an inappropriate comment, you can report it to us by clicking on the banner. We will review comments and decide whether to keep or delete them.\nCan’t find your comment on the site? Occasionally we experience technical difficulties and your comment may not appear on our site. We may not have approved it because it violates one of the above rules. We may have removed it because it violated our commenting guidelines. Why am I blocked from commenting? While we do our best to allow everyone to have an opinion in our online communities, sometimes commenters cross the line. From time to time, we block and ban commenters who violate our rules. Blockers use a variety of techniques and information to perform blocking, including emails, IP addresses, and any other information available. This prevents abusive commenters from posting comments on the site in the future, even if future or other comments are not abusive. Depending on the software we are using at the time, the ban could also cause other previous comments from the abuser to disappear.\n",
    "description": "",
    "tags": null,
    "title": "Comment Policy",
    "uri": "/en/comment-policy/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Disaster",
    "uri": "/en/tags/disaster/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Huei-Jun Kao",
    "uri": "/en/authors/huei-jun-kao/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Hung-Ying Chen",
    "uri": "/en/authors/hung-ying-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "intermediate",
    "uri": "/en/levels/intermediate/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/en/tags/introduction/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Jen-Wei Huang",
    "uri": "/en/authors/jen-wei-huang/"
  },
  {
    "content": "Learning Civil IoT Taiwan Open Data and Its Applications The “Civil IoT Taiwan” initiative aims to tackle urgent public needs and improve public services in areas such as air quality, earthquake monitoring, water resources, and disaster prevention. It leverages big data, artificial intelligence, and Internet of Things (IoT) to create intelligent systems that enhance public services and help both the government and citizens adapt to environmental changes. Tailored for various users, including government bodies, academia, industry, and the public, the project strives to bolster smart governance, aid industrial and academic growth, and elevate people’s well-being.\nA significant component of the Civil IoT Taiwan initiative is the “Civil IoT Taiwan Data Platform.” This platform offers users real-time and historical data in a consistent format, enhancing the efficiency of data browsing and retrieval. It fosters the use of model-based scientific computing and artificial intelligence by offering a wealth of reliable, high-quality data collected through the Civil IoT project. With this platform, the public gains access to comprehensive environmental information, enabling real-time monitoring of environmental changes. The platform also spurs the creation of value-added applications, harnessing collective creativity to address widespread issues.\nLooking ahead, the “Learning Civil IoT Taiwan” project is committed to three core objectives to ensure the continued growth and impact of the Civil IoT Taiwan project:\nEducational Engagement: It aims to provide educational resources for college and high school students, fostering interdisciplinary learning in information technology, geography, earth science, and social studies. Application Enhancement: The project seeks to refine and enhance existing applications of Civil IoT Taiwan, lowering barriers to entry and encouraging innovative developments to cater to public needs effectively. Platform Expansion: Efforts will be made to broaden data access methods, including the integration of the popular Python programming language, to attract a diverse technical audience and expand the user base of the data platform. ",
    "description": "",
    "tags": null,
    "title": "Learning Civil IoT Taiwan Open Data and Its Applications",
    "uri": "/en/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/en/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ling-Jyh Chen",
    "uri": "/en/authors/ling-jyh-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ming-Kuang Chung",
    "uri": "/en/authors/ming-kuang-chung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/en/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quake",
    "uri": "/en/tags/quake/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quen Luo",
    "uri": "/en/authors/quen-luo/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Sky Hung",
    "uri": "/en/authors/sky-hung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/en/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tze-Yu Sheng",
    "uri": "/en/authors/tze-yu-sheng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Water",
    "uri": "/en/tags/water/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Chi Peng",
    "uri": "/en/authors/yu-chi-peng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Shen Cheng",
    "uri": "/en/authors/yu-shen-cheng/"
  }
]
