[
  {
    "content": "1. Introduction Table Of Contents Environment Perception Open Data Civil IoT Taiwan Civil IoT Taiwan Data Service Platform References Environment Perception Since ancient times, human curiosity about the natural world began during the agricultural civilization era, when people would look up to the sky and observe the stars. During the Renaissance in the 16th century, Copernicus proposed the heliocentric theory based on the astronomical data available at the time. Galileo and Newton followed his line of thought, laying the foundation for modern science. In today’s society, with the rapid advancement of semiconductor technology, the tools we use to perceive our living environment are becoming increasingly diverse, precise, and miniature. When these miniature sensors are combined with the evolution of information technology and the real-time data transmission of the internet, we obtain an unprecedented amount of observational data.\nFaced with this vast ocean of data, scientists are striving to analyze patterns of environmental change and study the connections between these patterns and disasters. Their goal is to better predict disasters, improve our quality of life, enhance disaster prevention and mitigation, and promote harmonious coexistence between humans and the environment. This is not only an issue of concern for individuals, communities, nations, and the globe today, but it is also the core objective in the pursuit of a beautiful and sustainable living environment.\nOpen Data “Open data” is a strategy that involves organizing various types of digital data — whether text, numbers, images, videos, or sound — in specific processes and formats, then making them publicly available and encouraging their free use by all. According to the definition provided by the Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/), to be classified as open data, it must meet several criteria.\nOpen license or status: The open data content must be in the public domain and released with an open license without any additional restrictions. Access: Open data content must be freely accessible from the Internet. However, access to open data may also allow a one-time fee and conditional incidental terms under reasonable conditions. Machine readability: Open data content must be easily accessible, processed, and modified by computers. Open format: Open data content must be in an available form, accessed using free or open software. As the concepts of open source, open government, and open science become more widespread, open data has gradually become an important principle in policy-making and academic research for governments and the scientific community. Especially notable in recent times is the environmental sensing Internet of Things (IoT), which, due to its wide distribution in public spaces and the close relevance of its monitored environmental information to the daily lives of the public, is considered one of the most anticipated sources of open data.\nCommon open data formats found on the internet today include JSON, CSV, and XML:\nJSON (Javascript Object Notation): This is a lightweight data interchange format. It consists of attributes and values, and is not only machine-friendly but also easy for humans to read and interpret. This format is often chosen for data display, transmission, and storage on websites. CSV (Comma-Separated Values): As the name suggests, this is a method for storing tabular data in text format. In this format, each piece of data is a row, and each attribute is a column. When data is stored, all attributes are arranged in a specific order and separated by a specific symbol. This format is widely used for data import/export in software, web data transmission, and historical data storage. XML (Extensible Markup Language): Based on Standard Generalized Markup Language (SGML), XML is an extensible markup language. It allows users to customize their required tags and is suitable for creating documents containing structured data. XML is widely used in document presentation and online data exchange. Currently, open data commonly used in Taiwan are aggregated on the following platforms:\nTaiwan government open data platform (https://data.gov.tw/en/) Taiwan open weather data (https://opendata.cwb.gov.tw/devManual/insrtuction) Taiwan EPA open data platform (https://data.epa.gov.tw/en) Civil IoT Taiwan The “Civil IoT Taiwan” initiative is a major collaborative effort aimed at addressing four critical public needs, focusing on areas like air quality, earthquakes, water resources, and disaster prevention. This initiative is a part of the “Digital Technology” category within the larger “Forward-looking Infrastructure Development Program.” Key participants in this project include the Ministry of Science and Technology, the Ministry of Communications, the Ministry of Economic Affairs, the Ministry of the Interior, the Environmental Protection Agency, Academia Sinica, and the Council of Agriculture.\nThe core idea of this project is to harness the power of big data, artificial intelligence, and Internet of Things technologies to create advanced systems that enhance everyday life. These systems are designed to help both the government and the public tackle the challenges posed by environmental changes.\nOne of the key aspects of the Civil IoT Taiwan project is its inclusive approach, considering the needs and experiences of various users, including government agencies, academic institutions, businesses, and the general public. The overarching goals are to improve government efficiency, support the growth of academia and industry, and ultimately increase public well-being.\nThe Civil IoT Taiwan project encompasses four main areas:\nWater Resources: In partnership with central and local government water conservation agencies, the Civil IoT Taiwan project has set out to develop and deploy a range of water resource sensors. These include sensors for hydrological observation and for monitoring farmland irrigation. The goals are comprehensive:\nTo integrate information on surface and groundwater as well as new water sources. To create an IoT portal and a dynamic analysis and management platform for managing irrigation water, thereby enhancing flood control systems for river bureaus. To establish a cloud-based sewer management platform enabling remote, automated, and intelligent control for various levels of agencies. To facilitate combined usage of water resources and support applications like intelligent water management, dynamic irrigation analysis, sewage and sewer value-added applications, and flood warning systems for roads. Opening up data access aims to boost the development of water resource applications and collaborative decision-making between the public and private sectors. Air Quality: Collaborating with the Environmental Protection Agency, the Ministry of Economic Affairs, the Ministry of Science and Technology, and Academia Sinica, this project has launched an ambitious initiative to improve air quality monitoring.\nIt involves deploying a wide network of air quality sensors across Taiwan, establishing a sensor performance testing center, and developing various types of air quality sensors. A computing operation platform for the IoT and an intelligent environment sensing data center are being established to gather extensive data. A visualization platform for air quality data is being built to enhance IoT-based air quality monitoring and provide high-resolution data for intelligent environmental inspections, helping to pinpoint pollution hotspots for effective governance. The project also focuses on bolstering domestic technology in sensor development, aiming to establish Taiwan’s own airborne product sensing technology. Earthquakes: Recognizing Taiwan’s seismic activity, the project has significantly increased the number of quick-reporting earthquake stations for dense, high-quality data.\nIt has added and upgraded seismic and geophysical observatories, including GNSS stations, underground seismic stations, strong earthquake stations, geomagnetic stations, and groundwater stations. Enhancements are being made to monitor the Datun Volcano region, including advanced GNSS and seismic stations and drone surveillance. The project has also improved forecasting capabilities for earthquakes and tsunamis, expanded submarine optical cables, and established related stations. An integrated data management system for seismic and geophysical data is being developed to consolidate earthquake information, enhancing fault area monitoring and providing rapid, comprehensive earthquake reports for public early warnings and supporting earthquake disaster prevention industries. Disaster Prevention and Relief: A comprehensive “Civil Alert Information Platform” has been created, gathering 58 types of warning data (air, water, land, disaster, and livelihood) in one place.\nThis platform offers real-time disaster prevention and relief information to the public. Incorporating the EMIC2.0 system and other decision-assisting tools, it provides disaster personnel with crucial information on various disaster scenarios, notifications, and resources for effective decision-making. Historical data is being collected, standardized, and released to support the disaster prevention industry, enabling analysis and development of the disaster information chain. Civil IoT Taiwan Data Service Platform Additionally, the Civil IoT Taiwan project is establishing the Civil IoT Taiwan Data Service Platform, a key component designed to manage the diverse data generated by the project. This platform aims to offer stable and high-quality data for various environmental management needs. Embracing the concept of open data, the platform will:\nUse a standardized data format to ensure consistency and ease of use. Provide real-time data interfaces and historical data query services, making it easier for users to access current and past data. Enhance user experience by improving browsing and search speeds. Implement sensor data storage mechanisms that support scientific computing and artificial intelligence applications. The goal of the data service platform is to bridge the gap in environmental information availability. It will deliver more immediate and comprehensive environmental data, enabling the public to easily access and understand changes in their environment in real-time. This platform is not just for public awareness; it will also serve as a foundation for industrial innovation. The data it provides can be utilized for creating added value in various industries, fostering creativity among the populace, and offering high-quality solutions to their environmental challenges.\nCurrently, the Civil IoT Taiwan Data Service Platform uses the open data format of the OGC SensorThings API. Please refer to the following slides and pictures for relevant data format descriptions and data content in various fields:\nIntroduction to Civil IoT Taiwan Data Service Platform [PPT] Introduction to Civil IoT Taiwan Data Service Platform and OGC SensorThings API [Video] (in Chinese) Water Resources [PPT] Water Resources IoT [Video] (in Chinese) Air Quality [PPT] Environment Quality Sensing IoT [Video] (in Chinese) [PPT] Deployment of Micro PM2.5 Sensors [Video] (in Chinese) Earthquake [PPT] Joint Sea-Land Earthquake Observation [Video] (in Chinese) [PPT] Composite Earthquake Quick Report [Video] (in Chinese) Disaster prevention and relief [PPT] Civil Alert Open Data [Video] (in Chinese) [PPT] Integration of Disaster Prevention and Relief Information Systems [Video] (in Chinese) Since its inception in 2018, the Civil IoT Taiwan project has actively engaged the public and fostered innovation through a variety of initiatives:\nData Application Competitions: These competitions encourage participants to explore creative uses of the data generated by the Civil IoT Taiwan project, showcasing their skills in data manipulation and interpretation. Data Innovation Hackathons: These events bring together programmers, designers, and others interested in data innovation to collaborate intensively on software projects, using Civil IoT Taiwan data to create new solutions and applications. Physical and Virtual Exhibitions: These exhibitions provide platforms for participants to showcase their projects and innovations, allowing a broader audience to engage with and understand the potential applications of Civil IoT Taiwan data. In addition to these events, the project has developed comprehensive training materials and business coaching programs. These resources cover various aspects, including team building, idea development, and application services, to help participants effectively use the Civil IoT Taiwan Data Platform.\nOver the years, these efforts have culminated in several successful case studies. These cases demonstrate that the Civil IoT Taiwan project has evolved beyond just a government hardware initiative. It has become a foundational infrastructure for improving people’s livelihoods. By continuously providing high-quality sensor data, the project enhances everyday life and paves the way for more innovative, convenient, and compassionate information services.\nFor detailed examples and more information about applications and solutions utilizing Civil IoT Taiwan data in various fields, you can visit specific website resources:\nCivil IoT Taiwan Service and Solution Guide: Water resources Civil IoT Taiwan Service and Solution Guide: Air qualuty Civil IoT Taiwan Service and Solution Guide: Earthquake Civil IoT Taiwan Service and Solution Guide: Disaster prevention and relief References Open Definition: defining open in open data, open content, and open knowledge. Open Knowledge Foundation (https://opendefinition.org/od/2.1/en/) Civil IoT Taiwan (https://ci.taiwan.gov.tw) Civil IoT Taiwan Virtual Expo: Dialogue in Civil IoT (https://ci.taiwan.gov.tw/dialogue-in-civil-iot) Civil IoT Taiwan Service and Solution Guide (https://www.civiliottw.tadpi.org.tw) Civil IoT Taiwan Data Service Platform (https://ci.taiwan.gov.tw/dsp/) XML - Wikipedia (https://en.wikipedia.org/wiki/XML) JSON - Wikipedia (https://en.wikipedia.org/wiki/JSON) Comma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values) Standard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language) OGC SensorThings API Documentation (https://developers.sensorup.com/docs/) ",
    "description": "Introduction",
    "tags": [
      "Introduction"
    ],
    "title": "1. Introduction",
    "uri": "/en/ch1/"
  },
  {
    "content": "2. Overview of the Materials In this topic, we will introduce the overall structure of the developed materials of Civil IoT Taiwan Open Data, as well as the programming language Python and the development platform Google Colab used in the materials. In addition to conceptual descriptions, we also provide a large number of extended learning resources from shallow to deep, so that interested readers can further explore and learn according to their own needs.\n2.1. Material ArchitectureIntroduction of the material architecture\n2.2. Material ToolsA brief introduction of the programming language Python and the development platform Google Colab used in the materials\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "2. Overview of the Materials",
    "uri": "/en/ch2/"
  },
  {
    "content": "3. Data Access In this topic, we will describe how to directly access the data of the Civil IoT Taiwan Data Service Platform using simple Python syntax through the pyCIOT package. To gain a deeper understanding of the different ways to access data with examples, we divide this topic into two units for a more in-depth introduction:\n3.1. Basic Data Access MethodsWe introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.\n3.2. Data Access under Spatial or Temporal ConditionsWe introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "3. Data Access",
    "uri": "/en/ch3/"
  },
  {
    "content": "4. Time Series Data Analysis In this topic, we will introduce time series data analysis methods for IoT data. We will develop the following three units for a more in-depth exploration by using the Civil IoT Taiwan Data Service Platform.\n4.1. Time Series Data ProcessingWe use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.\n4.2. Time Series Data ForecastWe use the sensing data of the Civil IoT Taiwan Data Service Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare the prediction results of different data prediction models. We use graphics to present the data and discuss the significance of the data prediction of the dataset at different time resolutions in the real field, as well as possible derived applications.\n4.3. Time Series Data ClusteringWe introduce advanced data grouping analysis. We first present two time-series feature extraction methods, Fourier transform and wavelet transform, and briefly explain the similarities and differences between the two transform methods. We introduce two different time series comparison methods, Euclidean distance and dynamic time warping (DTW), and apply existing clustering algorithms accordingly. We then discuss data clustering with different temporal resolutions, as well as their representation and potential applications in the real world.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "4. Time Series Data Analysis",
    "uri": "/en/ch4/"
  },
  {
    "content": "5. Spatial Data Analysis In this topic, we introduce a series of spatial data processing and analysis methods according to the geospatial characteristics of IoT data. Through the use of the Civil IoT Taiwan Data Service Platform, we will develop the following units for a more in-depth introduction.\n5.1. Geospatial FilteringWe use Civil IoT Taiwan's earthquake and disaster prevention and relief data and filter the data of specific administrative areas by overlaying the administrative area boundary map obtained from the government open data platform. Then, we generate the image file of the data distribution location after the superimposed map. In addition, we also demonstrate how to Nest specific geometric topological regions and output nested results to files for drawing operations.\n5.2. Geospatial AnalysisWe use Civil IoT Taiwan's sensor data to introduce more advanced geospatial analysis. Using the GPS location coordinates in the site information, we first use the package to find the largest convex polygon (Convex Hull) to frame the geographical area covered by the sensor. Then we apply the Voronoi Diagram package to draw the distribution status of each sensor on the map according to the sensor and crops out the area of influence of each sensor. We adopt the spatial interpolation method for the space between sensors, applying different spatial interpolation algorithms. We then populate the spatial map with values based on the sensor values and generate a corresponding image output.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "5. Spatial Data Analysis",
    "uri": "/en/ch5/"
  },
  {
    "content": "6. Data Applications In this topic, we will focus on derivative applications using Civil IoT Taiwan open data. We strengthen the value and application services of Civil IoT Taiwan’s open data by importing other library packages and analysis algorithms. The units we expect to develop include:\n6.1. Machine Learning PreliminariesWe use air quality and water level data, combined with weather observations, using machine learning for data classification and data grouping. We demonstrate the standard process of machine learning and introduce how to further predict data through data classification and how to further explore data through data grouping.\n6.2. Anomaly DetectionWe use air quality data to demonstrate the anomaly detection framework commonly used in Taiwan's micro air quality sensing data. We learn by doing, step by step, from data preparation and feature extraction to data analysis, statistics, and induction. The readers will experience how to gradually achieve advanced and practical data application services by superimposing basic data analysis methods.\n6.3. Joint Data CalibrationWe use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "6. Data Applications",
    "uri": "/en/ch6/"
  },
  {
    "content": "7. System Integration and Applications In this theme, we will focus on the integration and application of the Civil IoTTaiwan Open Data and other application software, and through the professional functions of other application software, to further deepen and develop the value of the Civil IoT Taiwan Open Data.\n7.1. QGIS ApplicationWe introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.\n7.2. Tableau ApplicationWe introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.\n7.3. Leafmap ApplicationsWe introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.\n",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "7. System Applications",
    "uri": "/en/ch7/"
  },
  {
    "content": "This comprehensive set of materials on Civil IoT Taiwan data application is organized into seven distinct themes, each focusing on different aspects of data utilization and analysis. Here’s an overview of each theme:\nIntroduction: This section lays the foundation by introducing the Civil IoT Taiwan Project. It highlights the project’s achievements over the years and its significant impact on water resources, air quality, earthquake, and disaster prevention and relief. Through various materials, videos, and success stories, learners gain a profound understanding of the project’s relevance to everyday life.\nOverview of the Materials: Here, we outline the structure of the entire course, focusing on Python and the Google Colab platform. The section guides learners through the material’s layout and points them to additional resources for further exploration of related technologies.\nData Access Method: This critical part teaches how to access Civil IoT Taiwan Data using Python, specifically through a custom-developed tool, pyCIOT. It’s divided into two segments:\nBasic Data Access: Covers how to retrieve the latest sensor data from various fields and lists of all monitoring stations. Specific Spatio-temporal Data Access: Focuses on accessing data from a particular station at specific times or intervals and locating the latest data from stations near given coordinates. Alongside these methods, the section incorporates basic exploratory data analysis (EDA) and uses common statistical methods to describe data characteristics, concluding with simple data visualization techniques.\nData Analysis in Time Dimension: This section delves into time-series data analysis using Civil IoT Taiwan data, divided into three parts:\nTime Series Data Processing: Involves understanding moving averages, performing periodic analyses, disassembling time series data, and applying change point and outlier detection techniques. Time Series Data Forecast: Uses sensory data to compare different prediction methods and discusses the implications and potential applications of these forecasts. Time Series Data Clustering: Introduces advanced clustering analysis, including Fourier and wavelet transforms, and discusses the use of Euclidean and Dynamic Time Warping (DTW) distance for clustering. Data Analysis in Space Dimension: This theme focuses on spatial data analysis, covering:\nGeospatial Filtering: Demonstrates overlaying Civil IoT Taiwan data with administrative region maps and performing geospatial data distribution analyses. Geospatial Analysis: Teaches advanced techniques like constructing Convex Hulls, applying the Voronoi Diagram algorithm, and performing spatial interpolation. Data Applications: This practical segment explores the extended applications of Civil IoT Taiwan data:\nIntroduction to Machine Learning: Combines Civil IoT data with other datasets for predictive analytics, addressing machine learning processes, effectiveness evaluation, and avoiding biases. Anomaly Detection: Implements anomaly detection algorithms in air quality data, covering the entire process from data preparation to feature extraction and analysis. Dynamic Calibration Model: Demonstrates calibration models for air quality sensors, combining basic data analysis with machine learning. System Applications: This final theme integrates Civil IoT Taiwan data with various application software:\nQGIS Applications: Uses QGIS for organizing, analyzing, and presenting geospatial data from Civil IoT Taiwan. Tableau Applications: Demonstrates using Tableau for data visualization and integration with Civil IoT Taiwan data. Leafmap Applications: Shows how to use Leafmap, combined with Google Earth Engine, for advanced GIS applications and introduces the integration with Streamlit for web-based GIS services. This course material is designed to provide a thorough understanding and practical application of Civil IoT Taiwan data, leveraging Python and various data science tools to address public welfare challenges.\nReferences Civil IoT Taiwan. https://ci.taiwan.gov.tw Civil IoT Taiwan Data Platform. https://ci.taiwan.gov.tw/dsp/ QGIS - A Free and Open Source Geographic Information System. https://qgis.org/ Tableau - Business Intelligence and Analytics Software. https://www.tableau.com/ Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment. https://leafmap.org/ Streamlit - The fastest way to build and share data apps. https://streamlit.io/ Google Colaboratory. https://colab.research.google.com/ ",
    "description": "Introduction of the material architecture",
    "tags": [
      "Introduction"
    ],
    "title": "2.1. Material Architecture",
    "uri": "/en/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents What is pyCIOT? pyCIOT Basics Installation Import Package Data Access Air Quality Data Get all project codes: Air().get_source() Get all stations: Air().get_station() Get data of a station: Air().get_data() Water Resource Data Get all project codes: Water().get_source() Get all stations: Water().get_station() Get data of a station: Water().get_data() Earthquake Data Get all project codes: Quake().get_source() Get all stations: Quake().get_station() Get data of a station: Quake().get_data() Get data of an earthquake eventQuake().get_data() Weather Data Get all project codes: Weather().get_source() Get all stations: Weather().get_station() Get data of a station: Weather().get_data() CCTV Data Get all project codes: CCTV().get_source() Get data of a station: CCTV().get_data() Disaster Alert and Notification Data Get disaster alerts: Disaster().get_alert() Get historical data of disaster notifications: Disaster().get_notice() References This guide provides step-by-step instructions on how to use the pyCIOT package to access various data types such as air, water, earthquake, weather, CCTV, and disaster warnings from the Civil IoT Taiwan Data Service Platform. It covers methods to obtain the most recent sensor data from an individual location, a comprehensive list of all available locations, and the latest sensor data from every location.\nPlease note that this article is designed for individuals who have basic knowledge of operating a terminal and are familiar with the fundamental syntax of Python programming.\nWhat is pyCIOT? The Civil IoT Taiwan project offers a diverse range of data, each type having unique formats and access protocols. Despite being openly licensed, managing and processing this data can be challenging due to the varied methods required for downloading and handling it. To address these challenges, the pyCIOT suite was developed. This suite central\npyCIOT Basics Installation Firstly, we download and install the pyCIOT suite using pip. Pip is a package management system designed for Python, facilitating the installation and management of Python packages. The pyCIOT suite for this tutorial is hosted on the Python Package Index (PyPI). You can execute the following command in the terminal to install the pyCIOT library on your local machine, or to install other necessary packages alongside it:\n# Install the pyCIOT package using pip. !pip install pyCIOT Import Package To start using the package, simply input the import syntax and include pyCIOT.data in your Python script:\n# Import all functions and classes from the data module of pyCIOT. from pyCIOT.data import * The way you call a method from the package varies depending on how you import it. When using the from ... import ... syntax, there’s no need to include a prefix while calling a method. However, if you use the import ... approach, you must prefix the method name each time you use it. Alternatively, using import ... as ... allows you to call a method with a custom prefix specified after as.\n# Import the whole module and use it with the module name as a prefix. import pyCIOT.data a = pyCIOT.data.Air().get_source() # Import the module with an alias to shorten the module name. import pyCIOT.data as CIoT a = CIoT.Air().get_source() # Import all functions/classes from the module directly. Beware of name collisions in large projects. from pyCIOT.data import * a = Air().get_source() Data Access You can access data from the Civil IoT Taiwan project, including air, water, earthquake, weather, CCTV, and more, using the following methods:\n.get_source(): This function retrieves all project codes available on the Civil IoT Taiwan Data Service Platform, presented in an array format based on the type of data. .get_station(src=''): This method returns basic details of all stations in an array format. The src parameter is optional and can be used to specify a particular project code. .get_data(src='', stationID=''): This function provides basic information about all stations along with their most recent measurement results, also in an array format. Here, src is an optional parameter for specifying the project code, and stationID is also optional for specifying the device ID. For accessing disaster notification data, the following methods are used:\n.get_alert(): This retrieves alert data including details about the event, formatted in JSON. .get_notice(): This method returns notification data along with event information, also in JSON format. Please be aware that the pyCIOT package is continuously being updated. In cases where there is a discrepancy between this guide and the pyCIOT Package Document, the information in the package document should be considered as the most accurate and up-to-date.\nAir Quality Data Get all project codes: Air().get_source() # Use the get_source function of the Air class to retrieve all air-related project codes. a = Air().get_source() # Display the retrieved project codes. print(a) ['OBS:EPA', 'OBS:EPA_IoT', 'OBS:AS_IoT', 'OBS:MOST_IoT', 'OBS:NCNU_IoT'] Here are the valid project codes for accessing air quality data:\nOBS:EPA: This code refers to national-level monitoring stations operated by the Environmental Protection Agency (EPA). OBS:EPA_IoT: This is used for low-cost air quality stations also managed by the EPA. OBS:AS_IoT: These are micro air quality stations run by Academia Sinica. OBS:MOST_IoT: This code pertains to low-cost air quality Get all stations: Air().get_station() # Use the get_station function from the Air class, specifying the data source as EPA's Smart Urban Air Quality Micro-sensing, to retrieve the station list. b = Air().get_station(src=\"OBS:EPA_IoT\") # Retrieve and view the first five station items from the list b[0:5] [ { 'name': '智慧城鄉空品微型感測器-10287974676', 'description': '智慧城鄉空品微型感測器-10287974676', 'properties': { 'city': '新北市', 'areaType': '社區', 'isMobile': 'false', 'township': '鶯歌區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10287974676', 'locationId': 'TW040203A0507221', 'Description': '廣域SAQ-210', 'areaDescription': '鶯歌區' }, 'location': { 'latitude': 24.9507, 'longitude': 121.3408416, 'address': None } }, ... ] Get data of a station: Air().get_data() # Use the get_data function from the Air class, specifying the data source and station ID, to retrieve air quality data for the specified station. f = Air().get_data(src=\"OBS:EPA_IoT\", stationID=\"11613429495\") # View the retrieved air quality data. f [ {'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 30.6}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-28T06:23:08.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 9.8}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}} ] # Print the description of the air quality dataset print(f[0]['description']) for f_data in f[0]['data']: # If data described as \"temperature\" is found, print the corresponding value and timestamp. if f_data['description'] == '溫度': print(f_data['description'], ': ', f_data['values'][0]['value'], ' (', f_data['values'][0]['timestamp'], ')', sep='') 智慧城鄉空品微型感測器-11613429495 溫度: 30.6 (2022-08-28T06:22:08.000Z) Water Resource Data Get all project codes: Water().get_source() The function will return project names based on the specified input parameter:\nwater_level_station: Retrieves the names of projects related to water level monitoring. The current valid codes are WRA, WRA2, and IA. gate: Returns the names of projects associated with water gates. Valid codes for this category include WRA, WRA2, and IA. pumping_station: This parameter provides names of projects related to pumping stations, with valid codes being WRA2 and TPE. sensor: Lists the names of water sensor-related projects. The relevant codes here are WRA, WRA2, IA, and CPAMI. ``(none): When no parameter is specified, it returns the names of all water-related projects. # Use the get_source function of the Water class to retrieve all water-related project codes. wa = Water().get_source() # View the retrieved project codes. wa ['WATER_LEVEL:WRA_RIVER', 'WATER_LEVEL:WRA_GROUNDWATER', 'WATER_LEVEL:WRA2_DRAINAGE', 'WATER_LEVEL:IA_POND', 'WATER_LEVEL:IA_IRRIGATION', 'GATE:WRA', 'GATE:WRA2', 'GATE:IA', 'PUMPING:WRA2', 'PUMPING:TPE', 'FLOODING:WRA', 'FLOODING:WRA2'] Here are the valid project codes for accessing water resource data:\nWRA: Represents the Water Resource Agency. WRA2: Refers to the Water Resource Agency, in collaboration with county and city governments. IA: Stands for the Irrigation Agency. CPAMI: Denotes the Construction and Planning Agency. TPE: Indicates projects specific to Taipei City. Get all stations: Water().get_station() # Use the get_station function from the Water class, specifying the data source \"WATER_LEVEL:WRA_RIVER\", to retrieve the water level station list. wa = Water().get_station(src=\"WATER_LEVEL:WRA_RIVER\") # View the first item from the station list. wa[0] { 'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None} } Get data of a station: Water().get_data() # Use the get_data function from the Water class, specifying the data source and station ID, to retrieve water level data for the specified station. wa = Water().get_data(src=\"WATER_LEVEL:WRA_RIVER\", stationID=\"01790145-cd7e-4498-9240-f0fcd9061df2\") # View the retrieved water level data. wa [{'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'data': [{'name': '水位', 'description': ' Datastream_id=016e5ea0-7c7f-41a2-af41-eabacdbb613f, Datastream_FullName=延平.水位, Datastream_Description=現場觀測, Datastream_Category_type=河川水位站, Datastream_Category=水文', 'values': [{'timestamp': '2022-08-28T06:00:00.000Z', 'value': 157.41}]}], 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None}}] Earthquake Data Get all project codes: Quake().get_source() # Use the get_source function of the Quake class to retrieve all quake-related project codes. q = Quake().get_source() # View the retrieved project codes. q ['EARTHQUAKE:CWB+NCREE'] The valid project code for accessing earthquake data is:\nEARTHQUAKE:CWB+NCREE: This code is used for seismic monitoring stations jointly operated by the Central Weather Bureau and the National Center for Research on Earthquake Engineering. Get all stations: Quake().get_station() # Use the get_station function from the Quake class, specifying the data source \"EARTHQUAKE:CWB+NCREE\", to retrieve the earthquake station list. q = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") # View the first two items from the station list. q[0:2] [{'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'properties': {'authority': '中央氣象局', 'stationID': 'EGC', 'deviceType': 'FBA', 'stationName': 'Jiqi'}, 'location': {'latitude': 23.708, 'longitude': 121.548, 'address': None}}, {'name': '地震監測站-Xilin-ESL', 'description': '地震監測站-Xilin-ESL', 'properties': {'authority': '中央氣象局', 'stationID': 'ESL', 'deviceType': 'FBA', 'stationName': 'Xilin'}, 'location': {'latitude': 23.812, 'longitude': 121.442, 'address': None}}] Get data of a station: Quake().get_data() # Use the get_data function from the Quake class, specifying the data source \"EARTHQUAKE:CWB+NCREE\", to retrieve earthquake data. q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\") # View the last record from the data. q[-1] {'name': '第2022083號地震', 'description': '第2022083號地震', 'properties': {'depth': 43.6, 'authority': '中央氣象局', 'magnitude': 5.4}, 'data': [ { 'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18815)' },{ 'name': '地震監測站-Taichung City-TCU', 'description': '地震監測站-Taichung City-TCU', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18816)' }, ... ] 'location': {'latitude': 22.98, 'longitude': 121.37, 'address': None}} Get data of an earthquake eventQuake().get_data() # Use the get_data function from the Quake class, specifying the data source and event ID, to retrieve earthquake data for the specified event. q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\", eventID=\"2022083\") # View the retrieved earthquake data. q Weather Data Get all project codes: Weather().get_source() # Use the get_source function of the Weather class to retrieve all weather-related project codes. w = Weather().get_source() # View the retrieved project codes. w ['GENERAL:CWB', 'GENERAL:CWB_IoT', 'RAINFALL:CWB', 'RAINFALL:WRA', 'RAINFALL:WRA2', 'RAINFALL:IA', 'IMAGE:CWB'] The function returns project names based on the given input parameter:\nGENERAL: Retrieves names of projects related to weather stations. RAINFALL: Lists names of projects associated with rainfall monitoring stations. IMAGE: Provides names of projects involving radar integrated echo maps. (none): When no parameter is specified, it returns the names of all weather-related projects. Valid project codes for weather data include:\nGENERAL:CWB: For standard weather stations operated by the Central Weather Bureau. GENERAL:CWB_IoT: Refers to automatic weather monitoring stations by the Central Weather Bureau. RAINFALL:CWB: Codes for rainfall monitoring stations managed by the Central Weather Bureau. RAINFALL:WRA: Represents rainfall monitoring stations run by the Water Resource Agency. RAINFALL:WRA2: For rainfall monitoring stations by the Water Resource Agency, in collaboration with county and city governments. RAINFALL:IA: Indicates rainfall monitoring stations managed by the Irrigation Agency. IMAGE:CWB: Used for radar integrated echo maps created by the Central Weather Bureau. Get all stations: Weather().get_station() # Use the get_station function from the Weather class, specifying the data source \"RAINFALL:CWB\", to retrieve the rainfall station list. w = Weather().get_station(src=\"RAINFALL:CWB\") # View the retrieved station list. w [{'name': '雨量站-C1R120-上德文', 'description': '雨量站-C1R120-上德文', 'properties': {'city': '屏東縣', 'township': '三地門鄉', 'authority': '中央氣象局', 'stationID': 'C1R120', 'stationName': '上德文', 'stationType': '局屬無人測站'}, 'location': {'latitude': 22.765, 'longitude': 120.6964, 'address': None}}, ... ] Get data of a station: Weather().get_data() # Use the get_data function from the Weather class, specifying the data source and station ID, to retrieve rainfall data for the specified station. w = Weather().get_data(src=\"RAINFALL:CWB\", stationID=\"U2HA40\") # View the retrieved rainfall data. w [{'name': '雨量站-U2HA40-臺大內茅埔', 'description': '雨量站-U2HA40-臺大內茅埔', 'properties': {'city': '南投縣', 'township': '信義鄉', 'authority': '中央氣象局', 'stationID': 'U2HA40', 'stationName': '臺大內茅埔', 'stationType': '中央氣象局'}, 'data': [{'name': 'HOUR_12', 'description': '12小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'MIN_10', 'description': '10分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'RAIN', 'description': '60分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_6', 'description': '6小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_3', 'description': '3小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_24', 'description': '24小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'NOW', 'description': '本日累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'ELEV', 'description': '高度', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 507.0}]}], 'location': {'latitude': 23.6915, 'longitude': 120.843, 'address': None}}] CCTV Data Get all project codes: CCTV().get_source() # Use the get_source function of the CCTV class to retrieve all CCTV-related project codes. cctv = CCTV().get_source() # View the retrieved project codes. cctv ['IMAGE:EPA', 'IMAGE:WRA', 'IMAGE:COA'] The valid project codes for accessing CCTV data are:\nIMAGE:EPA: This code is used for real-time images from air quality monitoring stations managed by the Environmental Protection Agency (EPA). IMAGE:WRA: Refers to images related to water conservancy and disaster prevention provided by the Water Resource Agency. IMAGE:COA: Represents real-time images from landslide observation stations operated by the Council of Agriculture. Get data of a station: CCTV().get_data() # Use the get_data function from the CCTV class, specifying the data source \"IMAGE:EPA\", to retrieve CCTV image data from the EPA. cEPA = CCTV().get_data(\"IMAGE:EPA\") # View the third record from the data. cEPA[2] { 'name': '環保署空品監測即時影像器-萬里', 'description': '環保署-萬里-空品監測即時影像器', 'properties': { 'city': '新北市', 'basin': '北部空品區', 'authority': '環保署', 'stationName': '萬里', 'CCDIdentifier': '3'}, 'data': [ { 'name': '即時影像', 'description': '環保署-萬里-空品監測即時影像器', 'values': [ { 'timestamp': '2022-06-13T09:00:00.000Z', 'value': 'https://airtw.epa.gov.tw/AirSitePic/20220613/003-202206131700.jpg' } ] } ], 'location': {'latitude': 25.179667, 'longitude': 121.689881, 'address': None} } # Use the get_data function from the CCTV class, specifying the data source \"IMAGE:COA\", to retrieve CCTV image data from the COA. cCOA = CCTV().get_data(\"IMAGE:COA\") # View the first record from the data. cCOA[0] {'name': '行政院農委會土石流觀測站影像-大粗坑下游攝影機', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'properties': {'city': '新北市', 'township': '瑞芳鎮', 'StationId': '7', 'authority': '行政院農委會', 'stationName': '大粗坑下游攝影機', 'CCDIdentifier': '2'}, 'data': [{'name': '即時影像', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'values': [{'timestamp': '2099-12-31T00:00:00.000Z', 'value': 'http://dfm.swcb.gov.tw/debrisFinal/ShowCCDImg-LG.asp?StationID=7\u0026CCDId=2'}]}], 'location': {'latitude': 25.090878, 'longitude': 121.837815, 'address': '新北市瑞芳鎮弓橋里大粗坑'}} Disaster Alert and Notification Data The Civil IoT Taiwan Data Service Platform offers an extensive array of disaster-related information, featuring over 58 different disaster alerts and more than 41 distinct disaster notifications. Various units handle specific types of disaster alerts, while the central disaster response center is tasked with issuing disaster notifications.\nFor a comprehensive list of project codes related to these services, you can refer to the pyCIOT Package Document.\nGet disaster alerts: Disaster().get_alert() # Use the get_alert function from the Disaster class, specifying the parameter as \"5\", to retrieve disaster alert information. d = Disaster().get_alert(\"5\") # View the retrieved disaster alert information. d {'id': 'https://alerts.ncdr.nat.gov.tw/Json.aspx', 'title': 'NCDR_CAP-即時防災資訊(Json)', 'updated': '2021-10-12T08:32:00+08:00', 'author': {'name': 'NCDR'}, 'link': {'@rel': 'self', '@href': 'https://alerts.ncdr.nat.gov.tw/JSONAtomFeed.ashx?AlertType=5'}, 'entry': [ { 'id': 'CWB-Weather_typhoon-warning_202110102030001', 'title': '颱風', 'updated': '2021-10-10T20:30:05+08:00', 'author': {'name': '中央氣象局'}, 'link': {'@rel': 'alternate', '@href': 'https://b-alertsline.cdn.hinet.net/Capstorage/CWB/2021/Typhoon_warnings/fifows_typhoon-warning_202110102030.cap'}, 'summary': { '@type': 'html', '#text': '1SEA18KOMPASU圓規2021-10-10T12:00:00+00:0018.20,126.702330992150輕度颱風TROPICAL STORM2021-10-11T12:00:00+00:0019.20,121.902835980220輕度颱風 圓規（國際命名 KOMPASU）10日20時的中心位置在北緯 18.2 度，東經 126.7 度，即在鵝鑾鼻的東南東方約 730 公里之海面上。中心氣壓 992 百帕，近中心最大風速每秒 23 公尺（約每小時 83 公里），相當於 9 級風，瞬間最大陣風每秒 30 公尺（約每小時 108 公里），相當於 11 級風，七級風暴風半徑 150 公里，十級風暴風半徑 – 公里。以每小時21公里速度，向西進行，預測11日20時的中心位置在北緯 19.2 度，東經 121.9 度，即在鵝鑾鼻的南南東方約 320 公里之海面上。根據最新資料顯示，第18號颱風中心目前在鵝鑾鼻東南東方海面，向西移動，其暴風圈正逐漸向巴士海峽接近，對巴士海峽將構成威脅。巴士海峽航行及作業船隻應嚴加戒備。第18號颱風外圍環流影響，易有短延時強降雨，今(10日)晚至明(11)日基隆北海岸、宜蘭地區及新北山區有局部大雨發生的機率，請注意。＊第18號颱風及其外圍環流影響，今(10日)晚至明(11)日巴士海峽及臺灣附近各海面風浪逐漸增大，基隆北海岸、東半部（含蘭嶼、綠島）、西南部、恆春半島沿海易有長浪發生，前往海邊活動請特別注意安全。＊第18號颱風外圍環流影響，今(10日)晚至明(11)日臺南以北、東半部(含蘭嶼、綠島)、恆春半島、澎湖、金門、馬祖沿海及空曠地區將有9至12級強陣風，內陸地區及其他沿海空曠地區亦有較強陣風，請注意。＊第18號颱風外圍環流沉降影響，明(11)日南投、彰化至臺南及金門地區高溫炎熱，局部地區有36度以上高溫發生的機率，請注意。＊本警報單之颱風半徑為平均半徑，第18號颱風之7級風暴風半徑西南象限較小約60公里，其他象限約180公里，平均半徑約為150公里。'}, 'category': {'@term': '颱風'} },{ ... }, ... ] } Get historical data of disaster notifications: Disaster().get_notice() # Use the get_notice function from the Disaster class, specifying the parameter as \"ERA2_F1\", to retrieve the transportation disaster report (road and bridge section). d = Disaster().get_notice(\"ERA2_F1\") # 交通災情通報表（道路、橋梁部分） # View the retrieved transportation disaster report information. d \"maincmt\":{ \"prj_no\":\"專案代號\", \"org_name\":\"填報機關\", \"rpt_approval\":\"核定人\", \"rpt_phone\":\"聯絡電話\", \"rpt_mobile_phone\":\"行動電話\", \"rpt_no\":\"通報別\", \"rpt_user\":\"通報人\", \"rpt_time\":\"通報時間\" }, \"main\":{ \"prj_no\":\"2014224301\", \"org_name\":\"交通部公路總局\", ... }, \"detailcmt\":{ \"trfstatus\":\"狀態\", ... }, ... } References Python pyCIOT pypi package (https://pypi.org/project/pyCIOT/) Python pyCIOT Document (https://hackmd.io/@cclljj/pyCIOT_doc) ",
    "description": "We introduce how to obtain water, air, earthquake, and disaster data in the Civil IoT Taiwan Data Service Platform, including the latest sensing data for a single site, a list of all sites, and the latest current sensing data for all sites.",
    "tags": [
      "Python",
      "API",
      "Water",
      "Air",
      "Quake",
      "Disaster"
    ],
    "title": "3.1. Basic Data Access Methods",
    "uri": "/en/ch3/ch3.1/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Visualization Data Resample Moving Average Multi-line Charts Calendar Heatmap Data Quality Inspection Outlier Detection Change Point Detection) Missing Data Handling Data Decomposition References Time series data is data formed in the order of appearance in time. Usually, the time interval in the data will be the same (for example, one data every five minutes, or one data per hour), and the application fields are quite wide, such as financial information, space engineering, signal processing, etc. There are also many statistical related tools that can used in analysis. In addition, time series data is very close to everyday life. For example, with the intensification of global climate change, the global average temperature has become higher and higher in recent years, and the summer is unbearably hot. Also, certain seasons of the year tend to have particularly poor air quality, or certain times of the year tend to have worse air quality than others. If you want to know more about these changes in living environment, and how the corresponding sensor values change, you will need to use time series data analysis, which is to observe the relationship between data and time, and then get the results. This chapter will demonstrate using three types of data (air quality, water resources, weather) in the Civil IoT Taiwan Data Service Platform.\nGoal observe time series data using visualization tools check and process time series data decompose time series data to investigate its trend and seasonality Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, seaborn, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and calplot, which need to be installed by :\n!pip install --upgrade pip # Kats !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # calplot !pip install calplot After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import calplot import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import statsmodels.api as sm import os, zipfile from datetime import datetime, timedelta from dateutil import parser as datetime_parser from statsmodels.tsa.stattools import adfuller, kpss from statsmodels.tsa.seasonal import seasonal_decompose from kats.detectors.outlier import OutlierDetector from kats.detectors.cusum_detection import CUSUMDetector from kats.consts import TimeSeriesData, TimeSeriesIterator from IPython.core.pylabtools import figsize Data Access We use pandas for data processing. Pandas is a data science suite commonly used in Python language. It can also be thought of as a spreadsheet similar to Microsoft Excel in a programming language, and its Dataframe object provided by pandas can be thought of as a two-dimensional data structure. The dimensional data structure can store data in rows and columns, which is convenient for various data processing and operations.\nThe topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Visualization The first step in the processing of time series data is nothing more than to present the information one by one in chronological order so that users can see the changes in the overall data and derive more ideas and concepts for data analysis. Among them, using line charts to display data is the most commonly used data visualization method. For example, take the air quality data as an example:\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Data Resample As can be seen from the air quality data sequence diagram in the above figure, the distribution of data is actually very dense, and the changes in data values are sometimes small and violent. This is because the current sampling frequency of air quality data is about once every five minutes, and the collected environment is the surrounding environment information in life, so data density and fluctuation are inevitable. Because sampling every 5 minutes is too frequent, it is difficult to show general trends in ambient air pollution. Therefore, we adopt the method of resampling to calculate the average value of the data in a fixed time interval, so as to present the data of different time scales. For example, we use the following syntax to resample at a larger scale (hour, day, month) sampling rate according to the characteristics of the existing air quality data:\nair_hour = air.resample('H').mean() # hourly average air_day = air.resample('D').mean() # daily average air_month = air.resample('M').mean() # monthly average print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 Then we plot again with the hourly averaged resampling data, and we can see that the curve becomes clearer, but the fluctuation of the curve is still very large.\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air_hour[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() Moving Average For the chart of the original data, if you want to see a smoother change trend of the curve, you can apply the moving average method. The idea is to set a sampling window on the time axis of the original data, move the position of the sampling window smoothly, and calculate the average value of all values in the sampling window.\nFor example, if the size of the sampling window is 10, it means that the current data and the previous 9 data are averaged each time. After such processing, the meaning of each data is not only a certain time point, but the average value of the original time point and the previous time point. This removes abrupt changes and makes the overall curve smoother, thereby making it easier to observe overall trends.\n# plt.figure(figsize=(15, 10), dpi=60) MA = air_hour MA10 = MA.rolling(window=500, min_periods=1).mean() MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # MA10.plot(figsize(15, 10)) The blue line in the above figure is the original data, and the orange line is the curve after moving average. It can be clearly found that the orange line can better represent the change trend of the overall value, and there is also a certain degree of regular fluctuation, which is worthy of further analysis.\nMulti-line Charts In addition to presenting the original data in the form of a simple line chart, another common data visualization method is to cut the data into several continuous segments periodically in the time dimension, draw line charts separately, and superimpose them on the same multi-line chart. For example, we can cut the above air quality data into four sub-data sets of 2019, 2020, 2021, and 2022 according to different years, and draw their respective line charts on the same multi-line chart, as shown in the figure below.\nair_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] years = air_month['year'].unique() print(air) np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) plt.show() In this multi-line chart, we can see that the 2019 data has a significant portion of missing values, while the 2022 data was only recorded till July at the time of writing. At the same time, it can be found that in the four-year line chart, the curves of different years all reached the lowest point in summer, began to rise in autumn, and reached the highest point in winter, showing roughly the same trend of change.\nCalendar Heatmap The calendar heat map is a data visualization method that combines the calendar map and the heat map, which can more intuitively browse the distribution of the data and find the regularity of different time scales. We use calplot, a calendar heatmap suite for Python, and input the daily PM2.5 averages. Then we select the specified color (the parameter name is cmap, and we set it to GnBu in the following example. for detailed color options, please refer to the reference materials), and we can get the effect of the following figure, in which blue represents the greater value, green or white means lower values, if not colored or a value of 0 means there is no data for the day. From the resulting plot, we can see that the months in the middle part (summer) are lighter and the months in the left part (winter) are darker, just in line with our previous observations using the multi-line chart.\n# cmap: color map (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: specify the format of the text pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") Data Quality Inspection After the basic visualization of time series data, we will introduce the basic detection and processing methods of data quality. We will use kats, a Python language data processing and analysis suite, to perform outlier detection, change point detection, and handling missing values in sequence.\nOutlier Detection Outliers are those values in the data that are significantly different from other values. These differences may affect our judgment and analysis of the data. Therefore, outliers need to be identified and then flagged, removed, or treated specially. .\nWe first convert the data stored in the variable air_hour from its original dataframe format to the TimeSeriesData format used by the kats package and save the converted data into a variable named air_ts. Then we re-plot the line chart of the time series data.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') air_ts.plot(cols=[\"PM25\"]) We then used the OutlierDetector tool in the kats suite to detect outliers in the time series data, where outliers were less than 1.5 times the first quartile (Q1) minus the interquartile range (IQR) or greater than The third quartile (Q3) value plus 1.5 times the interquartile range.\noutlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] Finally, we delete the detected outliers from the original data, and re-plot the chart to compare it with the original one. We can clearly find some outliers (for example, there is an abnormal peak in 2022-07) have been removed.\noutliers_removed = outlierDetection.remover(interpolate=False) outliers_removed outliers_removed.plot(cols=['y_0']) Change Point Detection) A change point is a point in time at which the data suddenly changes significantly, representing the occurrence of an event, a change in the state of the data, or a change in the distribution of the data. Therefore, change point detection is often regarded as an important preprocessing step for data analysis and data prediction.\nIn the following example, we use daily averages of air quality data for change point detection. We use the TimeSeriesData data format of the kats package to store the data and use the CUSUMDetector detector provided by kats for detection. We use red dots to represent detected change points in the plot. Unfortunately, in this example, no obvious point of change was observed. Readers are advised to refer to this example and bring in other data for more exercise and testing.\nair_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') detector = CUSUMDetector(air_ts) change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # print(\"The change point is on\", change_points[0][0].start_time) # plot the results plt.xticks(rotation=45) detector.plot(change_points) plt.show() Missing Data Handling Missing data is a common problem when conducting big data analysis. Some of these missing values are already missing at the time of data collection (such as sensor failure, network disconnection, etc.), and some are eliminated during data preprocessing (outliers or abnormality). However, for subsequent data processing and analysis, we often need the data to maintain a fixed sampling rate to facilitate the application of various methods and tools. Therefore, various methods for imputing missing data have been derived. Below we introduce three commonly used methods:\nMark missing data as Nan (Not a number): Nan stands for not a number and is used to represent undefined or unrepresentable values. If it is known that subsequent data analysis will additionally deal with these special cases of Nan, this method can be adopted to maintain the authenticity of the information. Forward filling method: If Nan has difficulty in subsequent data analysis, missing values must be filled with appropriate numerical data. The easiest way to do this is forward fill, which uses the previous value to fill in the current missing value. # forward fill df_ffill = air.ffill(inplace=False) df_ffill.plot() 3. \u0006K-Nearest Neighbor (KNN) method: As the name suggests, the KNN method finds the k values that are closest to the missing value, and then fills the missing value with the average of these k values.\ndef knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): n_by_2 = np.ceil(n/2) lower = np.max([0, int(i-n_by_2)]) upper = np.min([len(ts)+1, int(i+n_by_2)]) ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) out[i] = np.nanmean(ts_near) return out # KNN df_knn = air.copy() df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) df_knn.plot() Data Decomposition In the previous example of basic data processing, we have been able to roughly observe the changing trend of data values and discover potential regular changes. In order to further explore the regularity of time series data changes, we introduce the data decomposition method. In this way, the original time series data can be disassembled into trend waves (trend), periodic waves (seasonal) and residual waves (residual).\nWe first replicated the daily average data of air quality data as air_process and processed the missing data using forward filling. Then, we first presented the raw data directly in the form of a line chart.\nair_process = air_day.copy() # new.round(1).head(12) air_process.ffill(inplace=True) air_process.plot() Then we use the seasonal_decompose method to decompose the air_process data, in which we need to set a period parameter, which refers to the period in which the data is decomposed. We first set it to 30 days, and then after execution, it will produce four pictures in sequence: raw data, trend chart, seasonal chart, and residual chart.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) decompose.plot().set_size_inches((15, 15)) plt.show() In the trend graph (trend), we can also find that the graph of the original data has very similar characteristics, with higher values around January and lower values around July; while in the seasonal graph (seasonal), we can find that the data has a fixed periodic change in each cycle (30 days), which means that the air quality data has a one-month change.\nIf we change the periodic variable to 365, i.e. decompose the data on a larger time scale (one year), we can see a trend of higher values around January and lower values around July from the seasonal plot, and this trend change occurs on a regular and regular basis. At the same time, it can be seen from the trend chart that the overall trend is slowing down, indicating that the concentration of PM2.5 is gradually decreasing under the overall trend. The results also confirmed our previous findings that no change points were detected, as the change trend of PM2.5 was a steady decline without abrupt changes.\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) decompose.plot().set_size_inches((15, 15)) plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "We use the sensing data of Civil IoT Taiwan Data Service Platform to guide readers to understand the use of moving average, perform periodic analysis of time series data, and then disassemble the time series data into long-term trends, seasonal changes and residual fluctuations. At the same time, we apply the existing Python language suites to perform change point detection and outlier detection to check the existing Civil IoT Taiwan data, and discuss potential implications of such values detected.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.1. Time Series Data Processing",
    "uri": "/en/ch4/ch4.1/"
  },
  {
    "content": "\nTable Of Contents Intersect Buffer Multi-ring buffer Distance matrix Brief Summary In the past, if we wanted to learn about things like air quality, earthquakes, or floods, we usually depended on information from public sources or experts to understand how widespread and intense these events were.\nImagine you want to check the air quality around your home. The Taiwan Air Quality Monitoring Network, managed by the Environmental Protection Administration Executive Yuan, is a key source of this info. But these high-tech weather stations are expensive and not very numerous, so the closest one might be 10 kilometers away. This distance leads to a question: does the air quality stay the same over such a distance? On the other hand, smaller, less expensive sensors used in the Internet of Things (IoT) can be placed closer to where we live. These sensors can give us a clearer picture of how air quality is affected by local elements like schools, busy streets, temples, or even home cooking. So, how do we find and use data from the right sensor stations?\nEvery IoT station has a specific location. Stations closer to each other often record similar data because they are in similar environments. This idea is summarized in the first law of geography by Waldo R. Tobler: “All things are related, but nearby things are more related than distant things.”\nHowever, different factors around each station can cause variations in the data. To ensure we get reliable data, we need to look at each station as a central point, choose nearby stations based on their administrative area or a specific distance (like a radius), and then display this information in charts or maps.\nIn this chapter, we’ll learn how to select and use spatial data from air quality monitoring stations (Environmental Protection Administration, EPA), weather stations (Central Weather Bureau, CWB), and flood sensors (Water Resources Agency, WRA).\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # downlaod the county boundary shpfile from open database !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # get flood sensors' data by pyCIOT wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") wea_list = CIoT.Weather().get_station('GENERAL:CWB') county = gpd.read_file('county.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] # convert to geopandas.GeoDataFrame flood_list = wa + wa2 flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") weather_df = pd.DataFrame([],columns = ['name','lon', 'lat']) for i in wea_list: #print(i['data'][0]) df = pd.DataFrame([[i['name'],i['location']['longitude'],i['location']['latitude']]],columns = ['name','lon', 'lat']) weather_df = pd.concat([weather_df,df]) #print(df) result_df = weather_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) gdf_weather = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") Intersect In simple terms, we can focus on areas like villages or towns as our main points of interest. By crossing different sets of data, we can identify the specific station IDs located within one of these areas. With the help of an API, we’re able to access real-time data, as well as averages calculated over an hour, a day, or even a week, from these stations. Additionally, this process allows us to compare stations within the same area to see if their data trends are similar. We can also spot any stations that are reporting data that stands out or is very different from the rest.\nimport matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=gdf_weather) # this is plotting the datapoints from the EPA dataframe basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # plotting the city's boundaries here, with facecolor = none to # remove the polygon's fill color plt.tight_layout(); # selecting the polygon's geometry field to filter out points that basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_weather, basemap, how='intersection') fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=intersected_data) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Buffer Additionally, some monitoring stations might be right on the edge of different areas. If we just use the official area borders to choose stations, our results might not be very accurate. To solve this, we use a method called ‘buffering’. This means we pick a station, then draw an imaginary circle around it using a set distance. This circle helps us include stations that are close by, even if they’re just outside the area’s official border.\nWith this ‘buffering’ idea, we can also pick important places like schools, parks, or factories as the center points and then look for stations near them. Also, if we use things like roads, rivers, or even larger areas like parks or industrial zones as our starting points, we can create a search area. This helps us find the exact monitoring station we need, in a much more precise way.\n# set the buffer distance for 0.05 degree fig, ax = plt.subplots(figsize=(6, 10)) buffer = intersected_data.buffer(0.05) buffer.plot(ax=ax, alpha=0.5) intersected_data.plot(ax=ax, color='red', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Multi-ring buffer We can create circles with different sizes around a central point, like targets with multiple rings. This helps us group stations that are close to each other based on how far away they are. We can then see if stations that are closer together have more similar readings or trends.\n# set the buffer distance 0.05 degree=blue；0.1 degree=green；0.2 degree=orange；0.3 degree=red fig, ax = plt.subplots(figsize=(6, 10)) buffer_03 = intersected_data.buffer(0.3) buffer_03.plot(ax=ax, color='red', alpha=1) buffer_02 = intersected_data.buffer(0.2) buffer_02.plot(ax=ax, color='orange', alpha=1) buffer_01 = intersected_data.buffer(0.1) buffer_01.plot(ax=ax, color='green', alpha=1) buffer_005 = intersected_data.buffer(0.05) buffer_005.plot(ax=ax, alpha=1) intersected_data.plot(ax=ax, color='black', alpha=0.5) # intersect with the flood sensors buffer = gpd.GeoDataFrame(buffer_03,geometry=buffer_03) buffer = buffer.to_crs(4326) intersected_flood = gpd.overlay(gdf_flood, buffer, how='intersection') intersected_flood.plot(ax=ax, color='lightgray', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); Distance matrix Each observation station has its own set of precise location coordinates. By using these coordinates along with trigonometric functions, we can calculate the exact distance between any two stations. This calculation lets us create what’s known as a distance matrix for all the stations. A distance matrix is simply a way to see at a glance how far apart different stations are from each other. This is especially useful for quickly checking if two stations are geographically close to one another (as shown in graph 3). For instance, in Taipei, we can apply this method to the locations of flood sensors. By converting their locations into a distance matrix, we can easily determine if these sensors are located near each other during flood events.\ngdf_weather[\"ID\"] = gdf_weather.index df = pd.DataFrame(gdf_weather, columns=['ID','lon', 'lat']) df.set_index('ID') # convert to the dustance matrix from scipy.spatial import distance_matrix pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index) Brief Summary Using the methods described earlier, we can develop a station selection mechanism based on administrative regions or topological concepts. Additionally, we can explore the relationships between sensor values by analyzing the geospatial features of these stations.\nReferences Geopanda documentation (https://geopandas.org/en/stable/docs.html) Introduction to the shp file (https://en.wikipedia.org/wiki/Shapefile) Taiwan’s Datum and coordinate system (https://wiki.osgeo.org/wiki/Taiwan_datums) Introduction to WGS 84 (EPSG:4326) coordinate system (https://epsg.io/4326) Introduction to TWD 97 (EPSG:3826) coordinate system (https://epsg.io/3826) ",
    "description": "We use Civil IoT Taiwan's earthquake and disaster prevention and relief data and filter the data of specific administrative areas by overlaying the administrative area boundary map obtained from the government open data platform. Then, we generate the image file of the data distribution location after the superimposed map. In addition, we also demonstrate how to Nest specific geometric topological regions and output nested results to files for drawing operations.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "5.1. Geospatial Filtering",
    "uri": "/en/ch5/ch5.1/"
  },
  {
    "content": "\nTable Of Contents Preamble Classification Clustering Package Installation and Importing Case 1: Location Type Classification of Air Quality Sensors Data Cleansing Outlier removal Train data and test data Using the pre-built Sklearn models Case 2: Clustering of Air Quality Sensors Data download and preprocessing Dynamic Time Warping (DTW) K-Mean clustering Correlation between data clusters and geographical locations Correlation between data clusters and wind directions Case 3: Clustering and Classification Application of Water and Meteorological Data Data download and preprocessing Correlation of flood sensors and rain gauges Data clustering to find highly correlated rain gauges Data classification to predict flooding by rainfall data References Preamble In the earlier sections, we’ve explored the extensive open data available through Civil IoT Taiwan. We’ve also looked at how this data can be analyzed and processed, focusing on different aspects of time and space. In this section, we’re going to delve deeper into how machine learning can be applied to this data. We’ll introduce two fundamental machine learning concepts: classification and clustering.\nClassification In machine learning, a common task is called a classification problem. Think of it like sorting things into different categories. Imagine we have a bunch of data points, let’s call this group X, and we’ve sorted each one into specific categories, which we’ll call Y. The challenge in classification is to create a smart tool, known as a classifier, that can look at new, unsorted data (X') and correctly predict which category (Y') they belong to, based on what it has learned from our sorted data.\nSo, the main goal here is to build a really good classifier. To do this, we start by creating a model. This model is trained using our categorized data, which teaches it to recognize patterns and make good predictions. We want our model to be good at understanding how our data is spread out and then use it to guess the categories of new, unseen data.\nThis whole process is what’s called supervised learning in the field of machine learning. Some popular types of classifiers are Nearest Neighbors, SVM Classifier, Decision Tree, and Random Forest. We won’t go into the details of how each model works in future articles but will use them as tools. For those interested in learning more about these models, plenty of resources are out there for a deeper dive.\nClustering Clustering issues are a lot like classification issues, with a key difference. In classification, we use data with known labels to understand data with unknown labels. Clustering, on the other hand, is like starting from scratch. It involves organizing data into different groups without any prior labels.\nTo put it in a more mathematical way, imagine we have a bunch of data, X, with no labels at all. The challenge of clustering is to split this X data into k groups using a specific method. The goal is to make sure that data within each group are very similar to each other, but quite different from data in other groups.\nThe methods we use for clustering depend heavily on the nature of the data. These methods look at how similar or different the data points are from one another. They try to bring similar data points together, while keeping the different ones separate. Some well-known clustering techniques include K-Means, DBSCAN, Hierarchical Clustering, and BIRCH. In future articles, we won’t go into the details of each method. Instead, we’ll use them as tools. If you’re curious about these methods, you can find more information in other resources to explore them further.\nPackage Installation and Importing In this article, we’re working with several software packages that are already set up and ready to use on Google Colab, our development platform. These include andas, numpy, matplotlib, json, os, glob, math, seaborn, tqdm, datetime, geopy, scipy, and warnings. You won’t need to install these separately.\nHowever, there are a few additional tools we’ll need which aren’t included in Google Colab by default. These are the pyCIOT, fastdtw, and sklearn packages, along with the TaipeiSansTCBeta-Regular font. We’ll need to install these ourselves.\n!pip3 install fastdtw --quiet !pip3 install scikit-learn --quiet !pip3 install pyCIOT --quiet !wget -q -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\u0026export=download Once the installation is finished, we can use the commands below to import the necessary packages, setting the stage for the tasks discussed in this article.\nfrom pyCIOT.data import * import json, os, glob, math import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib.font_manager import fontManager from tqdm import tqdm_notebook as tqdm from datetime import datetime, timedelta import seaborn as sns sns.set(font_scale=0.8) fontManager.addfont('TaipeiSansTCBeta-Regular.ttf') mpl.rc('font', family='Taipei Sans TC Beta') import warnings warnings.simplefilter(action='ignore') import geopy.distance from scipy.spatial.distance import euclidean from fastdtw import fastdtw import sklearn from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.cluster import KMeans Now, let’s take a look at how we can organize and group data using the water level and air quality information from Civil IoT Taiwan.\nCase 1: Location Type Classification of Air Quality Sensors In this example, we’re exploring how to use the data from the Environmental Protection Agency’s tiny air quality sensors (known as ‘OBS:EPA_IoT’) within the Civil IoT Taiwan framework to show how to categorize data.\nFirst, we utilize the Air().get_data() function from the pyCIOT library to fetch the most recent readings from all the EPA’s micro air quality sensors. Keep in mind, this process might take a bit of time because there’s a lot of data to gather.\nSource = 'OBS:EPA_IoT' Data = Air().get_data(src=Source) Data = [datapoint for datapoint in Data if len(datapoint['data']) == 3 and 'areaType' in datapoint['properties'].keys()] print(json.dumps(Data[0], indent=4, ensure_ascii=False)) { \"name\": \"智慧城鄉空品微型感測器-10287974676\", \"description\": \"智慧城鄉空品微型感測器-10287974676\", \"properties\": { \"city\": \"新北市\", \"areaType\": \"社區\", \"isMobile\": \"false\", \"township\": \"鶯歌區\", \"authority\": \"行政院環境保護署\", \"isDisplay\": \"true\", \"isOutdoor\": \"true\", \"stationID\": \"10287974676\", \"locationId\": \"TW040203A0507221\", \"Description\": \"廣域SAQ-210\", \"areaDescription\": \"鶯歌區\" }, \"data\": [ { \"name\": \"Relative humidity\", \"description\": \"相對溼度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 70.77 }, { \"name\": \"Temperature\", \"description\": \"溫度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 33.78 }, { \"name\": \"PM2.5\", \"description\": \"細懸浮微粒 PM2.5\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 9.09 } ], \"location\": { \"latitude\": 24.9507, \"longitude\": 121.3408416, \"address\": null } } In the data collected from each sensor, you’ll find details like temperature, relative humidity, and PM2.5 levels (a measure of air pollution). Additionally, each sensor’s basic information is included, such as the city, town, sensor ID, location ID, and the type of area it’s in, etc. For our purpose, we’re going to focus on the “location type” as a label for our data and create a classifier using the sensor data (temperature, relative humidity, and PM2.5 levels). First, let’s take a look at what the current “AreaType” data looks like.\nLabel = list(dict.fromkeys([datapoint['properties']['areaType'] for datapoint in Data if datapoint['properties']['areaType']])) count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"Before data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) Before data cleaning, There are 8620 records. { \"社區\": 223, \"交通\": 190, \"一般社區\": 2021, \"工業\": 12, \"測站比對\": 66, \"工業區\": 3333, \"交通區\": 683, \"鄰近工業區社區\": 948, \"輔助區\": 165, \"特殊區(民眾陳情熱區)\": 143, \"特殊區(敏感族群聚集區)\": 200, \"特殊區(測站比對)\": 32, \"輔助區(無測站區)\": 4, \"工業感測\": 196, \"特殊感測\": 4, \"輔助感測\": 295, \"交通感測\": 102, \"機動感測\": 2, \"社區感測\": 1 } Data Cleansing We started with 8,620 records from 19 different types of places. To make things clearer, we combined similar place types and narrowed our focus to four main categories: general communities, transportation areas, industrial areas, and communities near industrial zones. We organized the data using a specific code:\nfor datapoint in Data: if datapoint['properties']['areaType'] == '社區': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '社區感測': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '交通': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '交通感測': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '工業': datapoint['properties']['areaType'] = '工業區' elif datapoint['properties']['areaType'] == '工業感測': datapoint['properties']['areaType'] = '工業區' if not datapoint['properties']['areaType'] in ['一般社區', '交通區', '工業區', '鄰近工業區社區']: datapoint['properties']['areaType'] = None Data = [datapoint for datapoint in Data if datapoint['properties']['areaType'] != None] Label = ['一般社區', '交通區', '工業區', '鄰近工業區社區'] count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"After data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) After data cleaning, There are 7709 records. { \"一般社區\": 2245, \"交通區\": 975, \"工業區\": 3541, \"鄰近工業區社區\": 948 } After cleaning up the data, we were left with 7,709 records across these four main areas. We looked at each record’s temperature, relative humidity, and PM2.5 levels. In our three-dimensional map, we used different colors to show the data for each type of area.\nDataX, DataY = [], [] for datapoint in Data: TmpX = [None]*3 TmpY = None for rawdata_array in datapoint['data']: if(rawdata_array['name'] == 'Temperature'): TmpX[0] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'Relative humidity'): TmpX[1] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'PM2.5'): TmpX[2] = rawdata_array['values'][0].get('value') TmpY = Label.index(datapoint['properties']['areaType']) DataX.append(TmpX) DataY.append(TmpY) DataX_Numpy = np.array(DataX) DataY_Numpy = np.array(DataY) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature(℃)') ax.set_ylabel('Relative Humidity(%)') ax.set_zlabel('PM2.5(μg/$m^{3}$)') plt.show() The map revealed that most data points were clustered in certain areas, but some were spread out, far from the main group. These isolated data points are known as outliers. When we’re classifying or grouping data, outliers can skew our model or algorithm, making it less effective. Therefore, we removed these outliers to improve our analysis.\nOutlier removal We use a specific method to identify and remove unusual data points, or outliers, from our dataset. This method is based on standard statistical techniques and can be adjusted based on the unique requirements of different projects. In our case, we consider any sensor data from the Civil IoT Taiwan Data Platform an outlier if it’s more than two standard deviations away from the average. By removing these outliers, we then create a new three-dimensional map to better understand the data’s pattern.\ndef Outlier_Filter(arr, k): Boolean_Arr = np.ones(arr.shape[0], dtype=bool) for i in range(arr.shape[1]): Boolean_Arr = Boolean_Arr \u0026 (abs(arr[:,i] - np.mean(arr[:,i])) \u003c k*np.std(arr[:,i])) return Boolean_Arr OutlierFilter = Outlier_Filter(DataX_Numpy, 2) DataX_Numpy = DataX_Numpy[OutlierFilter] DataY_Numpy = DataY_Numpy[OutlierFilter] print(\"After removing Outliers, there are {} records left.\".format(DataX_Numpy.shape[0])) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature(℃)') ax.set_ylabel('Relative Humidity(%)') ax.set_zlabel('PM2.5(μg/$m^{3}$)') plt.show() After removing Outliers, there are 7161 records left. After this cleanup process, we found that we removed a total of 548 outliers (7,709 originally minus 7,161 remaining). The data that’s left shows a more consistent pattern in the three-dimensional space, with fewer extreme values on the edges. To make it easier to see these patterns, we create graphs showing the relationships between data points in two dimensions at a time.\nplt.rc('legend',fontsize=\"large\") fig, axes = plt.subplots(1,3,figsize=(24, 6)) for i in range(DataX_Numpy.shape[1]): for j in range(len(Label)): axes[i].scatter(DataX_Numpy[DataY_Numpy==j][:,i%3],DataX_Numpy[DataY_Numpy==j][:,(i+1)%3], s=1, label=Label[j]) axes[i].legend(loc=2) Axis_label = ['Temperature(℃)', 'Relative Humidity(%)', 'PM2.5(μg/$m^{3}$)'] axes[i].set_xlabel(Axis_label[i%3]) axes[i].set_ylabel(Axis_label[(i+1)%3]) plt.tight_layout() These graphs reveal interesting relationships between data points of different colors, which represent different types of areas in Civil IoT. Although it’s hard to explain these relationships in simple terms, we plan to use a classification model to develop a specific classifier to make sense of them.\nTrain data and test data Before we start training our classifier model, we need to do one important thing: divide our existing data into two parts – training data and test data. Basically, we’ll use the training data to teach the classifier how to do its job. The test data, on the other hand, is there to check how well our classifier can handle new, unseen data. To split our dataset into training and test data, we follow this example program, which divides them in a 4:1 ratio.\nindices = np.random.permutation(DataX_Numpy.shape[0]) Train_idx, Test_idx = indices[:int(DataX_Numpy.shape[0]*0.8)], indices[80:(DataX_Numpy.shape[0] - int(DataX_Numpy.shape[0]*0.8))] TrainX, TestX = DataX_Numpy[Train_idx,:], DataX_Numpy[Test_idx,:] TrainY, TestY = DataY_Numpy[Train_idx], DataY_Numpy[Test_idx] Using the pre-built Sklearn models We apply a variety of classification techniques using the Python package Scikit Learn (sklearn) for both training and testing purposes. Our approach encompasses nine distinct models: nearest neighbor, linear SVM, RBF SVM, decision tree, random forest, neural network, Adaboost, Naive Bayes, and QDA. The process involves initially feeding training data to refine these models, followed by using test data to evaluate their predictive capabilities. The effectiveness of these models is assessed by comparing the predicted outcomes against actual labels, using a tool known as a confusion matrix. This matrix helps us visualize the accuracy of each model in predicting various combinations of labels.\nclassifier_names = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(TrainX, TrainY) Result = model.predict(TestX) mat = confusion_matrix(TestY, Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=Label, yticklabels=Label, ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(classifier_names[i], round(accuracy_score(Result, TestY), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() In our exploration of these nine models, we observed that the RBF SVM model stands out by achieving a classification accuracy of nearly 70%. It’s noteworthy that this result was attained using unprocessed data. For those interested in delving deeper into the classifier’s potential, further investigation and data analysis can enhance its proficiency in categorizing diverse data types.\nCase 2: Clustering of Air Quality Sensors In this case, we’re working with the air quality data from Taiwan’s EPA monitoring stations, which is part of the Civil IoT Taiwan initiative. Our focus is on examining the historical data to see how similar the data trends are among different groups of stations.\nData download and preprocessing To gather all the air quality data from Taiwan’s EPA monitoring stations for 2021, we downloaded it from the Civil IoT Taiwan Data Service Platform’s historical database. After downloading, we unpacked the files into the /content directory.\n!wget -O 'EPA_OD_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FnkrDkv53nvbJf5ZyL5a6256m65ZOB5ris56uZL0VQQV9PRF8yMDIxLnppcA%3D%3D\" !unzip -q 'EPA_OD_2021.zip' \u0026\u0026 rm 'EPA_OD_2021.zip' !unzip -q '/content/EPA_OD_2021/EPA_OD_202112.zip' -d '/content' !rm -rf '/content/EPA_OD_2021' Our first step was to focus on December 2021 data. We removed fields we didn’t need: Pollutant, SiteId, Status, SO2_AVG. Next, we converted the data type of the sensor readings to floating-point numbers, making it easier to work with them later.\nDataframe = pd.read_csv(\"/content/EPA_OD_202112.csv\", parse_dates=['PublishTime']) Dataframe = Dataframe.drop(columns=[\"Pollutant\", \"SiteId\", \"Status\", \"SO2_AVG\"]) Numerical_ColumnNames = list(Dataframe.columns.values) for ColumnName in ['SiteName', 'County', 'PublishTime']: Numerical_ColumnNames.remove(ColumnName) for Numerical_ColumnName in Numerical_ColumnNames: Dataframe[Numerical_ColumnName] = pd.to_numeric(Dataframe[Numerical_ColumnName], errors='coerce').astype('float64') Dataframe = Dataframe.dropna() Dataframe.head() Considering the huge volume of data for the entire month, we aimed to speed up our sample program. To do this, we took a subset of data, specifically from December 13 to 17, 2021, and labeled this FiveDay_Dataframe. In the example below, you’ll see how we combined this data using the Country and SiteName fields. Finally, we arranged the data in order of their publication times.\nFiveDay_Dataframe = Dataframe.loc[(Dataframe['PublishTime'] \u003c= '2021-12-17 23:00:00') \u0026 (Dataframe['PublishTime'] \u003e= '2021-12-13 00:00:00')] FiveDay_Dataframe['CountyAndSiteName'] = FiveDay_Dataframe['County'] + FiveDay_Dataframe['SiteName'] FiveDay_Dataframe = FiveDay_Dataframe.drop(columns=[\"County\", \"SiteName\"]) FiveDay_Dataframe = FiveDay_Dataframe.sort_values(by=['CountyAndSiteName','PublishTime']) FiveDay_Dataframe = FiveDay_Dataframe.set_index(keys = ['CountyAndSiteName']) FiveDay_Dataframe Dynamic Time Warping (DTW) Next, we assess how “similar” two sites are and turn this similarity into a numerical value. The main way we do this is by lining up the data from two monitoring stations based on the time they were recorded and then comparing the differences in their air quality readings. However, since air pollution can happen at different times and last for varying lengths at each site, we need a more adaptable approach to gauge their similarity. That’s why we use the dynamic time warping (DTW) method. The closer the DTW distance is between two stations, the more similar they are.\nSite_TimeSeriesData = dict() for Site in np.unique(FiveDay_Dataframe.index.values): tmp = FiveDay_Dataframe[FiveDay_Dataframe.index == Site] tmp = tmp.groupby(['CountyAndSiteName', 'PublishTime'], as_index=False).mean() tmp = tmp.loc[:,~tmp.columns.isin(['CountyAndSiteName', 'PublishTime'])] Site_TimeSeriesData[Site] = tmp.to_numpy() DictKeys = Site_TimeSeriesData.keys() Sites_DTW = dict() for i, key1 in enumerate(DictKeys): for j, key2 in enumerate(DictKeys): if i \u003e= j: continue else: Sites_DTW[str(key1)+\" \"+str(key2)] = fastdtw(Site_TimeSeriesData[key1][:,:-4], Site_TimeSeriesData[key2][:,:-4], dist=euclidean)[0] Sites_DTW_keys = np.array(list(Sites_DTW.keys())) Site_DTW_Numpy = np.array([[value] for _, value in Sites_DTW.items()]) In the figure below, we’ve charted the DTW distances between all the sites, arranged from smallest to largest. To delve deeper, we’ll next start analyzing this data using clustering algorithms.\nfig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) ax.scatter(Site_DTW_Numpy[:,0], [1]*len(Sites_DTW.items()), s=0.05) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') K-Mean clustering We applied a technique called “K-Means clustering,” which is part of the sklearn software, to organize our data into groups. This method requires us to decide on the number of groups (or clusters) beforehand, so we chose to start with 3. To do this, we used a specific code, and then we visually represented our findings. In our chart, we displayed the group number on the vertical (Y) axis and the level of similarity to other data, calculated using a method called DTW, on the horizontal (X) axis.\nfrom sklearn.cluster import KMeans model = KMeans(n_clusters=3, random_state=0).fit([[value] for _, value in Sites_DTW.items()]) Result = model.labels_ for i in np.unique(Result): print(\"Number of Cluster{} : {}\".format(i,len(Result[Result==i]))) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): ax.scatter(Site_DTW_Numpy[Result==i][:,0],[i]*len(Site_DTW_Numpy[Result==i]), s=0.05) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Number of Cluster0 : 1165 Number of Cluster1 : 994 Number of Cluster2 : 542 Our results from using the K-Means algorithm revealed that our initial data was separated into three distinct groups, containing 1,165, 994, and 542 items of data each. To get a better understanding of why these particular groups were formed, we’re now looking into each possible factor that might have influenced their creation.\nCorrelation between data clusters and geographical locations We started by considering that air quality changes might be specific to certain areas. To investigate this, we looked at whether the patterns we found in air quality data were connected to where the air quality monitoring stations are located. First, we got the GPS coordinates of these stations and calculated how far apart they are from each other.\nNext, we grouped the data from these stations based on similarities and differences in their readings. Using this grouping, we then analyzed how the distances between stations in each group varied. We’ve illustrated these findings in the image below.\nDist_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Dist_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Dist_for_Cluster.append(geopy.distance.geodesic(coord1, coord2).km) Dist_for_Cluster = np.array(Dist_for_Cluster) Dist_for_Clusters[i] = Dist_for_Cluster Dist_for_Clusters = np.array(Dist_for_Clusters) # for Dist_for_Cluster in Dist_for_Clusters: # print(np.mean(Dist_for_Cluster)) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): gtMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003enp.mean(Dist_for_Clusters[i])] ltMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003cnp.mean(Dist_for_Clusters[i])] print(\"Mean Distance between Sites for Cluster{} : {}\".format(i, np.mean(Dist_for_Clusters[i]))) print(\"In Cluster{} there are {:.2%} less than mean, and {:.2%} greater than mean.\\n\".format(i, len(ltMean)/len(Dist_for_Clusters[i]), len(gtMean)/len(Dist_for_Clusters[i]))) ax.scatter(gtMean, [i]*len(gtMean), s=0.05, color=\"orange\") ax.scatter(ltMean, [i]*len(ltMean), s=0.05, color=\"pink\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=0.5) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Mean Distance between Sites for Cluster0 : 84.34126465234523 In Cluster0 there are 60.09% less than mean, and 39.91% greater than mean. Mean Distance between Sites for Cluster1 : 180.26230465399215 In Cluster1 there are 54.53% less than mean, and 45.47% greater than mean. Mean Distance between Sites for Cluster2 : 234.89206124762546 In Cluster2 there are 39.48% less than mean, and 60.52% greater than mean. Our analysis revealed that groups with higher DTW values (meaning the air quality data from these stations was less similar over time) tended to have stations that were further apart. Conversely, groups with more similar air quality data had stations that were closer together. This suggests that the spread of air pollutants is indeed influenced by how far apart these monitoring stations are, supporting our original idea.\nCorrelation between data clusters and wind directions First, we considered that changes in air quality might be influenced by the direction of the wind in the area. So, we looked into whether there’s a link between the patterns we found in the air quality data and the wind direction at the air quality monitoring station. We started by getting the GPS coordinates of the station and then worked out the directional relationship between two places on the map. Next, we examined how the patterns in the data were connected to the wind direction at the station’s location. After this, we did some basic statistical analysis and presented our findings in the figure below.\ndef get_bearing(lat1, long1, lat2, long2): dLon = (long2 - long1) x = math.cos(math.radians(lat2)) * math.sin(math.radians(dLon)) y = math.cos(math.radians(lat1)) * math.sin(math.radians(lat2)) - math.sin(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.cos(math.radians(dLon)) brng = np.arctan2(x,y) brng = np.degrees(brng) return brng def Check_Wind_Dirc(brng, wind_dirc): if brng \u003e 180: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng - 180 \u003c wind_dirc + 45) and (brng - 180 \u003e wind_dirc - 45)) else: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng + 180 \u003c wind_dirc + 45) and (brng + 180 \u003e wind_dirc - 45)) Brng_for_Clusters = [None]*len(np.unique(Result)) Boolean_WindRelated_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Brng_for_Cluster = [] Boolean_WindRelated_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Brng_Between_Site = get_bearing(coord1[0], coord1[1], coord2[0], coord2[1]) Brng_for_Cluster.append(Brng_Between_Site) MeanWindDirc1 = np.mean(Site_TimeSeriesData[Site1][:,-3]) MeanWindDirc2 = np.mean(Site_TimeSeriesData[Site2][:,-3]) Boolean_WindRelated_for_Cluster.append(Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc1) or Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc2)) Brng_for_Cluster = np.array(Brng_for_Cluster) Boolean_WindRelated_for_Cluster = np.array(Boolean_WindRelated_for_Cluster) Boolean_WindRelated_for_Clusters[i] = Boolean_WindRelated_for_Cluster Brng_for_Clusters[i] = Brng_for_Cluster Brng_for_Clusters = np.array(Brng_for_Clusters) Boolean_WindRelated_for_Clusters = np.array(Boolean_WindRelated_for_Clusters) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): print(\"Relevance for Cluster{} : {:.2%}\".format(i, len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True])/len(Dist_for_Clusters[i]))) ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True]), s=2, color=\"green\") ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False]), s=0.05, color=\"violet\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=2) ax.get_yaxis().set_visible(False) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Relevance for Cluster0 : 54.08% Relevance for Cluster1 : 39.24% Relevance for Cluster2 : 22.69% Our analysis indicates that when the DTW (Dynamic Time Warping) value is lower, there’s a stronger link between the direction from one station to another and the local wind direction — and the opposite is true when the DTW value is higher. This shows that the way air pollution spreads is indeed influenced by the wind direction around the area, supporting our initial theory.\nCase 3: Clustering and Classification Application of Water and Meteorological Data This example demonstrates how we use two types of data: rainfall data (from the Central Weather Bureau) and flood sensor data (from the Water Resource Agency), both sourced from Civil IoT Taiwan. We analyze past data using a method called data clustering. This method helps us identify which river water level stations are most affected by changes in rainfall. Then, we apply a technique known as data classification to predict if a certain area might experience flooding, based solely on rainfall data.\nData download and preprocessing To gather all the 2021 data from rain gauges (Central Weather Bureau) and flood sensors (Water Resource Agency), we use specific codes to download from the historical database of the Civil IoT Taiwan Data Service Platform. After downloading, we unpack the files into the /content directory.\n!wget -O 'Rain_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZLzIwMjEuemlw\" !wget -O 'Rain_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZL3JhaW5fc3RhdGlvbi5jc3Y%3D\" !unzip -q 'Rain_2021.zip' \u0026\u0026 rm 'Rain_2021.zip' !find '/content/2021' -name '*.zip' -exec unzip -q {} -d '/content/Rain_2021_csv' \\; !rm -rf '/content/2021' !wget -O 'Flood_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoLzIwMjEuemlw\" !wget -O 'Flood_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoL3N0YXRpb25f5rC05Yip572y77yI6IiH57ij5biC5pS%2F5bqc5ZCI5bu677yJX%2Ba3ueawtOaEn%2Ba4rOWZqC5jc3Y%3D\" !unzip -q 'Flood_2021.zip' \u0026\u0026 rm 'Flood_2021.zip' !find '/content/2021' -name '*_QC.zip' -exec unzip -q {} -d '/content/Flood_2021_csv' \\; !rm -rf '/content/2021' Starting with the rain gauge data, we first remove fields we don’t need: MIN_10, HOUR_6, HOUR_12, and NOW. Next, we discard any data recorded after November, keeping the rest in the Rain_df. Additionally, we bring in details about the rain gauges and save this information in Rain_Station_df. Please note that this process involves a large amount of data and may take some time to complete.\ncsv_files = glob.glob(os.path.join(\"/content/Rain_2021_csv\", \"*.csv\")) csv_files.sort() Rain_df = pd.DataFrame() for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['obsTime']) tmp_df.drop(['MIN_10','HOUR_6', 'HOUR_12', 'NOW'], axis=1, inplace=True) try: tmp_df = tmp_df.loc[tmp_df['obsTime'].dt.minute == 00] Rain_df = pd.concat([Rain_df, tmp_df]) except: print(csv_file) continue Rain_df = Rain_df.loc[Rain_df['obsTime'] \u003c \"2021-11-01 00:00:00\"] num = Rain_df._get_numeric_data() num[num \u003c 0] = 0 Rain_df.dropna(inplace=True) Rain_df.sort_values(by=['station_id','obsTime'], inplace=True) Rain_Station_df = pd.read_csv('/content/Rain_Stataion.csv') Rain_df Similarly, for the flood sensor data, we exclude any records after November. We also remove entries with missing information, storing the remaining data in Flood_df. Information about the flood sensors is also imported and saved in Flood_Station_df. Like before, this step involves processing a significant volume of data, so a considerable amount of time might be needed.\ncsv_files = glob.glob(os.path.join(\"/content/Flood_2021_csv\", \"*_QC.csv\")) csv_files.sort() Flood_df = pd.DataFrame() for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['timestamp']) tmp_df = tmp_df.loc[(tmp_df['PQ_unit'] == 'cm')] Flood_df = pd.concat([Flood_df,tmp_df], axis=0, ignore_index=True) Flood_df = Flood_df.loc[Flood_df['timestamp'] \u003c \"2021-11-01 00:00:00\"] Flood_df.replace(-999.0,0.0, inplace=True) Flood_df.dropna(inplace=True) Flood_df.sort_values(by=['timestamp'], inplace=True) Flood_Station_df = pd.read_csv('/content/Flood_Stataion.csv') Flood_df Correlation of flood sensors and rain gauges We’re dealing with a lot of data from flood sensors. To make it easier to understand, let’s focus on one specific flood sensor, number 43b2aec1-69b0-437b-b2a2-27c76a3949e8, which is located in Yunlin County. We’ll store the data from this sensor in an object called Flood_Site_df. This will be our example for further analysis.\nFlood_Site_df = Flood_df.loc[Flood_df['station_id'] == '43b2aec1-69b0-437b-b2a2-27c76a3949e8'] Flood_Site_df.head() Next, we want to find out how closely this flood sensor’s data matches with the data from rain gauges. To do this, we use a method called Dynamic Time Warping (DTW). DTW helps us measure how similar two sets of data are. The lower the DTW value, the more similar the data. To make this easier to understand, we’ll define similarity as the opposite of the DTW value. This means a smaller DTW value indicates more similarity. We’ll calculate this similarity between our chosen flood sensor and the data from all the rain gauges.\nFlood_Sensor_np = np.array([[v,v,v] for v in Flood_Site_df['value'].to_numpy()]) Site_dtw_Dist = dict() Rain_tmp_df = Rain_df.loc[(Rain_df['obsTime'].dt.hour == 1)] for Site in tqdm(np.unique(Rain_Station_df.loc[Rain_Station_df['city']=='雲林縣']['station_id'].to_numpy())): tmp_df = Rain_tmp_df.loc[(Rain_tmp_df['station_id'] == Site)] if tmp_df.empty: continue tmp_np = tmp_df[['RAIN','HOUR_3','HOUR_24']].to_numpy() Site_dtw_Dist[Site] = (1/fastdtw(Flood_Sensor_np, tmp_np, dist=euclidean)[0]) Site_dtw_Dist = dict(sorted(Site_dtw_Dist.items(), key=lambda item: item[1])) print(json.dumps(Site_dtw_Dist, indent=4, ensure_ascii=False)) { \"88K590\": 4.580649481044748e-05, \"C0K560\": 4.744655320519647e-05, \"C0K490\": 4.79216996101994e-05, \"C0K520\": 5.038234963332513e-05, \"C0K420\": 5.0674082994877385e-05, \"C0K250\": 5.1021366345465985e-05, \"C0K280\": 5.118406054309105e-05, \"A0K420\": 5.1515699268157996e-05, \"C0K400\": 5.178763059243615e-05, \"O1J810\": 5.2282255279259976e-05, \"C0K240\": 5.2470804312991397e-05, \"01J960\": 5.334885670256585e-05, \"C0K470\": 5.438256498969844e-05, \"81K580\": 5.45854441959214e-05, \"C0K410\": 5.5066753408217084e-05, \"A2K570\": 5.520214274022887e-05, \"01J970\": 5.529887546186233e-05, \"C0K480\": 5.5374254960644355e-05, \"C0K460\": 5.5657892623955056e-05, \"72K220\": 5.5690175197363816e-05, \"C0K510\": 5.5742273217039165e-05, \"C1K540\": 5.618025674136218e-05, \"C0K550\": 5.621240903075098e-05, \"C0K450\": 5.62197509062689e-05, \"C0K291\": 5.6380522616008906e-05, \"C0K330\": 5.638960953991442e-05, \"C0K530\": 5.6525582441919285e-05, \"C0K500\": 5.6825555408648244e-05, \"C0K440\": 5.692254536595e-05, \"C0K430\": 5.697351917955081e-05, \"01J930\": 5.7648109890427854e-05, \"C0K580\": 5.770344946580767e-05, \"C0K390\": 5.782553930260475e-05, \"01J100\": 5.7933240408734325e-05, \"01K060\": 5.8343415644572526e-05 } Data clustering to find highly correlated rain gauges We grouped the rain gauges into three categories using a technique known as clustering, which groups items based on how similar they are to each other. Our focus was to identify the group most closely matching the flood sensor data. In our case, we ended up with three groups containing 9, 23, and 3 rain gauges respectively. The second group, with 23 gauges, had rainfall patterns most closely resembling the data from the flood sensors.\ncluster_model = KMeans(n_clusters=3).fit([[value] for _, value in Site_dtw_Dist.items()]) Result = cluster_model.labels_ for i in np.unique(Result): print(\"Number of Cluster {} : {}\".format(i,len(Result[Result==i]))) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) fig.text(0.5, 0.01, 'DTW distance', ha='center', va='center') fig.text(0.06, 0.5, 'Group number', ha='center', va='center', rotation='vertical') Site_DTW_Numpy = np.array([value for _, value in Site_dtw_Dist.items()]) Site_Name_Numpy = np.array([key for key, _ in Site_dtw_Dist.items()]) Mean_Dis_For_Cluster = [None] * len(np.unique(Result)) for i in np.unique(Result): Mean_Dis_For_Cluster[i] = (np.mean(Site_DTW_Numpy[Result==i])) ax.scatter(Site_DTW_Numpy[Result==i],[i]*len(Site_DTW_Numpy[Result==i]), s=10) print(\"Mean Distance of Cluster {} : {}\".format(i,Mean_Dis_For_Cluster[i])) ax.get_yaxis().set_visible(False) Best_Cluster = np.where(Mean_Dis_For_Cluster == np.amax(Mean_Dis_For_Cluster))[0] Best_Site = Site_Name_Numpy[Result == Best_Cluster] print(Best_Site) Number of Cluster0 : 23 Number of Cluster1 : 3 Number of Cluster2 : 9 Mean Similarity of Cluster0 : 5.6307994901628334e-05 Mean Similarity of Cluster1 : 4.7058249208614454e-05 Mean Similarity of Cluster2 : 5.1629678408018994e-05 ['C0K470' '81K580' 'C0K410' 'A2K570' '01J970' 'C0K480' 'C0K460' '72K220' 'C0K510' 'C1K540' 'C0K550' 'C0K450' 'C0K291' 'C0K330' 'C0K530' 'C0K500' 'C0K440' 'C0K430' '01J930' 'C0K580' 'C0K390' '01J100' '01K060'] To delve deeper into the relationship between the flood sensor and these 23 rain gauges, we arranged the data from the chosen flood sensor in a time sequence.\ntmp= Flood_Site_df['value'].to_numpy() fig= plt.figure(figsize=(6, 3), dpi=150) ax= fig.add_subplot(1,1,1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) ax.set_xlabel('Time sequence') ax.set_ylabel('Water level') Next, we created 23 separate charts, each showing the hourly rainfall recorded by one of the 23 rain gauges in the group we identified as most similar. These charts revealed a notable pattern: when the flood sensor readings were high, the rainfall measurements from the rain gauges also tended to increase. This trend between the two sets of data was strikingly similar, aligning well with what we would logically expect.\nfig = plt.figure(figsize=(8, 2*(len(Best_Site)//4+1)), dpi=150) for i, Site in enumerate(Best_Site): tmp = Rain_df.loc[Rain_df['station_id']==Site]['RAIN'] ax = fig.add_subplot(len(Best_Site)//4+1,4,i+1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) Data classification to predict flooding by rainfall data To start, we’re using data from flood sensors to determine if a flood happened at a certain location. This data is treated as our label. We also use information from closely related rain gauges to create a simple tool (classifier) to predict flooding. We’ve divided data from January 2021 to October 2021 into two parts: training data (first seven months) and testing data (next two months). In our system, if a flood sensor records a value above 0, we consider it a flood event; if it records 0, we mark it as no flood event. This sorted data is saved in an object called Train_DataSet.\nPlease note that, for simplicity, we’re using a basic rule where any water level above 0 indicates a flood. However, real-world criteria for defining floods are more complex. For accurate applications, please refer to relevant laws and regulations.\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=10*len(Flooding)).values Train_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(0) Similarly, for data from August 2021 to October 2021, a value above 0 in a flood sensor is marked as a flood, and a value of 0 as no flood. This sorted data is stored in another object called Test_DataSet.\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=2*len(Flooding)).values Test_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(0) For training and testing, we’re using a Python package called Scikit Learn (sklearn). We’re trying out nine different methods, including nearest neighbor, linear SVM, RBF SVM, decision tree, random forest, neural network, Adaboost, Naive Bayes, and QDA. We feed in the training data for fine-tuning and then the test data for predictions. To evaluate how well our model works, we compare the test data with predicted results using a tool known as a confusion matrix.\nnames = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(Train_DataSet['x'], Train_DataSet['y']) Result = model.predict(Test_DataSet['x']) mat = confusion_matrix(Test_DataSet['y'], Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=[\"不淹水\",\"淹水\"], yticklabels=[\"不淹水\",\"淹水\"], ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(names[i], round(accuracy_score(Result, Test_DataSet['y']), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() In our experiment, the nearest neighbor method showed a success rate of about 7.3% in classifying floods. This is just a basic attempt, using unprocessed data. With more detailed analysis and research on the data, we can likely improve our success rate. For those interested in data classification, further study in this area can help enhance the effectiveness of these classifiers in dealing with various types of data.\nReferences Ibrahim Saidi, Your First Machine Learning Project in Python, Medium, Jan, 2022, (https://ibrahimsaidi.com.au/your-first-machine-learning-project-in-python-e3b90170ae41) Esmaeil Alizadeh, An Illustrative Introduction to Dynamic Time Warping, Medium, Oct. 2020, (https://towardsdatascience.com/an-illustrative-introduction-to-dynamic-time-warping-36aa98513b98) Jason Brownlee, 10 Clustering Algorithms With Python, Machine Learning Mastery, Aug. 2020, (https://machinelearningmastery.com/clustering-algorithms-with-python/) Alexandra Amidon, How to Apply K-means Clustering to Time Series Data, TOwards Data Science, July 2020, (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) scikit-learn Tutorials (https://scikit-learn.org/stable/tutorial/index.html) scikit-learn Classifier comparison (https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) FastDTW - A Python implementation of FastDTW, (https://github.com/slaypni/fastdtw) Nearest Neighbors - Wikipedia, (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) SVM Classifier - Wikipedia, (https://en.wikipedia.org/wiki/Support-vector_machine) Decision Tree - Wikipedia, (https://en.wikipedia.org/wiki/Decision_tree) Random Forest - Wikipedia, (https://en.wikipedia.org/wiki/Random_forest) K-Means - Wikipedia, (https://en.wikipedia.org/wiki/K-means_clustering) DBSCAN - Wikipedia, (https://en.wikipedia.org/wiki/DBSCAN) Hierarchical Clustering - Wikipedia, (https://en.wikipedia.org/wiki/Hierarchical_clustering) BIRCH - Wikipedia, (https://en.wikipedia.org/wiki/BIRCH) Confusion matrix - Wikipedia, (https://en.wikipedia.org/wiki/Confusion_matrix#Confusion_matrices_with_more_than_two_categories) ",
    "description": "We use air quality and water level data, combined with weather observations, using machine learning for data classification and data grouping. We demonstrate the standard process of machine learning and introduce how to further predict data through data classification and how to further explore data through data grouping.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "6.1. Machine Learning Preliminaries",
    "uri": "/en/ch6/ch6.1/"
  },
  {
    "content": " Table Of Contents Goal QGIS Basic Operation Data Preparation GeoJSON Output Data Filter and Change Colors Export Thematic Maps Example 2: Distribution of Emergency Shelters Conclusion References QGIS, which stands for Quantum Geographic Information System, is an open-source desktop application that allows users to work with geographic data. Not only can you visualize the data collected by users in various geographic formats, but you can also process, analyze, and integrate geospatial information. One of its powerful features is the ability to create thematic maps.\nIn this article, we’ll explore how QGIS assists in analyzing and presenting PM2.5 data obtained from Civil IoT Taiwan. After completing the analysis, we’ll generate a thematic map to interpret the results. Additionally, we’ll demonstrate how to combine disaster prevention data from Civil IoT Taiwan to create a distribution map of disaster shelters. This functionality enables citizens to easily locate the nearest shelters in times of need.\nNote Please note: The version of QGIS used in this article is 3.16.8. However, the functions discussed here are fundamental features of the software. If you’re using a different version, you should still be able to use these functions without any issues.\nGoal Import data into QGIS Basic geographic and spatial data analysis in QGIS Export thematic maps from QGIS QGIS Basic Operation After entering the QGIS software, you can see the following operation interface. In addition to the data frame in the middle area, the upper part is the standard toolbar, which provides various basic operation tools and functions. There are two sub-areas on the left, which are the data catalog window and layers; on the right is the analysis toolbar, which provides various analysis tools.\nData Preparation Data source: Civil IoT Taiwan - Historical Data (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器)\nIn this section, we’ll begin by explaining how to import data into QGIS. When dealing with data that might be split across multiple tables during storage, it’s crucial to pay attention to whether this applies to the dataset you’re analyzing. If so, you’ll need to recombine the data into a single, cohesive table.\nLet’s break down the process:\nImport data\nStart by importing the CSV file directly downloaded from Civil IoT Taiwan Historical Data into QGIS. Keep in mind that the data contains Chinese characters, which can sometimes appear garbled during the import process. To import the data, follow these steps: - Go to the top menu and select Layer \u003e Add Layer \u003e Add Delimited Text Layer. - The import interface will appear, allowing you to configure the import settings. Join data\nSince the original data divides PM2.5 and station coordinates (latitude and longitude) into two files, it is necessary to join PM2.5 and latitude and longitude coordinates first. The join method is as follows, right-click the file to be joined \u003e Properties \u003e joins \u003e click the + sign to enter Add Vector Join, as shown below There are four options after entering:\nJoin Layer: the layer you want to join Join field: corresponding to the Target field, which is the reference for Join (similar to Primary key) Target field: corresponding to the Join field, which is the reference for Join (similar to Foreign key) Joined field: the fields you want to join GeoJSON Output Next, we’ll utilize the built-in features of QGIS to transform the original CSV data into GeoJSON files, which represent geographical data in JSON format. The process involves navigating to Processing \u003e Toolbox \u003e Create points from the table.\nRemember, once you’ve clicked on that option, choose the table you want to import. Specify longitude for the X-coordinate and latitude for the Y-coordinate. Set the Target CRS to WGS 84 (which corresponds to latitude and longitude coordinates). Finally, provide a name for the output file, as demonstrated below.\"\nThen select the file format you want to export, and click “Save”.\nData Filter and Change Colors Next, we’ll showcase how to utilize QGIS for filtering the necessary stations and dynamically adjusting their colors based on the PM2.5 values.\nUse Intersection to filter the required stations by county\nBefore proceeding with the screening process, you’ll need to download the shapefiles (shp files) for municipalities, counties, and cities from the government data open platform open platform. Once you’ve obtained these files, import the shapefiles for county and city boundaries into QGIS, following the steps below: Note Note: As part of the Civil IoT Taiwan project, National Chi Nan University has deployed micro air quality sensors in Changhua and Nantou counties and cities. Consequently, the data collected this time does not include information from Changhua and Nantou.\nClick on the ‘Select Feature’ icon. Then, select the desired county by clicking on its name. The selected counties will be highlighted in yellow. Let’s take New Taipei City as an example, as shown in the figure below: 3. In the ‘Processing Toolbox’, search for the ‘Intersection’ function. Click on it to open the interface, which provides three input options: Input Layer, Overlay Layer, and Intersection (Output) 4. Place the PM2.5 layer in the ‘Input Layer’, and add the county-city boundary layer to the ‘Overlay Layer’. Make sure to check the ‘Select features only’ option. This ensures that only the stations intersecting with the previously selected New Taipei City are filtered out. 5. Finally, enter a name for the exported file in the ‘Intersection’ section. The supported export file formats are the same as the ones you’ve previously selected.\nUpon completing these steps, you’ll obtain the desired results.\"\nDisplay different colors according to the value of PM2.5\nThen we right click on the PM2.5 layer and choose Properties. In the Symbology section, you’ll find the dot color settings. Follow these steps for each PM2.5 station:\nChange the original setting from “No Symbols” to “Graduated.” Select PM25 in the “Value” part. Click the “Classify” button at the bottom. Set the number of colors in “Classes” (Remember, it’s best not to have too many categories). Go to “Color ramp” and choose the color for each value. Click “OK” when you’re done. Once you’ve completed these steps, the resulting image will display dots with colors corresponding to the PM 2.5 value. The layer on the right will show how different colors represent varying PM 2.5 levels.\nNote Note: If you’re using the new version of QGIS (after 3.16), you can easily add the OpenStreetMap basemap by clicking XYZ Tiles and selecting OpenStreetMap in the figure below.\nExport Thematic Maps After configuring the previous settings, the next step involves exporting your QGIS project as a JPG image. Click on Project and then select New Print Layout. A dialog for setting the layout name will appear. Once you’ve completed the layout name setting, the following screen will be displayed. On the left side, click “Add map” and choose the area on the drawing canvas where you want to include the PM 2.5 map. Next, still on the left side, click “Add Legend” and select a range to import the label. You can adjust font size, color, and other properties in the “Item Properties” on the right. To finish, add the title, scale bar, and compass to create a comprehensive thematic map. Lastly, in the upper left corner, click “Layout” and choose “Export as Image” to save your thematic map as an image file. Example 2: Distribution of Emergency Shelters Data source: https://data.gov.tw/dataset/73242\nGovernment public information has neatly organized Taiwan’s emergency shelters into electronic files, making it convenient for citizens to download and utilize them. In this example, we’ll demonstrate how to locate the nearest shelter to your home using QGIS.\nWe start by obtaining the shelter information from the provided URL and load this information into QGIS following the method mentioned earlier.\nGiven the substantial number of shelters across Taiwan, this article will focus on analyzing shelters specifically in Taipei City. However, the same approach can be applied to other counties and cities. If you’re curious, feel free to explore this process for yourself.\nWe then begin by using the intersection method to identify shelters within Taipei City.\nNext, we utilize Voronoi Polygons from the side toolbar to create a Voronoi diagram, and we ill in the layer of shelters within the Voronoi Polygons and click “Run” as depicted below.\nThe Voronoi Diagram’s characteristics will reveal the location of the closest shelter to your residence, as shown in the image.\nOnce the analysis is complete, you can generate a thematic map using the same method as before.\nConclusion In this chapter, we’ve covered the process of importing data into QGIS and utilizing its analysis tools to explore that data. Additionally, we’ve introduced the data exporting method, which allows you to transform your analyzed data into thematic maps for interpretation. Keep in mind that there are many other features in QGIS that we haven’t covered here. If you’re keen on diving deeper into QGIS, we recommend exploring additional resources below.\nReferences QGIS: A Free and Open Source Geographic Information System (https://qgis.org/) QGIS Tutorials and Tips (https://www.qgistutorials.com/en/) QGIS Documentation (https://www.qgis.org/en/docs/index.html) ",
    "description": "We introduce the presentation of geographic data using the QGIS system, and use the data from Civil IoT Taiwan as an example to perform geospatial analysis by clicking and dragging. We also discuss the advantages and disadvantages of QGIS software and when to use it.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.1. QGIS Application",
    "uri": "/en/ch7/ch7.1/"
  },
  {
    "content": "This website provides data application descriptions and examples based on various open data from Civil IoT Taiwan. Please refer to the original website description for the authorization requirements of different available data used on this website. For the Python language packages and software packages used in the program examples provided on this website, please refer to the relevant webpage instructions of each package and software for authorization. The content of articles offered on this site is licensed under CC-BY (https://creativecommons.org/licenses/by/4.0/).\nThe organization of this website project is as follows:\nOrganizer: Science \u0026 Technology Policy Research and Information Center, National Applied Research Laboratories Co-organizer: Institute of Information Science, Academia Sinica Host: Ling-Jyh Chen Advisors: Chih-Chieh Hung; National Chung Hsing University Li-Pen Wang; National Taiwan University Jen-Wei Huang; National Cheng Kung University Wei-Jia Huang; LASS Chih-Hao Liu; National Science and Technology Center for Disaster Reduction Tsui-Ping Hung; Taipei Municipal Yu Cheng Senior High School Huei-Jun Kao; Taipei Municipal Nan-Gang High School Authors: Hung-Ying Chen, Yu-Shen Cheng, Ming-Kuang Chung, Jen-Wei Huang, Sky Hung, Huei-Jun Kao, Quen Luo, Yu-Chi Peng, Tze-Yu Sheng, Cheng-Jia Wu Reviewers: Chia-Kai Liu; DSP, Inc. Wuulong Hsu; LASS Hsin-Cheng Hsieh; National Central University Yu-Hung Liu; Cianjhen Senior High School, Kaohsiung Chien-Hua Ke; National Keelung Senior High School, Keelung Ting-Yen Hung; The Affiliated Senior High School of National Taiwan Normal University ",
    "description": "",
    "tags": [
      "Introduction"
    ],
    "title": "About",
    "uri": "/en/about/"
  },
  {
    "content": " Table Of Contents Programming Language - Python Development Platform – Google Colab References Programming Language - Python This material is tailored for those interested in harnessing data from the Civil IoT for public welfare purposes, using Python — a language that’s highly favored in the data science community. The course is structured to offer an engaging, hands-on learning experience. It breaks down complex topics into manageable sections, guiding participants through practical applications of Civil IoT data. This approach not only helps students gain valuable experience in handling this specific type of data but also equips them with skills transferable to other data science ventures.\nPython’s prominence in data science is owed to several key strengths:\nEase of Learning: Python is known for its straightforward and intuitive syntax. It doesn’t involve the complexity of languages like C or Java. Its syntax is more akin to reading simple English, making it easier for students with basic knowledge of English and logical thinking to understand and master Python code. Extensive Libraries: Over its development spanning more than three decades, Python has amassed a vast array of libraries, thanks to its robust open-source community. These libraries extend Python’s functionality, making it versatile for various applications. In this course, we introduce a specially tailored Python package (pyCIOT), designed to simplify the learning process and enhance proficiency in using Civil IoT data. Self-learning Friendly: Python’s diverse package ecosystem necessitates an ability to read and comprehend code. Students are encouraged to read the documentation and integrate different packages to solve specific problems. This course fosters an environment of learning and innovation, building upon existing programming knowledge without starting from scratch. Python learning becomes as immersive as engaging with a compelling story, enhanced by continuous practice and exploration. Thanks to these attributes, Python has become the go-to language in data science, particularly for beginners. Beyond traditional textbooks, the internet offers a wealth of resources for those eager to deepen their understanding of Python, making it an invaluable tool for students committed to exploring the field further.\nFree Courses Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) Free e-Book Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) Development Platform – Google Colab As an interpreted language, Python operates differently from compiled languages like C. It translates code into machine language at runtime, meaning the Python code is converted and executed line by line as the program runs. This process is similar to how a live translator works in real-time, translating sentences during a conversation with someone who speaks a different language.\nThis interpretive nature gives Python several advantages in terms of development environments:\nJupyter Notebooks: Renowned for their notebook-style layout, Jupyter Notebooks allow for the integration of explanatory text (in natural language) and Python code. This feature is particularly beneficial for developers and data analysts, as it facilitates clear documentation and step-by-step coding. Google Colab: Building on the concept of Jupyter, Google introduced Colab, a platform that enables users to write and run Python code in the cloud. This reduces the demand for local computing power and storage and makes collaboration and sharing more straightforward. Google Colab offers several key benefits for Python programmers:\nAs an interpreted language, Python operates differently from compiled languages like C. It translates code into machine language at runtime, meaning the Python code is converted and executed line by line as the program runs. This process is similar to how a live translator works in real-time, translating sentences during a conversation with someone who speaks a different language.\nThis interpretive nature gives Python several advantages in terms of development environments:\nJupyter Notebooks: Renowned for their notebook-style layout, Jupyter Notebooks allow for the integration of explanatory text (in natural language) and Python code. This feature is particularly beneficial for developers and data analysts, as it facilitates clear documentation and step-by-step coding.\nGoogle Colab: Building on the concept of Jupyter, Google introduced Colab, a platform that enables users to write and run Python code in the cloud. This reduces the demand for local computing power and storage, and makes collaboration and sharing more straightforward.\nGoogle Colab offers several key benefits for Python programmers:\nNo Installation Needed: It comes pre-equipped with most common Python packages, saving users from the hassle of configuration and dependency management. Cloud-Based Storage: Since the code and data are stored in Google’s cloud, they can be accessed and edited from any location or device, simplifying the processes of data transfer and storage. Collaborative Features: Google Colab supports real-time online collaboration, allowing multiple users to share and jointly work on code. This is especially useful for team-based projects and collaborative endeavors. Free Access to Advanced Resources: Users get free access to powerful computing resources like GPUs and TPUs, enabling them to run complex, computation-heavy tasks without being limited by their local machine’s capabilities. Google Colab has thus become an invaluable tool for both novice and experienced Python developers and data science practitioners. It offers an accessible platform for beginners to start their journey and for professionals to advance their skills. For those interested in exploring more about Google Colab and its capabilities, various online resources provide detailed insights and tutorials. For instance:\nGetting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) References Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "A brief introduction of the programming language Python and the development platform Google Colab used in the materials",
    "tags": [
      "Introduction"
    ],
    "title": "2.2. Material Tools",
    "uri": "/en/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents Data access under temporal conditions Data access under spatial conditions Case study: Is the air quality here worse than there? Import data Remove invalid data Calculate distance Pandas package Results References This article will access the data of the Civil IoT Taiwan project from the perspective of time and space, and carry out a simple implementation with the theme of air quality monitoring. It will covers the following topics:\nthe usage of the datetime, math, numpy, and pandas packages the processing of JSON format data data processing using Pandas DataFrame Data access under temporal conditions When using get_data() in pyCIOT, data can be obtained according to the start time and end time. The format is passed in the time_range as a dictionary (Dict), which are start, end and num_of_data respectively.\nstart and end refer to the start and end time of data collection, in ISO8601 or Datetime format. num_of_data will control the number of data acquisitions not to exceed this number. If the data in the range exceeds num_of_data, it will be collected at intervals, so that the time interval between data and data tends to be the same.\nTaking air quality data as an example, the acquired data can go back one day at most, so when the end variable is set greater than one day, no data will be acquired. In addition, since the update frequency of each sensor in the Civil IoT Taiwan project is different, the number of data sets per day for different sensors will also be different. For details, please refer to: https://ci.taiwan.gov.tw/dsp/dataset_air.aspx\nfrom datetime import datetime, timedelta end_date = datetime.now() # current datetime isodate_end = end_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format start_date = datetime.now() + timedelta(days = -1) # yesterday isodate_start = start_date.isoformat().split(\".\")[0]+\"Z\" # convert to ISO8601 format time = { \"start\": isodate_start, \"end\": isodate_end, \"num_of_data\": 15 } data = Air().get_data(\"OBS:EPA_IoT\", stationIds=[\"11613429495\"], time_range=time) data The data will be stored in data in List format, and stored separately according to different properties. The data of temperature, relative humidity and PM2.5 will be stored in the ‘values’ list under the corresponding name respectively, and the time of each data record will be marked and displayed in ISO8601.\n[{'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 30.7}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T12:54:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:53:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 11.9}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 12.15}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 12.2}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 12.22}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 12.31}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 12.19}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 12.26}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 12.17}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 12.04}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 11.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 11.67}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 11.56}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 11.56}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}}] Data access under spatial conditions In pyCIOT, there are also methods to get specific data based on the region. If we take the latitude and longitude of a specific location as the center, we can use the radius distance to form a search range (circle) to get the station ID and sensing value of the specific space.\nThe data format of a specific area is also passed in as a dictionary (Dict), where the latitude, longitude and radius are “latitude”, “longitude” and “distance” respectively. The filtering functions of a specific area and a specific time can be used at the same time. When searching for a specific area, the stations to be observed can also be put into the “stationIds”, and the stations outside the area can be removed by the way.\nloc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # 半徑(km) } c = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) c[0] { 'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': { 'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [ { 'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 35.84}] },{ 'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 59.5}] },{ 'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 11.09}] } ], 'location': { 'latitude': 24.998769, 'longitude': 121.512717, 'address': None } } The above are the basic methods commonly used to obtain pyCIOT station data, using time and space as the filtering criteria, and applicable to all data including location and timestamp types. To demonstrate, we give some simple examples and implement them using these pyCIOT packages.\nCase study: Is the air quality here worse than there? Project codes: OBS:EPA_IoT (low-cost air quality stations by EPA） Target location: Nanshijiao MRT Station Exit 1 (GPS coordinates: 24.990550, 121.507532) Region of interest: Zhonghe District, New Taipei City Import data First, we need to get all the information about the target location and the region of interest. We can use the method of “data access under spatial conditions” to set the latitude and longitude at Exit 1 of Nanshijiao MRT Station, and set the distance to three kilometers. Then, we simply use Air().get_data() to obtain the data:\nloc = { \"latitude\": 24.990550, \"longitude\": 121.507532, \"distance\": 3.0 # (km) } EPA_IoT_zhonghe_data_raw = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) print(\"len:\", len(EPA_IoT_zhonghe_data_raw)) EPA_IoT_zhonghe_data_raw[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [{'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 94.84}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 3.81}]}, {'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 25.72}]}], 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}} Remove invalid data Of the in-scope sites, not every station is still running smoothly. In order to remove potentially problematic sites, we observe the data characteristics of invalid stations and find that all three data (temperature, humidity, PM2.5 concentration) are 0. So we just need to pick and delete this data before we can move on to the next step.\n# Data cleaning EPA_IoT_zhonghe_data = [] for datajson in EPA_IoT_zhonghe_data_raw: # 確認資料存在 if \"data\" not in datajson: continue; # 將格式轉換為 Temperature, Relative_Humidity 和 PM2_5 for rawdata_array in datajson['data']: if(rawdata_array['name'] == 'Temperature'): datajson['Temperature'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'Relative humidity'): datajson['Relative_Humidity'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'PM2.5'): datajson['PM2_5'] = rawdata_array['values'][0]['value'] datajson.pop('data') # 確認所有資料皆為有效，同時去除無資料之檢測站 if \"Relative_Humidity\" not in datajson.keys(): continue if \"PM2_5\" not in datajson.keys(): continue if \"Temperature\" not in datajson.keys(): continue if(datajson['Relative_Humidity'] == 0 and datajson['PM2_5'] == 0 and datajson['Temperature'] == 0): continue EPA_IoT_zhonghe_data.append(datajson) print(\"len:\", len(EPA_IoT_zhonghe_data)) EPA_IoT_zhonghe_data[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24} Calculate distance Assuming that there is no error in the data of each station, the data of the station closest to the target position is the data to be compared. To find the nearest station, we need to calculate the distance between each station and the target location.\nWe can use the point-to-point distance formula to calculate and sort to find the closest station to the target location. But here we use the standard Haversine formula to calculate the spherical distance between two points on Earth. The following is the implementation in the WGS84 coordinate system:\nimport math def LLs2Dist(lat1, lon1, lat2, lon2): R = 6371 dLat = (lat2 - lat1) * math.pi / 180.0 dLon = (lon2 - lon1) * math.pi / 180.0 a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) * math.sin(dLon / 2) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) dist = R * c return dist for data in EPA_IoT_zhonghe_data: data['distance'] = LLs2Dist(data['location']['latitude'], data['location']['longitude'], 24.990550, 121.507532)# (24.990550, 121.507532) EPA_IoT_zhonghe_data[0] {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24, 'distance': 1.052754763080127} Pandas package Pandas is a Python package of commonly used data manipulation and analysis. DataFrame format is used to store two-dimensional or multi-column data format, which is very suitable for data analysis. We convert the processed data into a DataFrame, pick out the required fields, and sort them according to the previously calculated distance from small to large.\nimport pandas as pd df = pd.json_normalize(EPA_IoT_zhonghe_data) #Results contain the required data df EPA_IoT_zhonghe_data_raw = df[['distance', 'PM2_5', 'Temperature', 'Relative_Humidity', 'properties.stationID', 'location.latitude', 'location.longitude', 'properties.areaType']] EPA_IoT_zhonghe_data_raw = EPA_IoT_zhonghe_data_raw.sort_values(by=['distance', 'PM2_5'], ascending=True) EPA_IoT_zhonghe_data_raw Results To know if the air quality at the target location is better than the area of interest, we can roughly use the distribution of air quality across all stations. You can use tools such as the numpy package, a common data science processing library in Python, or directly calculate the mean and standard deviation to get the answer.\nimport numpy as np zhonghe_target = EPA_IoT_zhonghe_data_raw.iloc[0,1] zhonghe_ave = np.mean(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) zhonghe_std = np.std(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) result = (zhonghe_target-zhonghe_ave)/zhonghe_std print('Mean:', zhonghe_ave, 'std:', zhonghe_std) print('PM2.5 of the neareat station:', zhonghe_target) print('The target is ', result, 'standard deviations from the mean.\\n') if(result\u003e0): print('Result: The air quality at the target location is worse.') else: print('Result: The air quality at the target location is better.') Mean: 6.71 std: 3.18 PM2.5 of the neareat station:7.38 The target is 0.21 standard deviations from the mean. Result: The air quality at the target location is worse. References Python pyCIOT package (https://pypi.org/project/pyCIOT/) pandas - Python Data Analysis Library (https://pandas.pydata.org/) 10 minutes to pandas — pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) NumPy (https://numpy.org/) NumPy quickstart (https://numpy.org/doc/stable/user/quickstart.html) Haversine formula - Wikipedia (https://en.wikipedia.org/wiki/Haversine_formula) ",
    "description": "We introduce how to obtain the data of a project in a specific time or time period, and the data of a project in a specific geographical area in the Civil IoT Taiwan Data Service Platform. We also demonstrate the application through a simple example.",
    "tags": [
      "Python",
      "API",
      "Air"
    ],
    "title": "3.2. Data Access under Spatial or Temporal Conditions",
    "uri": "/en/ch3/ch3.2/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access Air Quality Data Water Level Data Meteorological Data Data Preprocessing Stationary Evaluation Data Forecast ARIMA SARIMAX auto_arima Prophet LSTM Holt-Winter Comparison References The previous chapter has introduced various methods of processing time series data, including visual presentation of data, decomposition of time series data, etc. In this chapter, we will further extract the characteristics of these data and use various predictive models to find the law of the data and predict the future.\nGoal Stationary evaluation of time series data Comparison of different forecasting models The practice of time series data forecasting Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, statsmodels, and warnings packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use two additional packages that Colab does not have pre-installed: kats and pmdarima, which need to be installed by :\n!pip install --upgrade pip !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 !pip install pmdarima After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport warnings import numpy as np import pandas as pd import pmdarima as pm import statsmodels.api as sm import matplotlib.pyplot as plt import os, zipfile from dateutil import parser as datetime_parser from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.tsa.stattools import adfuller, kpss from kats.consts import TimeSeriesData, TimeSeriesIterator from kats.detectors.outlier import OutlierDetector from kats.models.prophet import ProphetModel, ProphetParams from kats.models.lstm import LSTMModel, LSTMParams from kats.models.holtwinters import HoltWintersParams, HoltWintersModel Data Access The topic of this paper is the analysis and processing of time series data. We will use the air quality, water level and meteorological data on the Civil IoT Taiwan Data Service Platform for data access demonstration, and then use the air quality data for further data analysis. Among them, each type of data is the data observed by a collection of stations for a long time, and the time field name in the dataframe is set to timestamp. Because the value of the time field is unique, we also use this field as the index of the dataframe.\nAir Quality Data Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the data archive of “Academia Sinica - Micro Air Quality Sensors” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Air folder.\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') The CSV_Air folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 74DA38C7D2AC), we need to read each CSV file and put the data for that station into a dataframe called air. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.info()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 Water Level Data Like the example of air quality data, since we are going to use long-term historical data this time, we do not directly use the data access methods of the pyCIOT suite, but directly download the data archive of “Water Resources Agency - Groundwater Level Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Water folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Water folder.\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') The CSV_Water folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code 338c9c1c-57d8-41d7-9af2-731fb86e632c), we need to read each CSV file and put the data for that station into a dataframe called water. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 Meteorological Data We download the data archive of “Central Weather Bureau - Automatic Weather Station” from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Weather folder.\nAt the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily file and store it in the CSV_Weather folder.\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') The CSV_Weather folder now contains all daily sensor data in CSV format. To filter out data for a single station (such as the station with code C0U750), we need to read each CSV file and put the data for that station into a dataframe called weather. Finally, we delete all downloaded data and data generated after decompression to save storage space in the cloud.\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather Finally, we rearrange the data in the site, delete unnecessary field information, and sort them by time as follows:\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN Above, we have successfully demonstrated the reading example of air quality data (air), water level data (water) and meteorological data (weather). In the following discussion, we will use air quality data to demonstrate basic time series data processing. The same methods can also be easily applied to water level data or meteorological data and obtain similar results. You are encouraged to try it yourself.\nData Preprocessing We first resample the data according to the method introduced in Section 4.1 and take the hourly average (air_hour), daily average (air_day), and monthly average (air_month) of the data, respectively.\nair_hour = air.resample('H').mean() air_day = air.resample('D').mean() air_month = air.resample('M').mean() Then we remove the outliers in the air_hour data according to the outlier detection method introduced in Section 4.1 and fill in the missing data with the Forward fill method.\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') # remove the outliers outlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outliers_removed = outlierDetection.remover(interpolate=False) air_hour_df = outliers_removed.to_dataframe() air_hour_df.rename(columns={'time': 'timestamp', 'y_0': 'PM25'}, inplace=True) air_hour_df.set_index('timestamp', inplace=True) air_hour = air_hour_df air_hour = air_hour.resample('H').mean() # fill in the missing data with the Forward fill method air_hour.ffill(inplace=True) Stationary Evaluation Before proceeding to predict the data, we first check the stationarity of the data. We select the period we want to detect (for example, 2020-06-10 ~ 2020-06-17) and store the data of this period in the data variable.\ndata = air_hour.loc['2020-06-10':'2020-06-17'] Then we calculate these data’s mean (mean) and variation (var) and plot them.\nnmp = data.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() It can be seen from the figure that the mean does not change much, but the variance varies greatly. We say that such data are poorly stationary; conversely, if the data is stationary, the change in its mean and variance will have nothing to do with time.\nIn other words, if the data distribution has a particular trend over time, it has no stationarity. If the data distribution does not change over time, while the mean and variance remain fixed, it has stationarity. The information on stationarity helps find a suitable model and predict future values.\nThere are at least two common ways to check whether data is stationary:\nAugmented Dickey Fuller (ADF) test: Using the unit root test，the data are stationary if the p-value \u003c 0.05. Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test: Contrary to the ADF test, if p-value \u003c 0.05, the data is not stationary. # ADF Test result = adfuller(data.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -2.7026194088541704 p-value: 0.07358609270498144 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.620177 p-value: 0.020802 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 If we take the sample data we use as an example, the p-value obtained by the ADF test is 0.073, so the information is not stationary. To achieve stationarity, we next differentiate the data, subtract the i-1th data from the i-th data, and use the obtained results to test again.\nIn the data format of the dataframe, we can directly use data.diff() to differentiate the data and name the differentiated data as data_diff.\ndata_diff = data.diff() data_diff PM25 timestamp\t2020-06-10 00:00:00\tNaN 2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 We can see that the first data is Nan because the first data cannot be subtracted from the previous data, so we have to discard the first data.\ndata_diff = data_diff[1:] data_diff PM25 timestamp\t2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 2020-06-10 05:00:00\t-1.300000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 Then we plot the data to observe the relationship between the mean and variance of the data after differentiation over time.\nnmp = data_diff.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() From the above results, we find that the change in the mean is still small, while the change in the variance becomes smaller. We then repeat the above stationarity evaluation steps:\n# PM25 # ADF Test result = adfuller(data_diff.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data_diff.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -13.350457196046884 p-value: 5.682260865619701e-25 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.114105 p-value: 0.100000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 After the test, the p-value of the ADF test is 5.68e-25, which shows that the data after a difference is stationary, and this result will be used in the subsequent prediction model in the following.\nData Forecast After data preprocessing, we demonstrate using different prediction models to predict time series data. We will use ARIMA, SARIMAX, auto_arima, Prophet, LSTM, and Holt-Winter models.\nARIMA The ARIMA model is an extended version of the ARMA model, so we first introduce the ARMA model and split the ARMA model into two parts, namely:\nAutoregressive model (AR): Use a parameter p and make a linear combination of the previous p historical values to predict the current value. Moving average model (MA): Use a parameter q and make a linear combination of the previous q prediction errors using the AR model to predict the current value. The ARIMA model uses one more parameter, d, than the ARMA model. If the data is not stationary, it needs to be differentiated, and the parameter d represents the number of times to be differentiated.\nBelow we use air quality data to conduct an exercise. First, we plot the data to select the piece of data to use:\nair_hour.loc['2020-06-01':'2020-06-30']['PM25'].plot(figsize=(12, 8)) We select a piece of data that we want to use and divide the data into two parts:\nTrain data: used to train the model and find the most suitable parameters. Test data: used to evaluate the model’s accuracy in data prediction. In our following example, we set the length of the test data to be 48 hours (train_len=-48) and the training data to be all the data minus the last 48 hours.\ndata_arima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_arima.iloc[:train_len] test = data_arima.iloc[train_len:] We first evaluate the stationarity of this piece of data. Then we determine the value of the d parameter by the number of differentiations required.\n# Run Dicky-Fuller test result = adfuller(train) # Print test statistic print('The test stastics:', result[0]) # Print p-value print(\"The p-value:\",result[1]) The test stastics: -3.1129543556288826 The p-value: 0.025609243615341074 Since the p-value is already smaller than 0.05, we can continue exploring the parameters p and q in the ARIMA model without differentiations (d=0). We take a simple method to combine the possible combinations of p and q, respectively, and determine the parameter values by evaluating the quality of the resulting models.\nWe can use the AIC or BIC method to evaluate whether the model fits the training data. Generally speaking, the smaller the judged value, the better the effect of the model. For example, we first limit the range of p and q between 0 and 2 so that there are nine possible combinations. Then, we check the values of AIC and BIC, respectively, and use the combination of p and q with the smallest value as the decision value of the parameters.\nwarnings.filterwarnings('ignore') order_aic_bic =[] # Loop over p values from 0-2 for p in range(3): # Loop over q values from 0-2 for q in range(3): try: # create and fit ARMA(p,q) model model = sm.tsa.statespace.SARIMAX(train['PM25'], order=(p, 0, q)) results = model.fit() # Print order and results order_aic_bic.append((p, q, results.aic, results.bic)) except: print(p, q, None, None) # Make DataFrame of model order and AIC/BIC scores order_df = pd.DataFrame(order_aic_bic, columns=['p', 'q', 'aic','bic']) # lets sort them by AIC and BIC # Sort by AIC print(\"Sorted by AIC \") # print(\"\\n\") print(order_df.sort_values('aic').reset_index(drop=True)) # Sort by BIC print(\"Sorted by BIC \") # print(\"\\n\") print(order_df.sort_values('bic').reset_index(drop=True)) Sorted by AIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 Sorted by BIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 We found that when (p,q) = (1,0), the values of AIC and BIC are the smallest, representing the best configuration of the model. Therefore, we set the three parameters p, d, and q to 1, 0, and 0, respectively, and then started training the model.\n# Instantiate model object model = ARIMA(train, order=(1,0,0)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ============================================================================== Dep. Variable: PM25 No. Observations: 72 Model: ARIMA(1, 0, 0) Log Likelihood -168.853 Date: Fri, 26 Aug 2022 AIC 343.706 Time: 05:01:13 BIC 350.536 Sample: 06-17-2020 HQIC 346.425 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ const 6.3774 1.959 3.255 0.001 2.537 10.218 ar.L1 0.7792 0.047 16.584 0.000 0.687 0.871 sigma2 6.2934 0.746 8.438 0.000 4.832 7.755 =================================================================================== Ljung-Box (L1) (Q): 0.08 Jarque-Bera (JB): 76.49 Prob(Q): 0.77 Prob(JB): 0.00 Heteroskedasticity (H): 2.30 Skew: 1.44 Prob(H) (two-sided): 0.05 Kurtosis: 7.15 =================================================================================== We then use the test data to make predictions and evaluate the accuracy of the predictions. From the resulting graph, we can find that the curve of the data prediction result is too smooth, which is very different from the actual value. If you observe the changing trend of the overall data, you will find that the raw data itself fluctuates regularly, but ARIMA can only predict the trend of the data. If you want to predict the data’s value accurately, there is still a considerable gap in the results.\ndata_arima['forecast'] = results.predict(start=24*5-48, end=24*5) data_arima[['PM25', 'forecast']].plot(figsize=(12, 8)) SARIMAX data_sarimax = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_sarimax.iloc[:train_len] test = data_sarimax.iloc[train_len:] We next introduce the SARIMAX model. The SARIMAX model has seven parameters, p, d, q, P, D, Q, s. These parameters can be divided into two groups: the first group is order=(p, d, q), and they are the same as the parameters of the ARIMA model; the other group is seasonal_order=(P, D, Q, s), and they are the periodic AR model parameters, the periodic differentiation times, the periodic MA model parameters, and the periodic length.\n參數 說明 p AR 模型參數 d 達到平穩性所需要的差分次數 q MA 模型參數 P 週期性的 AR 模型參數 D 週期上達到平穩性所需要的差分次數 Q 週期性的 MA 模型參數 s 週期長度 Since the previous observations demonstrated that these data generally have a periodic change of about 24 hours, we set s=24 and use the following commands to build the model.\n# Instantiate model object model = SARIMAX(train, order=(1,0,0), seasonal_order=(0, 1, 0, 24)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ========================================================================================== Dep. Variable: PM25 No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:26 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== Next, we make predictions using the test data and visualize the prediction results. Although the SARIMA model’s prediction results still have room to improve, they are already much better than the ARIMA model.\ndata_sarimax['forecast'] = results.predict(start=24*5-48, end=24*5) data_sarimax[['PM25', 'forecast']].plot(figsize=(12, 8)) auto_arima We use the pmdarima Python package, which is similar to the auto.arima model in R. The package can automatically find the most suitable ARIMA model parameters, increasing users’ convenience when using ARIMA models. The pmdarima.ARIMA object in the pmdarima package currently contains three models: ARMA, ARIMA, and SARIMAX. When using the pmdarima.auto_arima method, as long as the parameters p, q, P, and Q ranges are provided, the most suitable parameter combination is found within the specified range.\nNext, we will implement how to use pmdarima.auto_arima, and first divide the data set into training data and test data:\ndata_autoarima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_autoarima.iloc[:train_len] test = data_autoarima.iloc[train_len:] For the four parameters p, q, P, Q, we use start and max to specify the corresponding ranges. We also set the periodic parameter seasonal to True and the periodic variable m to 24 hours. Then we can directly get the best model parameter combination and model fitting results.\nresults = pm.auto_arima(train,start_p=0, d=0, start_q=0, max_p=5, max_d=5, max_q=5, start_P=0, D=1, start_Q=0, max_P=5, max_D=5, max_Q=5, m=24, seasonal=True, error_action='warn', trace = True, supress_warnings=True, stepwise = True, random_state=20, n_fits = 20) print(results.summary()) Performing stepwise search to minimize aic ARIMA(0,0,0)(0,1,0)[24] intercept : AIC=268.023, Time=0.04 sec ARIMA(1,0,0)(1,1,0)[24] intercept : AIC=247.639, Time=0.85 sec ARIMA(0,0,1)(0,1,1)[24] intercept : AIC=250.711, Time=0.79 sec ARIMA(0,0,0)(0,1,0)[24] : AIC=271.305, Time=0.04 sec ARIMA(1,0,0)(0,1,0)[24] intercept : AIC=247.106, Time=0.09 sec ARIMA(1,0,0)(0,1,1)[24] intercept : AIC=247.668, Time=0.45 sec ARIMA(1,0,0)(1,1,1)[24] intercept : AIC=inf, Time=2.63 sec ARIMA(2,0,0)(0,1,0)[24] intercept : AIC=249.013, Time=0.15 sec ARIMA(1,0,1)(0,1,0)[24] intercept : AIC=248.924, Time=0.21 sec ARIMA(0,0,1)(0,1,0)[24] intercept : AIC=250.901, Time=0.11 sec ARIMA(2,0,1)(0,1,0)[24] intercept : AIC=250.579, Time=0.30 sec ARIMA(1,0,0)(0,1,0)[24] : AIC=246.926, Time=0.06 sec ARIMA(1,0,0)(1,1,0)[24] : AIC=247.866, Time=0.28 sec ARIMA(1,0,0)(0,1,1)[24] : AIC=247.933, Time=0.31 sec ARIMA(1,0,0)(1,1,1)[24] : AIC=inf, Time=2.35 sec ARIMA(2,0,0)(0,1,0)[24] : AIC=248.910, Time=0.08 sec ARIMA(1,0,1)(0,1,0)[24] : AIC=248.893, Time=0.09 sec ARIMA(0,0,1)(0,1,0)[24] : AIC=252.779, Time=0.08 sec ARIMA(2,0,1)(0,1,0)[24] : AIC=250.561, Time=0.17 sec Best model: ARIMA(1,0,0)(0,1,0)[24] Total fit time: 9.122 seconds SARIMAX Results ========================================================================================== Dep. Variable: y No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:37 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== Finally, we use the best model found for data prediction, and plot the prediction results and test data on the same graph in the form of an overlay. Since the best model found this time is the SARIMAX model just introduced, the results of both predictors are roughly the same.\nresults.predict(n_periods=10) 2020-06-20 00:00:00 10.371336 2020-06-20 01:00:00 13.142043 2020-06-20 02:00:00 13.505843 2020-06-20 03:00:00 9.506395 2020-06-20 04:00:00 7.450378 2020-06-20 05:00:00 7.782850 2020-06-20 06:00:00 7.633757 2020-06-20 07:00:00 5.200781 2020-06-20 08:00:00 3.634188 2020-06-20 09:00:00 3.946824 Freq: H, dtype: float64 data_autoarima['forecast']= pd.DataFrame(results.predict(n_periods=48), index=test.index) data_autoarima[['PM25', 'forecast']].plot(figsize=(12, 8)) Prophet Next, we use the Prophet model provided in the kats suite for data prediction. This model is proposed by Facebook’s data science team, and it is good at predicting periodic time series data and can tolerate missing data, data shift, and outliers.\nWe first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_prophet = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_prophet.iloc[:train_len] test = data_prophet.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) We then use ProphetParams to set the Prophet model’s parameters and the training data and parameters to initialize the ProphetModel. Then we use the fit method to build the model and use the predict method to predict the data, and then we can get the final prediction result.\n# Specify parameters params = ProphetParams(seasonality_mode=\"multiplicative\") # Create a model instance m = ProphetModel(trainData, params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq=\"H\") data_prophet['forecast'] = fcst[['time','fcst']].set_index('time') fcst time\tfcst\tfcst_lower fcst_upper 0\t2020-06-20 00:00:00\t14.705192\t12.268361\t17.042476 1\t2020-06-20 01:00:00\t15.089580\t12.625568\t17.573396 2\t2020-06-20 02:00:00\t14.921077\t12.459802\t17.411335 3\t2020-06-20 03:00:00\t13.846131\t11.444988\t16.200284 4\t2020-06-20 04:00:00\t12.278140\t9.863531\t14.858334 5\t2020-06-20 05:00:00\t10.934739\t8.372450\t13.501025 6\t2020-06-20 06:00:00\t10.126712\t7.654647\t12.658054 7\t2020-06-20 07:00:00\t9.535067\t7.034313\t11.762639 8\t2020-06-20 08:00:00\t8.661877\t6.255147\t11.132732 9\t2020-06-20 09:00:00\t7.424133\t5.055052\t9.770750 10\t2020-06-20 10:00:00\t6.229786\t3.640543\t8.625856 11\t2020-06-20 11:00:00\t5.464764\t3.039011\t7.939283 12\t2020-06-20 12:00:00\t4.998005\t2.692023\t7.550191 13\t2020-06-20 13:00:00\t4.334771\t1.961382\t6.506875 14\t2020-06-20 14:00:00\t3.349172\t1.059836\t5.768178 15\t2020-06-20 15:00:00\t2.819902\t0.399350\t5.226658 16\t2020-06-20 16:00:00\t4.060070\t1.556264\t6.322976 17\t2020-06-20 17:00:00\t7.792830\t5.331987\t10.237182 18\t2020-06-20 18:00:00\t13.257767\t10.873149\t15.542380 19\t2020-06-20 19:00:00\t18.466805\t15.895210\t20.874602 20\t2020-06-20 20:00:00\t21.535994\t19.150397\t23.960260 21\t2020-06-20 21:00:00\t22.005943\t19.509141\t24.691836 22\t2020-06-20 22:00:00\t21.014449\t18.610361\t23.661906 23\t2020-06-20 23:00:00\t20.191905\t17.600568\t22.868388 24\t2020-06-21 00:00:00\t20.286952\t17.734177\t22.905280 25\t2020-06-21 01:00:00\t20.728067\t18.235829\t23.235212 26\t2020-06-21 02:00:00\t20.411124\t17.755181\t22.777073 27\t2020-06-21 03:00:00\t18.863739\t16.261775\t21.315573 28\t2020-06-21 04:00:00\t16.661351\t13.905466\t19.374726 29\t2020-06-21 05:00:00\t14.781150\t12.401465\t17.499478 30\t2020-06-21 06:00:00\t13.637436\t11.206142\t16.239831 31\t2020-06-21 07:00:00\t12.793609\t9.940829\t15.319559 32\t2020-06-21 08:00:00\t11.580455\t9.059603\t14.261605 33\t2020-06-21 09:00:00\t9.891025\t7.230943\t12.471543 34\t2020-06-21 10:00:00\t8.271552\t5.840853\t10.677227 35\t2020-06-21 11:00:00\t7.231671\t4.829449\t9.733231 36\t2020-06-21 12:00:00\t6.592515\t4.108251\t9.107216 37\t2020-06-21 13:00:00\t5.699548\t3.288052\t8.019402 38\t2020-06-21 14:00:00\t4.389985\t1.848621\t6.825121 39\t2020-06-21 15:00:00\t3.685033\t1.196467\t6.150064 40\t2020-06-21 16:00:00\t5.289956\t2.907623\t8.012851 41\t2020-06-21 17:00:00\t10.124029\t7.397842\t12.676256 42\t2020-06-21 18:00:00\t17.174959\t14.670539\t19.856592 43\t2020-06-21 19:00:00\t23.856724\t21.102924\t26.712359 44\t2020-06-21 20:00:00\t27.746195\t24.636118\t30.673178 45\t2020-06-21 21:00:00\t28.276321\t25.175013\t31.543197 46\t2020-06-21 22:00:00\t26.932054\t23.690073\t29.882014 47\t2020-06-21 23:00:00\t25.811943\t22.960132\t28.912079 data_prophet timestamp\tPM25\tforecast 2020-06-17 00:00:00\t6.300000\tNaN 2020-06-17 01:00:00\t11.444444\tNaN 2020-06-17 02:00:00\t6.777778\tNaN 2020-06-17 03:00:00\t4.875000\tNaN 2020-06-17 04:00:00\t5.444444\tNaN ...\t...\t... 2020-06-21 19:00:00\t18.777778\t23.856724 2020-06-21 20:00:00\t21.400000\t27.746195 2020-06-21 21:00:00\t11.222222\t28.276321 2020-06-21 22:00:00\t9.800000\t26.932054 2020-06-21 23:00:00\t8.100000\t25.811943 We use the built-in drawing method of ProphetModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range, and overall the data prediction results are satisfactory.\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() LSTM Next, we introduce the Long Short-Term Memory (LSTM) model for data prediction. The LSTM model is a predictive model suitable for continuous data because it will generate different long-term and short-term memories for data at different times and use it to predict the final result. Currently, the LSTM model is provided in the kats package, so we can directly use the syntax similar to using the Prophet model.\nWe first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_lstm = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_lstm.iloc[:train_len] test = data_lstm.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) Then we select the parameters of the LSTM model in order, namely the number of training times (num_epochs), the time length of data read in at one time (time_window), and the number of neural network layers related to long-term and short-term memory (hidden_size). Then you can directly perform model training and data prediction.\nparams = LSTMParams( hidden_size=10, # number of hidden layers time_window=24, num_epochs=30 ) m = LSTMModel(trainData, params) m.fit() fcst = m.predict(steps=48, freq=\"H\") data_lstm['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst\tfcst_lower\tfcst_upper 0\t2020-06-20 00:00:00\t11.905971\t11.310672\t12.501269 1\t2020-06-20 01:00:00\t10.804338\t10.264121\t11.344554 2\t2020-06-20 02:00:00\t9.740741\t9.253704\t10.227778 3\t2020-06-20 03:00:00\t8.696406\t8.261586\t9.131226 4\t2020-06-20 04:00:00\t7.656923\t7.274077\t8.039769 5\t2020-06-20 05:00:00\t6.608442\t6.278019\t6.938864 6\t2020-06-20 06:00:00\t5.543790\t5.266600\t5.820979 7\t2020-06-20 07:00:00\t4.469023\t4.245572\t4.692474 8\t2020-06-20 08:00:00\t3.408312\t3.237897\t3.578728 9\t2020-06-20 09:00:00\t2.411980\t2.291381\t2.532578 10\t2020-06-20 10:00:00\t1.564808\t1.486567\t1.643048 11\t2020-06-20 11:00:00\t0.982147\t0.933040\t1.031255 12\t2020-06-20 12:00:00\t0.792612\t0.752981\t0.832242 13\t2020-06-20 13:00:00\t1.105420\t1.050149\t1.160691 14\t2020-06-20 14:00:00\t1.979013\t1.880062\t2.077964 15\t2020-06-20 15:00:00\t3.408440\t3.238018\t3.578862 16\t2020-06-20 16:00:00\t5.337892\t5.070997\t5.604786 17\t2020-06-20 17:00:00\t7.659332\t7.276365\t8.042299 18\t2020-06-20 18:00:00\t10.104147\t9.598940\t10.609355 19\t2020-06-20 19:00:00\t12.047168\t11.444809\t12.649526 20\t2020-06-20 20:00:00\t12.880240\t12.236228\t13.524252 21\t2020-06-20 21:00:00\t12.748750\t12.111312\t13.386187 22\t2020-06-20 22:00:00\t12.128366\t11.521947\t12.734784 23\t2020-06-20 23:00:00\t11.311866\t10.746273\t11.877459 24\t2020-06-21 00:00:00\t10.419082\t9.898128\t10.940036 25\t2020-06-21 01:00:00\t9.494399\t9.019679\t9.969119 26\t2020-06-21 02:00:00\t8.551890\t8.124296\t8.979485 27\t2020-06-21 03:00:00\t7.592260\t7.212647\t7.971873 28\t2020-06-21 04:00:00\t6.613075\t6.282421\t6.943729 29\t2020-06-21 05:00:00\t5.614669\t5.333936\t5.895402 30\t2020-06-21 06:00:00\t4.605963\t4.375664\t4.836261 31\t2020-06-21 07:00:00\t3.611552\t3.430974\t3.792129 32\t2020-06-21 08:00:00\t2.679572\t2.545593\t2.813550 33\t2020-06-21 09:00:00\t1.887442\t1.793070\t1.981814 34\t2020-06-21 10:00:00\t1.340268\t1.273255\t1.407282 35\t2020-06-21 11:00:00\t1.156494\t1.098669\t1.214318 36\t2020-06-21 12:00:00\t1.440240\t1.368228\t1.512252 37\t2020-06-21 13:00:00\t2.251431\t2.138859\t2.364002 38\t2020-06-21 14:00:00\t3.592712\t3.413076\t3.772347 39\t2020-06-21 15:00:00\t5.415959\t5.145161\t5.686757 40\t2020-06-21 16:00:00\t7.613187\t7.232528\t7.993847 41\t2020-06-21 17:00:00\t9.918564\t9.422636\t10.414493 42\t2020-06-21 18:00:00\t11.755348\t11.167580\t12.343115 43\t2020-06-21 19:00:00\t12.576593\t11.947764\t13.205423 44\t2020-06-21 20:00:00\t12.489052\t11.864599\t13.113504 45\t2020-06-21 21:00:00\t11.915885\t11.320090\t12.511679 46\t2020-06-21 22:00:00\t11.133274\t10.576610\t11.689938 47\t2020-06-21 23:00:00\t10.264495\t9.751270\t10.777719 We also use the built-in drawing method of LSTMModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range, but overall, the data prediction result (blue curve) is slightly lower than the test data (black curve).\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() Holt-Winter We also use the Holt-Winter model provided by the kats package, a method that uses moving averages to assign weights to historical data for data forecasting. We first divide the dataset into training and prediction data and observe the changes in the training data by drawing.\ndata_hw = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_hw.iloc[:train_len] test = data_hw.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) Then we need to set the parameters of the Holt-Winter model, which are to select whether to use addition or multiplication to decompose the time series data (the following example uses multiplication, mul), and the length of the period (the following example uses 24 hours). Then we can perform model training and data prediction.\nwarnings.simplefilter(action='ignore') # Specify parameters params = HoltWintersParams( trend=\"mul\", seasonal=\"mul\", seasonal_periods=24, ) # Create a model instance m = HoltWintersModel( data=trainData, params=params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq='H') data_hw['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst 72\t2020-06-20 00:00:00\t14.140232 73\t2020-06-20 01:00:00\t14.571588 74\t2020-06-20 02:00:00\t12.797056 75\t2020-06-20 03:00:00\t10.061594 76\t2020-06-20 04:00:00\t9.927476 77\t2020-06-20 05:00:00\t8.732691 78\t2020-06-20 06:00:00\t10.257460 79\t2020-06-20 07:00:00\t8.169070 80\t2020-06-20 08:00:00\t6.005400 81\t2020-06-20 09:00:00\t5.038056 82\t2020-06-20 10:00:00\t6.391835 83\t2020-06-20 11:00:00\t5.435677 84\t2020-06-20 12:00:00\t3.536135 85\t2020-06-20 13:00:00\t2.725477 86\t2020-06-20 14:00:00\t2.588198 87\t2020-06-20 15:00:00\t2.967987 88\t2020-06-20 16:00:00\t3.329448 89\t2020-06-20 17:00:00\t4.409821 90\t2020-06-20 18:00:00\t10.295263 91\t2020-06-20 19:00:00\t10.587033 92\t2020-06-20 20:00:00\t14.061718 93\t2020-06-20 21:00:00\t18.597275 94\t2020-06-20 22:00:00\t12.040684 95\t2020-06-20 23:00:00\t12.124081 96\t2020-06-21 00:00:00\t13.522973 97\t2020-06-21 01:00:00\t13.935499 98\t2020-06-21 02:00:00\t12.238431 99\t2020-06-21 03:00:00\t9.622379 100\t2020-06-21 04:00:00\t9.494116 101\t2020-06-21 05:00:00\t8.351486 102\t2020-06-21 06:00:00\t9.809694 103\t2020-06-21 07:00:00\t7.812468 104\t2020-06-21 08:00:00\t5.743248 105\t2020-06-21 09:00:00\t4.818132 106\t2020-06-21 10:00:00\t6.112815 107\t2020-06-21 11:00:00\t5.198396 108\t2020-06-21 12:00:00\t3.381773 109\t2020-06-21 13:00:00\t2.606503 110\t2020-06-21 14:00:00\t2.475216 111\t2020-06-21 15:00:00\t2.838426 112\t2020-06-21 16:00:00\t3.184109 113\t2020-06-21 17:00:00\t4.217320 114\t2020-06-21 18:00:00\t9.845847 115\t2020-06-21 19:00:00\t10.124881 116\t2020-06-21 20:00:00\t13.447887 117\t2020-06-21 21:00:00\t17.785455 118\t2020-06-21 22:00:00\t11.515076 119\t2020-06-21 23:00:00\t11.594832 We also use the built-in drawing method of HoltWintersModel to draw the training data (black curve) and prediction results (blue curve).\nm.plot() To evaluate the correctness of the prediction results, we also use another drawing method to draw the training data (black curve), test data (black curve), and prediction results (blue curve) at the same time. The figure shows that the blue and black curves are roughly consistent in the changing trend and value range. Still, overall, the data prediction result (blue curve) responds slightly slower to the rising slope than the test data (black curve).\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') # ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() Comparison Finally, to facilitate observation and comparison, we will draw the prediction results of the six models introduced in the figure below simultaneously (Note: You must first run all the codes of the above prediction models to see the results of these six prediction models). We can observe and compare the prediction accuracy of the six models under different time intervals and curve change characteristics, which is convenient for users to decide on the final model selection and possible future applications.\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8)) data_arima[['PM25', 'forecast']].plot(ax=axes[0, 0], title='ARIMA') data_sarimax[['PM25', 'forecast']].plot(ax=axes[1, 0], title='SARIMAX') data_autoarima[['PM25', 'forecast']].plot(ax=axes[2, 0], title='auto_arima') data_prophet[['PM25', 'forecast']].plot(ax=axes[0, 1], title='Prophet') data_lstm[['PM25', 'forecast']].plot(ax=axes[1, 1], title='LSTM') data_hw[['PM25', 'forecast']].plot(ax=axes[2, 1], title='Holt-Winter') fig.tight_layout(pad=1, w_pad=2, h_pad=5) References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Rob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice, 3rd edition (https://otexts.com/fpp3/) Stationarity, NIST Engineering Statistics Handbook (https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm) Unit root test - Wikipedia (https://en.wikipedia.org/wiki/Unit_root_test) Akaike information criterion (AIC) - Wikipedia (https://en.wikipedia.org/wiki/Akaike_information_criterion) Bayesian information criterion (BIC) - Wikipedia (https://en.wikipedia.org/wiki/Bayesian_information_criterion) ARIMA models (https://otexts.com/fpp2/arima.html) SARIMAX: Introduction (https://www.statsmodels.org/stable/examples/notebooks/generated/statespace_sarimax_stata.html) Prophet: Forecasting at scale (https://facebook.github.io/prophet/) Long short-term memory (LSTM) – Wikipedia (https://en.wikipedia.org/wiki/Long_short-term_memory) Time Series Forecasting with ARIMA Models In Python [Part 1] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48?gi=264dc7630363) Time Series Forecasting with ARIMA Models In Python [Part 2] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-2-91a30d10efb0) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057) Kats - Time Series Forecasting By Facebook | by Himanshu Sharma | MLearning.ai | Medium (https://medium.com/mlearning-ai/kats-time-series-forecasting-by-facebook-a2741794d814) ",
    "description": "We use the sensing data of the Civil IoT Taiwan Data Service Platform and apply existing Python data science packages (such as scikit-learn, Kats, etc.) to compare the prediction results of different data prediction models. We use graphics to present the data and discuss the significance of the data prediction of the dataset at different time resolutions in the real field, as well as possible derived applications.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "4.2. Time Series Data Forecast",
    "uri": "/en/ch4/ch4.2/"
  },
  {
    "content": "\nTable Of Contents Voronoi diagram Convex hull Clustering Kernel density Spatial interpolation Inverse Distance Weighting Kriging Nearest neighbor Interpolation Contour Profile References Environmental microsensors scattered throughout our surroundings play a crucial role in gathering information. This data aids us in making informed decisions and taking appropriate actions. A key aspect of this process involves understanding the spatial relationships between sensor stations when analyzing their data. These station locations can reveal geometric patterns or form spatial clusters. Moreover, by considering the location of these stations and the differences in their readings, we can estimate values for areas without stations. This approach allows us to develop a more comprehensive overview of value distribution. This detailed distribution helps us investigate how these values relate to various environmental factors. In this section, we’ll utilize data from flooding sensors and groundwater table observation stations across different counties, provided by the Water Resources Agency (MOEA), to conduct some basic spatial analyses.\nVoronoi diagram Initially, we need to figure out the specific area each monitoring station covers and whether the data from these stations accurately reflect the conditions of their respective areas. To do this, we can use something called a Voronoi Diagram. Imagine drawing lines halfway between each pair of nearby stations and then extending these lines until they form a shape like a polygon. Each station sits at the center of one of these polygons, and the data it collects represents what’s happening inside that polygon. We’ll try this out by creating Voronoi Diagrams using data from flood sensors in Chiayi City and Chiayi County. This will give us a basic idea of how far the influence of each flood sensor reaches.\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install pykrige !pip install pykrige #install elevation !pip install elevation #install affine rasterio !pip install affine rasterio #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # downlaod the county boundary shpfile from open database !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # # get flood sensors' data by pyCIOT wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") flood_list = wa + wa2 county = gpd.read_file('/content/shp/COUNTY_MOI_1090820.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_flood, basemap, how='intersection') from scipy.spatial import Voronoi, voronoi_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) inputp = intersected_data[['lon','lat']] basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); vor = Voronoi(inputp) voronoi_plot_2d(vor,ax = ax,show_vertices=False,) plt.show() In addition, we can map out the areas covered by each station using a technique called Delaunay Triangulation. This involves picking one station as a central point and then connecting it to the two nearest stations, forming a triangular area. If we treat this triangular area as if it has uniform conditions, we can estimate conditions in this area using the average data from its three corner stations.\nIn summary, these two methods – Voronoi Diagrams and Delaunay Triangulation – help us visualize the layout of these sensors and the geographical patterns they create.\nfrom scipy.spatial import Delaunay, delaunay_plot_2d import numpy as np fig, ax = plt.subplots(figsize=(6, 10)) #input should be array inputp = np.array(inputp) tri = Delaunay(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); delaunay_plot_2d(tri,ax=ax) plt.show() Convex hull The Convex Hull algorithm is designed to pick out stations that are on the outer edges of a group of stations. It creates the smallest possible polygon that includes all the points. This way, we can identify the main area where these stations are clustered and perform further calculations. To use the Convex Hull algorithm, we first sort the stations by their x coordinates. If two stations have the same x coordinate, then we sort them by their y coordinates. This sorting helps us find the outermost points that will form the edges of our polygon. (There are also other methods that use similar ideas.) This algorithm is useful for determining the effective monitoring areas of stations. For instance, it allows us to analyze the coverage area of flooding sensors by looking at their placement in Chiayi City and Chiayi County.\nfrom scipy.spatial import ConvexHull, convex_hull_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) hull = ConvexHull(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); convex_hull_plot_2d(hull,ax=ax) plt.tight_layout() Clustering The closer the monitoring stations are to each other, the more they share similar environmental influences. By using the K-means algorithm, we can group these stations into clusters. This helps us understand how sensor data from the Civil IoT Taiwan Data Platform is related to the environment around them.\nWhen we use K-means clustering, we first decide on the number of groups, or ‘clusters’, we want to create (let’s call this number ’n’). The algorithm then picks ’n’ starting points at random, which act as the centers of these clusters. It looks for stations nearest to each center, considering the straight-line distance from the stations to these central points.\nK-means clustering groups the stations based on these distances and calculates an average for each cluster. It repeatedly adjusts the positions of the centers and regroups the stations to ensure that each station is as close as possible to the center of its cluster. The process ends when the stations can’t get any closer to the centers, indicating that the best grouping has been achieved.\n# get the groundwater station data through pyCIOT count = 733 num = 0 water_level = pd.DataFrame([]) while(num\u003c=count): url_level = \"https://sta.ci.taiwan.gov.tw/STA_WaterResource_v2/v1.0/Datastreams?$skip=\"+str(num)+\"\u0026$filter=%28%28Thing%2Fproperties%2Fauthority_type+eq+%27%E6%B0%B4%E5%88%A9%E7%BD%B2%27%29+and+substringof%28%27Datastream_Category_type%3D%E5%9C%B0%E4%B8%8B%E6%B0%B4%E4%BD%8D%E7%AB%99%27%2CDatastreams%2Fdescription%29%29\u0026$expand=Thing,Thing%28%24expand%3DLocations%29,Observations%28%24top%3D1%3B%24orderby%3DphenomenonTime+desc%3B%24top%3D1%29\u0026$count=true\" ssl._create_default_https_context = ssl._create_unverified_context r_l = urllib.request.urlopen(url_level) string_l = r_l.read().decode('utf-8') jf_level = json.loads(string_l) station = pd.DataFrame(jf_level['value']).filter(items=['Thing','observedArea','Observations']) station['lat']=station['observedArea'] for i in range(len(station)): station['Thing'][i] = station['Thing'][i]['properties']['stationName'] if pd.isnull(station['observedArea'][i]): station['lat'][i]=-1 station['observedArea'][i]=-1 else: station['lat'][i]=station['lat'][i]['coordinates'][1] station['observedArea'][i]=station['observedArea'][i]['coordinates'][0] if len(station['Observations'][i])!=0: station['Observations'][i] = station['Observations'][i][0]['result'] else: station['Observations'][i] = -1 station = station.rename(columns={\"Thing\": \"name\", 'observedArea': 'lon'}) if num ==0 : water_level = station else: water_level = pd.concat([water_level, station]) num+=100 result_df = water_level.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) station = station[station.lon!=-1] gdf_level = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") # intersect with county boundary basemap = county.loc[county['COUNTYNAME'].isin([\"雲林縣\"])] basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_level, basemap, how='intersection') from sklearn.cluster import KMeans from scipy.spatial import ConvexHull import folium clusterp = intersected_data[[\"name\",\"lon\", 'lat', 'Observations']] # 1. identify the clusters by kmeans #1.1 pre-processing X = clusterp.iloc[:, 1:3].values # ddecide the number groups by elbow method wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1, 11), wcss) plt.title('The Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show() # 1.2 applying K-Means model kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42) y_kmeans = kmeans.fit_predict(X) # 1.3 map data back to df clusterp['cluster'] = y_kmeans+1 # to step up to group 1 to 4 # 2. display the result on maps m = folium.Map(location=[clusterp['lat'].mean(), clusterp['lon'].mean()], tiles='CartoDB positron', zoom_start=7) # create the layer by kmeans layer1 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup1\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer1) layer2 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup2\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer2) layer3 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup3\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer3) layer4 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup4\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer4) # create symbology my_symbol_css_class= \"\"\" \u003cstyle\u003e .fa-g1:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g4 '; } .fa-g1bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g4 '; } \u003c/style\u003e \"\"\" # add the symbology code to the map m.get_root().html.add_child(folium.Element(my_symbol_css_class)) for index, row in clusterp.iterrows(): if row['cluster'] == 1: color='black' fa_symbol = 'fa-g1' lay = layer1 elif row['cluster'] == 2: color='purple' fa_symbol = 'fa-g2' lay = layer2 elif row['cluster'] == 3: color='orange' fa_symbol = 'fa-g3' lay = layer3 elif row['cluster'] == 4: color='blue' fa_symbol = 'fa-g4' lay = layer4 folium.Marker( location=[row['lat'], row['lon']], title = row['name']+ 'group:{}'.format(str(row[\"cluster\"])), popup = row['name']+ 'group:{},value:{}'.format(str(row[\"cluster\"]),str(row['Observations'])), icon= folium.Icon(color=color, icon=fa_symbol, prefix='fa')).add_to(lay) # display the result on maps layer_list = [layer1,layer2,layer3,layer4] color_list = ['black','purple','orange','blue'] for g in clusterp['cluster'].unique(): latlon_cut =clusterp[clusterp['cluster']==g].iloc[:, 1:3] hull = ConvexHull(latlon_cut.values) Lat = latlon_cut.values[hull.vertices,0] Long = latlon_cut.values[hull.vertices,1] cluster = pd.DataFrame({'lat':Lat,'lon':Long }) area = list(zip(cluster['lat'],cluster['lon'])) list_index = g-1 lay_cluster = layer_list[list_index ] folium.Polygon(locations=area, color=color_list[list_index], weight=2, fill=True, fill_opacity=0.1, opacity=0.8).add_to(lay_cluster) folium.LayerControl(collapsed=False,position= 'bottomright').add_to(m) print(m) m.save('River_clustering.html') Kernel density The idea of density is like measuring how crowded things are in a specific space. We usually calculate it using the formula d = N/A, where d stands for density, N is the number of things we’re looking at (like events or objects), and A is the size of the area we’re considering. But there’s a catch: this formula can be tricky because the size of the area can skew our results. For example, the same number of things in a small town versus a big city will give us different densities, making it hard to accurately measure how clustered or spread out these things are.\nTo get around this problem, we use something called kernel density. Imagine each thing we’re observing as the center of a circle, and we then look at everything within a certain distance (radius) of that center. Instead of just counting each thing once, we add up the values of everything within that radius. This approach lets us standardize the “area” part in our density formula, leading to a more uniform way of measuring how things are distributed. This helps us get a better picture of how tightly or loosely clustered things are in an area.\nbasemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] basemap = basemap.set_crs(4326,allow_override=True) gdf = gpd.overlay(gdf_level, basemap, how='intersection') # selecting the polygon's geometry field to filter out points that # are not overlaid import plotly.express as px fig = px.density_mapbox(gdf, lat='lat', lon='lon', z='Observations', radius=25, center=dict(lat=23.5, lon=120.5), zoom=8, mapbox_style=\"stamen-terrain\") fig.show() Spatial interpolation Using microsensors in our environment is a bit like doing a detailed survey of an area. This helps us guess how things like population are spread out. However, we can’t cover every inch of the Earth with these sensors, so we end up with some places having lots of data and others with very little. This is where spatial interpolation comes in. It’s a fancy way of using statistics to fill in the gaps in areas where we don’t have data, giving us a better overall picture.\nBefore we dive into spatial interpolation, we need to understand two types of models: deterministic and stochastic. In a deterministic model, we use known patterns to guess missing information. For example, in Taiwan, house numbers are organized with odd and even numbers on opposite sides of the street. So, if you know the numbers of the houses before and after a certain house, you can figure out that missing number. If house 6 is before and house 10 is after, then the missing house is likely number 8.\nOn the other hand, a stochastic model acknowledges that the world is complex and full of uncertainty. Instead of relying on set patterns, it uses probability and various factors to make educated guesses, accepting that these predictions might not always be spot on.\nInverse Distance Weighting To simplify, Inverse Distance Weighting (IDW) is like making an educated guess about a certain area based on the information we have from nearby locations. Imagine you know how much it rained in several nearby towns. If two towns are 100 meters apart and the difference in rainfall is 10 units, you’d guess that for every 10 meters closer you get to one town, the rainfall changes by 1 unit. IDW takes this idea but adjusts it because things in nature don’t always change evenly (like rain might not decrease in a straight line as you move away from a cloud). So, IDW uses a special calculation that gives more importance to what’s happening closer to the point we’re interested in.\nimport numpy as np import matplotlib.pyplot as plt from scipy.interpolate import Rbf def distance_matrix(x0, y0, x1, y1): obs = np.vstack((x0, y0)).T interp = np.vstack((x1, y1)).T # build distance matrix d0 = np.subtract.outer(obs[:,0], interp[:,0]) d1 = np.subtract.outer(obs[:,1], interp[:,1]) print(d0.dtype,d1.dtype) return np.hypot(d0, d1) def simple_idw(x, y, z, xi, yi, pows): dist = distance_matrix(x,y, xi,yi) # the IDW weight is 1 / distance weights = 1.0 / dist # weight is 1 weights /= weights.sum(axis=0) # set the Z value zi = np.dot(weights.T, z) return zi fig, ax = plt.subplots(figsize=(6, 4)) ax.set_aspect('equal') pows = 2 nx, ny = 100, 100 xmin, xmax = 119.8, 121.2 ymin, ymax = 23, 24 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] x = interpolatep['lon'] y = interpolatep['lat'] z = interpolatep['Observations'] x = x.astype(\"float64\") y = y.astype(\"float64\") z = z.astype(\"float64\") xi = np.linspace(xmin,xmax, nx) yi = np.linspace(ymin, ymax, ny) xi, yi = np.meshgrid(xi, yi) xi, yi = xi.flatten(), yi.flatten() # 計算 IDW grid = simple_idw(x,y,z,xi,yi,pows) grid = grid.reshape((ny, nx)) grid = grid.astype(\"float64\") plt.imshow(grid, extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='lightgray'); ax.scatter(x, y, marker=\".\", color='orange', s=z,label=\"input point\") plt.colorbar() plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.title('IDW') plt.show() Kriging Kriging is another method, somewhat like IDW, but it’s more sophisticated. It starts by creating a semi-variogram, which is a fancy way of mapping how values (like temperature, for example) change over distance. Kriging then divides the area into zones based on this map. When estimating an unknown point, Kriging considers the specific characteristics of each zone, making its guesses more tailored to local variations.\n# set the data extend and resolution import numpy as np resolution = 0.1 # cell size in meters gridx = np.arange(119.8, 121.2, resolution) gridy = np.arange(23, 24, resolution) # set the raster to polygon import itertools from shapely.geometry import Polygon def pixel2poly(x, y, z, resolution): \"\"\" x: x coords of cell y: y coords of cell z: matrix of values for each (x,y) resolution: spatial resolution of each cell \"\"\" polygons = [] values = [] half_res = resolution / 2 for i, j in itertools.product(range(len(x)), range(len(y))): minx, maxx = x[i] - half_res, x[i] + half_res miny, maxy = y[j] - half_res, y[j] + half_res polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)])) if isinstance(z, (int, float)): values.append(z) else: values.append(z[j, i]) return polygons, values # calculate ithe pykrige package from pykrige.ok import OrdinaryKriging krig = OrdinaryKriging(x=gdf[\"lon\"], y=gdf[\"lat\"], z=gdf['Observations'], variogram_model=\"spherical\", pseudo_inv=True) z, ss = krig.execute(\"grid\", gridx, gridy) plt.imshow(z); # dispaly by plotly import plotly.express as px polygons, values = pixel2poly(gridx, gridy, z, resolution) water_model = (gpd.GeoDataFrame({\"water_modelled\": values}, geometry=polygons, crs=\"EPSG:4326\") .to_crs(\"EPSG:4326\") ) fig = px.choropleth_mapbox(water_model, geojson=water_model.geometry, locations=water_model.index, color=\"water_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5, center={\"lat\": 24, \"lon\": 121}, zoom=6, mapbox_style=\"carto-positron\") fig.update_layout(margin=dict(l=0, r=0, t=30, b=10)) fig.update_traces(marker_line_width=0) Nearest neighbor Interpolation Nearest Neighbor Interpolation is much simpler. To find out something about a specific place, like how much rain fell there, you just look at the nearest weather station and use its data. This method works on the idea that places close to each other are likely to be more similar. It’s commonly used in things like enlarging photos, where you want to fill in new pixels based on the ones nearby.\nfrom scipy.interpolate import NearestNDInterpolator import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(6, 4)) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") X = np.linspace(min(xd), max(xd)) Y = np.linspace(min(yd), max(yd)) X, Y = np.meshgrid(X, Y) # 2D grid for interpolation interp = NearestNDInterpolator(list(zip(xd, yd)), zd) Z = interp(X, Y) im = ax.pcolormesh(X, Y, Z, shading='auto') basemap.plot(ax=ax, facecolor='none', edgecolor='gray'); sns.scatterplot(x='lon', y='lat', data=interpolatep,label=\"input point\") plt.legend() plt.colorbar(im) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.show() Contour Once we have interpolated data (like rainfall or temperature) across a region, we can visualize it using contours. Think of it like drawing elevation lines on a map, but instead of showing height, they show things like temperature or pollution levels. Lines closer together mean rapid changes, just like steep hills on a topographic map.\nfrom osgeo import gdal import numpy as np import matplotlib import matplotlib.pyplot as plt import elevation fig, ax = plt.subplots(figsize=(6, 10)) X = np.linspace(xmin, xmax) Y = np.linspace(ymin, ymax) krig = OrdinaryKriging(x=interpolatep['lon'], y=interpolatep['lat'], z=interpolatep['Observations'], variogram_model=\"spherical\") z, ss = krig.execute(\"grid\", X, Y) im = ax.contourf(z, cmap = \"viridis\", levels = list(range(-30, 30, 10)),extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='black'); plt.title(\"Elevation Contours Taiwan\") plt.show() Profile Lastly, there’s geographical profiling, which is another way to analyze this kind of data. Imagine drawing a straight line between two points and then looking at how values change along that line. It’s useful for understanding how things like air pollution (like PM2.5 particles) might change from one side of a road to the other.\ndef export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename): '''Export and save a kernel density raster.''' xres = (max_x - min_x) / len(XX) yres = (max_y - min_y) / len(YY) transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres) with rasterio.open( filename, mode = \"w\", driver = \"GTiff\", height = Z.shape[0], width = Z.shape[1], count = 1, dtype = Z.dtype, crs = proj, transform = transform, ) as new_dataset: new_dataset.write(Z, 1) from pykrige.ok import OrdinaryKriging from affine import Affine import rasterio import math start_cor = [119.9,23.2] end_cor = [120.1,23.9] npoints=100 X = np.linspace(xmin, xmax, npoints) Y = np.linspace(ymin, ymax, npoints) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") krig = OrdinaryKriging(x=xd, y=yd, z=zd, variogram_model=\"spherical\") zr, ss = krig.execute(\"grid\", X, Y) export_kde_raster(Z = zr, XX = X, YY = Y, min_x = xmin, max_x = xmax, min_y = ymin, max_y = ymax, proj = 4326, filename = \"kriging_result.tif\") kriging = rasterio.open(\"kriging_result.tif\",mode='r') dist = math.sqrt((end_cor[0]-start_cor[0])**2+(end_cor[1]-start_cor[1])**2)*111 npoints=500 lat = np.linspace(start_cor[1], end_cor[1],npoints) lon = np.linspace(start_cor[0], end_cor[0],npoints) distarray = np.linspace(0, dist,npoints) np.append(distarray, dist) df = pd.DataFrame({'Latitude': lat, 'Longtitude':lon,'h_distance':distarray}) df['Observations']=0 gdf_pcs = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longtitude, df.Latitude)) gdf_pcs.crs = {'init':'epsg:4326'} for index, row in gdf_pcs.iterrows(): rows, cols = kriging.index(row['geometry'].x,row['geometry'].y) kri_data = kriging.read(1) df['Observations'].loc[index] = kri_data[rows, cols] profile = df[['h_distance','Observations']] profile.plot(x='h_distance',y='Observations') kriging.close() References Geopandas https://ithelp.ithome.com.tw/articles/10202336 scipy.spatial (https://docs.scipy.org/doc/scipy/reference/spatial.html) scipy.interpolate (https://docs.scipy.org/doc/scipy/reference/interpolate.html) pykrige (https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/api.html#krigging-algorithms) ",
    "description": "We use Civil IoT Taiwan's sensor data to introduce more advanced geospatial analysis. Using the GPS location coordinates in the site information, we first use the package to find the largest convex polygon (Convex Hull) to frame the geographical area covered by the sensor. Then we apply the Voronoi Diagram package to draw the distribution status of each sensor on the map according to the sensor and crops out the area of influence of each sensor. We adopt the spatial interpolation method for the space between sensors, applying different spatial interpolation algorithms. We then populate the spatial map with values based on the sensor values and generate a corresponding image output.",
    "tags": [
      "Python",
      "Water",
      "Air"
    ],
    "title": "5.2. Geospatial Analysis",
    "uri": "/en/ch5/ch5.2/"
  },
  {
    "content": "\nTable Of Contents Anomaly Detection Framework Types of abnormal events Possible causes of abnormal events Case Study Package Installation and Importing Initialization and Data Access Find nearby sensors Time slicing every five minutes Average the sensing values of nearby sensors every time slice Abnormal event judgements Implementation of the Malfunction Detection module Implementation of the Real-time Emission Detection module Implementation of the Device Ranking module References Anomaly Detection Framework In various countries and cities, we’ve seen the successful implementation of expansive networks for monitoring air quality at a micro level. One key issue with these tiny sensors is making sure they provide accurate data and can spot any unusual readings as they happen. Back in 2018, a team from the Network Research Laboratory at Academia Sinica’s Institute of Information Science in Taiwan developed a special system for this purpose, known as the Anomaly Detection Framework (ADF).\nThe ADF is composed of four main parts:\nTime-Sliced Anomaly Detection (TSAD): This part is all about catching odd data from the sensors, either related to time or location, right when it occurs. It then sends this information to other parts of the system for deeper analysis. Real-time Emission Detection (RED): Using the findings from TSAD, RED focuses on identifying possible pollution events happening in a specific area, doing this in real-time. Sensor Reliability Evaluation (Device Ranking, DR): DR takes the data flagged by TSAD to assess how trustworthy each small sensor device is. Abnormal Use of Machine Detection (Malfunction Detection, MD): Similar to DR, MD also uses TSAD’s data but with a different goal. It looks for sensors that might not be used properly. For example, it can find sensors placed indoors or near constant sources of pollution, which could affect their readings. Each of these modules plays a crucial role in ensuring the micro sensors used in air quality monitoring systems work effectively and reliably.\nTypes of abnormal events In the ADF framework, the TSAD module assesses unusual occurrences in time or space whenever the micro air quality sensor gets new data. Let’s use the micro air quality sensor as an example to explain this:\nTime-related abnormal events: We start with the assumption that air spreads out evenly and slowly. Therefore, the readings from the same micro air quality sensor should change only slightly over a short period. If there’s a rapid and significant change in these readings in a short time, it could indicate something abnormal happening related to time.\nSpace-related abnormal events: We generally expect that outdoor air disperses uniformly over a geographical area. This means that the readings from a micro air quality sensor should be roughly similar to those from sensors nearby. If the readings from one specific sensor are drastically different from those of its neighbors at the same time, it might suggest an unusual event happening in the location of that sensor.\nPossible causes of abnormal events There are several reasons why the abnormal events we’ve talked about might happen. Some of the most common ones include:\nPutting the sensor in the wrong place: Sometimes, the sensor is set up in a specific area, which means it can’t accurately measure the overall environment. This could be because it’s placed next to a temple, inside a barbecue shop, or in any indoor area that doesn’t have good air flow. Issues with the sensor itself or how it’s set up: For instance, the sensor might be installed facing the wrong way, which affects how it takes in air. Or, the fan in the sensor could be dirty, which would make it work poorly. Sudden nearby pollution: This could be something like someone smoking close to the sensor, a nearby fire, or other pollutants being released right next to it. Case Study In this article, we’re going to explore air quality data from the Civil IoT Taiwan project. Specifically, we’ll look at data from small air quality sensors placed around campuses in Kaohsiung City. Our focus will be on how to use the ADF detection framework to identify sensors that might be inside buildings or near sources of pollution. Additionally, we’ll discuss how to assess and rank these sensors based on how reliable their data is.\nPackage Installation and Importing In this article, we’re going to work with several tools that are already set up for us on Google Colab, our development platform. These include pandas, numpy, plotly, and geopy packages. Since they are pre-installed, we don’t need to worry about installing them ourselves. To get started, we’ll simply import these packages using the syntax provided below. This will set everything up for the tasks we’ll be tackling in this article.\nimport pandas as pd import numpy as np import plotly.express as px from geopy.distance import geodesic Initialization and Data Access For our analysis, we’re going to use small air quality sensors located in Kaohsiung, which are part of the Civil IoT Taiwan Project. We’ll focus on specific areas and timeframes:\nFor the area, we’ll look at places between latitudes 22.631231 and 22.584989, and longitudes 120.263422 and 120.346764. The time period we’re interested in is from October 15, 2022, to October 28, 2022. You can find the raw data from these air quality sensors on the Civil IoT Taiwan Data Service Platform at this link. This data is typically used for academic purposes, but to make things easier, we’ve already downloaded it and put it into a file named allLoc.csv, which you can use for the examples in this article.\nHere’s what we do with this data:\nWe start by opening the data file to see what’s inside. DF = pd.read_csv(\"https://LearnCIOT.github.io/data/allLoc.csv\") DF.head() Next, we look for the GPS coordinates of each sensor in the file. Since these sensors don’t move, we calculate the average longitude and latitude for each one. This gives us a fixed geographical location for each sensor. dfId = DF[[\"device_id\",\"lon\",\"lat\"]].groupby(\"device_id\").mean().reset_index() print(dfId.head()) device_id lon lat 0 74DA38F207DE 120.340 22.603 1 74DA38F20A10 120.311 22.631 2 74DA38F20B20 120.304 22.626 3 74DA38F20B80 120.350 22.599 4 74DA38F20BB6 120.324 22.600 Finally, we create a map showing where each sensor is located. This helps us see how the sensors are spread out geographically in the dataset. fig_map = px.scatter_mapbox(dfId, lat=\"lat\", lon=\"lon\", color_continuous_scale=px.colors.cyclical.IceFire, zoom=9, mapbox_style=\"carto-positron\") fig_map.show() Find nearby sensors The micro air quality sensors on the campus are permanently installed, so their GPS locations remain the same. To make data analysis faster later on, we first create a list of “neighbors” for each sensor. In our approach, two small sensors are considered neighbors if they are 3 kilometers or less apart from each other.\nWe begin by creating a function called countDis that measures the actual distance in kilometers between any two given GPS coordinates.\ndef countDis(deviceA, deviceB): return geodesic((deviceA[\"lat\"], deviceB['lon']), (deviceB[\"lat\"], deviceB['lon'])).km Next, we change the format of the sensor data from a DataFrame, which is a specific type of data structure, to a Dictionary, another type of data structure. This allows us to calculate the distance between each pair of sensors. If the distance between any two sensors is less than 3 kilometers, we add them to each other’s list of neighboring sensors, which we call dicNeighbor.\n# set the maximum distance of two neighbors DISTENCE = 3 ## convert Dataframe -\u003e Dictionary # {iD: {'lon': 0.0, 'lat': 0.0}, ...} dictId = dfId.set_index(\"device_id\").to_dict(\"index\") ## obtain the list of sensor device_id listId = dfId[\"device_id\"].to_list() ## initialize dicNeighbor # {iD: []} dicNeighbor = {iD: [] for iD in listId} ## use countDis to calculate distance of every two sensors # The two sensors are deem to be neighbors of each other if their distance is less than 3km # Time complexity: N! for x in range(len(listId)): for y in range(x+1, len(listId)): if ( countDis( dictId[listId[x]], dictId[listId[y]]) \u003c DISTENCE ): dicNeighbor[listId[x]].append( listId[y] ) dicNeighbor[listId[y]].append( listId[x] ) Time slicing every five minutes In the original data, each sensor’s readings aren’t aligned in time. To tackle this in the ADF framework, we break down the sensor data into segments based on regular time intervals. This process creates a ’time slice’ that gives us a comprehensive view of the data collected over each time period. We start by combining the date and time fields from the original data into a single field, using Python’s datetime data type, and call this new field datetime. After creating datetime, we remove fields that are no longer needed, such as date, time, RH, temperature, lat, and lon.\n# combine the 'date' and 'time' columns to a new column 'datetime' DF[\"datetime\"] = DF[\"date\"] + \" \" + DF[\"time\"] # remove some non-necessary columns DF.drop(columns=[\"date\",\"time\", \"RH\",\"temperature\",\"lat\",\"lon\"], inplace=True) # convert the new 'datetime' column to datetime data type DF['datetime'] = pd.to_datetime(DF.datetime) Considering the data from campus micro air quality sensors is updated roughly every 5 minutes, we set our time slice interval, named FREQ, to 5 minutes. This means we calculate the average of the sensor readings every 5 minutes. To ensure the data’s accuracy, we’ve also conducted extra checks and eliminated any readings where the PM2.5 levels were incorrectly showing as negative values.\nFREQ = '5min' dfMean = DF.groupby(['device_id', pd.Grouper(key = 'datetime', freq = FREQ)]).agg('mean') # remove invalid records (i.e., PM2.5 \u003c 0) dfMean = dfMean[ dfMean['PM2.5'] \u003e= 0 ] print(dfMean.head()) PM2.5 device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 2022-10-15 00:05:00 37.0 2022-10-15 00:10:00 37.0 2022-10-15 00:20:00 36.0 2022-10-15 00:25:00 36.0 Average the sensing values of nearby sensors every time slice To calculate the average sensing value of the neighboring sensors of a specific sensor on a particular time slice, we write the cal_mean function, which can return the number and average sensing values of the neighboring sensors according to the input device number iD and time stamp dt.\ndef cal_mean(iD, dt): neighborPM25 = dfMean[ (dfMean.index.get_level_values('datetime') == dt) \u0026 (dfMean.index.get_level_values(0).isin(dicNeighbor[iD])) ][\"PM2.5\"] avg = neighborPM25.mean() neighbor_num = neighborPM25.count() return avg, neighbor_num Then, for each timestamp of each sensor in dfMean, we calculate the number of neighboring sensors and the average sensing value and store them in two new fields, avg and neighbor_num, respectively. Note that we use the syntax of zip and apply it to bring the values of DataFrame into the function for operation:\nWe use the zip syntax for packaging the two parameter values returned by apply_func. We use the syntax of apply to match the rules of DataFrame and receive the return value of apply_func. def apply_func(x): return cal_mean(x.name[0], x.name[1]) dfMean['avg'], dfMean['neighbor_num'] = zip(*dfMean.apply(apply_func, axis=1)) print(dfMean.head()) PM2.5 avg neighbor_num device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 2022-10-15 00:05:00 37.0 19.888889 9 2022-10-15 00:10:00 37.0 16.500000 12 2022-10-15 00:20:00 36.0 16.750000 8 2022-10-15 00:25:00 36.0 17.000000 11 Abnormal event judgements We’ve been looking into what counts as an “abnormal event” in our data. Basically, this means when a sensor’s reading is way off what we’d normally expect. What’s considered normal can depend on different things – like readings from nearby sensors, past readings from the same sensor, or even other kinds of information.\nBut deciding what’s “way off” from normal isn’t so straightforward, as it really depends on how big or small the sensor readings usually are.\nTo tackle this, we’ve broken down the average values (noted as dfMean['avg']) into nine different ranges. For each range, we’ve worked out the typical variation (or standard deviation) for PM2.5 sensor readings. This helps us set specific limits to decide if a reading is abnormal.\nFor instance, if a sensor usually reads 10 (in micrograms per cubic meter), we’d expect normal readings from nearby sensors to be within 6.6 above or below this (so between 10-6.6 and 10+6.6). If it’s outside this range, we’d consider it abnormal.\nRaw value (ug/m3) threshold 0-11 6.6 12-23 6.6 24-35 9.35 36-41 13.5 42-47 17.0 48-58 23.0 59-64 27.5 65-70 33.5 71+ 91.5 We’ve written a function called THRESHOLD that uses these limits to check readings. It adds this info to our data in a new column, PM_thr.\ndef THRESHOLD(value): if value\u003c12: return 6.6 elif value\u003c24: return 6.6 elif value\u003c36: return 9.35 elif value\u003c42: return 13.5 elif value\u003c48: return 17.0 elif value\u003c54: return 23.0 elif value\u003c59: return 27.5 elif value\u003c65: return 33.5 elif value\u003c71: return 40.5 else: return 91.5 dfMean['PM_thr'] = dfMean['PM2.5'].apply(THRESHOLD) Since our latest data is from October 28, 2022, we’re using October 29, 2022, as our reference date. We’ve added another column, day, to keep track of how each reading’s date compares to this reference date. Below, you can see the current setup of our data table, dfMEAN.\nTARGET_DATE = \"2022-10-29\" dfMean = dfMean.assign(days = lambda x: ( (pd.to_datetime(TARGET_DATE + \" 23:59:59\") - x.index.get_level_values('datetime')).days ) ) print(dfMean.head()) PM2.5 avg neighbor_num PM_thr days device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 13.5 14 2022-10-15 00:05:00 37.0 19.888889 9 13.5 14 2022-10-15 00:10:00 37.0 16.500000 12 13.5 14 2022-10-15 00:20:00 36.0 16.750000 8 13.5 14 2022-10-15 00:25:00 36.0 17.000000 11 13.5 14 Implementation of the Malfunction Detection module In this example, we set up a system to figure out if a sensor is either indoors or near a pollution source. We do this by comparing the PM2.5 (a type of pollution measurement) readings from a specific sensor with other sensors located within a 3-kilometer radius.\nIf a sensor’s PM2.5 reading is lower than the average of nearby sensors minus a certain acceptable threshold (called PM_thr), we label it as indoor. If it’s higher than the average plus the threshold, we label it as emission, indicating it might be near a pollution source. However, to make sure our conclusions are accurate, we only consider areas where there are more than two other sensors nearby.\nMINIMUM_NEIGHBORS = 2 dfMean[\"indoor\"] = ((dfMean['avg'] - dfMean['PM2.5']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean[\"emission\"] = ((dfMean['PM2.5'] - dfMean['avg']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean We’ve noticed that the labels indoor and emission can change based on daily air quality. So, to get a more reliable result, we look at data over different time periods: 1 day, 7 days, and 14 days. For each sensor, we calculate how often it was labeled indoor or emission during these periods.\nTo improve accuracy and avoid errors caused by unusual environmental conditions, we adjust our calculations. If a sensor is labeled indoor or emission less than one-third of the time in any period, we disregard that label.\n# initialize dictIndoor = {iD: [] for iD in listId} dictEmission = {iD: [] for iD in listId} for iD in listId: dfId = dfMean.loc[iD] for day in [1, 7, 14]: indoor = (dfId[ dfId['days'] \u003c= day]['indoor'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictIndoor[iD].append( indoor if indoor \u003e 0.333 else 0 ) emission = (dfId[ dfId['days'] \u003c= day]['emission'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictEmission[iD].append( emission if emission \u003e 0.333 else 0 ) We keep track of our findings in two records: dictIndoor for indoor sensors and dictEmission for sensors near pollution sources. By examining these records, we can see how the time period affects our labeling.\ndictIndoor {'74DA38F207DE': [0, 0, 0], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0.995, 0.86, 0.816], '74DA38F20B80': [0.995, 0.872, 0.867], '74DA38F20BB6': [0, 0, 0], '74DA38F20C16': [0.989, 0.535, 0], '74DA38F20D7C': [0, 0, 0], '74DA38F20D8A': [0, 0, 0], '74DA38F20DCE': [0, 0, 0], '74DA38F20DD0': [0.984, 0.871, 0.865], '74DA38F20DD8': [0, 0.368, 0.369], '74DA38F20DDC': [0, 0, 0], '74DA38F20DE0': [0, 0, 0], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0, 0, 0], '74DA38F20E42': [0.99, 0.866, 0.87], '74DA38F20E44': [0, 0, 0], '74DA38F20F0C': [0.979, 0.872, 0.876], '74DA38F20F2C': [0, 0, 0], '74DA38F210FE': [0.99, 0.847, 0.864]} dictEmission {'74DA38F207DE': [0.92, 0.737, 0.735], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0, 0, 0], '74DA38F20B80': [0, 0, 0], '74DA38F20BB6': [0.492, 0.339, 0], '74DA38F20C16': [0, 0, 0], '74DA38F20D7C': [0.553, 0.342, 0.337], '74DA38F20D8A': [0.672, 0.457, 0.388], '74DA38F20DCE': [0.786, 0.556, 0.516], '74DA38F20DD0': [0, 0, 0], '74DA38F20DD8': [0, 0, 0], '74DA38F20DDC': [0.345, 0, 0], '74DA38F20DE0': [0.601, 0.503, 0.492], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0.938, 0.75, 0.75], '74DA38F20E42': [0, 0, 0], '74DA38F20E44': [0.938, 0.69, 0.6], '74DA38F20F0C': [0, 0, 0], '74DA38F20F2C': [0.744, 0.575, 0.544], '74DA38F210FE': [0, 0, 0]} Since different time lengths give us different insights, we use a weighting method. We assign different weights to the results from 1-day, 7-day, and 14-day periods, using A for 1 day, B for 7 days, and 1-A-B for 14 days.\nWe also factor in that a sensor operates about 8 hours a day, 40 hours over 7 days, and 80 hours over 14 days. A sensor is definitively labeled as indoor or emission if its weighted score meets or exceeds a certain threshold (MD_thresh). This threshold is calculated based on the operation hours and the weights assigned to different time periods, i.e.,\nMD_thresh = (8.0/24.0)*A+(40.0/168.0)B+(80.0/336.0)(1-A-B)\nFor instance, if we set A as 0.2 and B as 0.3, we can then determine which sensors are consistently indoor or emission based on these weighted calculations.\nA=0.2 B=0.3 MD_thresh=(8.0/24.0)*A+(40.0/168.0)*B+(80.0/336.0)*(1-A-B) listIndoorDevice = [] listEmissionDevice = [] for iD in listId: rate1 = A*dictIndoor[iD][0] + B*dictIndoor[iD][1] + (1-A-B)*dictIndoor[iD][2] if rate1 \u003e MD_thresh: listIndoorDevice.append( (iD, rate1) ) rate2 = A*dictEmission[iD][0] + B*dictEmission[iD][1] + (1-A-B)*dictEmission[iD][2] if rate2 \u003e MD_thresh: listEmissionDevice.append( (iD, rate2) ) Finally, we review the results in listIndoorDevice and listEmissionDevice to see our weighted judgment outcomes.\nlistIndoorDevice [('74DA38F20B20', 0.865), ('74DA38F20B80', 0.8941), ('74DA38F20C16', 0.3583), ('74DA38F20DD0', 0.8906), ('74DA38F20DD8', 0.2949), ('74DA38F20E42', 0.8928), ('74DA38F20F0C', 0.8954), ('74DA38F210FE', 0.8841)] listEmissionDevice [('74DA38F207DE', 0.7726), ('74DA38F20D7C', 0.38170000000000004), ('74DA38F20D8A', 0.4655), ('74DA38F20DCE', 0.5820000000000001), ('74DA38F20DE0', 0.5171), ('74DA38F20E0E', 0.7876), ('74DA38F20E44', 0.6945999999999999), ('74DA38F20F2C', 0.5933)] Implementation of the Real-time Emission Detection module To detect air pollution as it happens, we use a method where we focus on the readings from small sensors. If the most recent measurement from a sensor is at least 20% higher than the one before it, we consider this a sign that there might be a significant increase in air pollution nearby. This approach is particularly effective for tracking changes in PM2.5 levels, a common air pollutant. However, we only apply this method when the sensor’s readings are above 20. We do this because when the readings are very low, even a 20% increase might not indicate actual pollution – it could just be a normal fluctuation or a minor error in the sensor’s measurement. This way, we avoid mistakenly identifying a situation as pollution when it isn’t.\ndfMean['red'] = False for iD in listId: dfId = dfMean.loc[iD] p_index = '' p_row = [] for index, row in dfId.iterrows(): red = False if p_index: diff = row['PM2.5'] - p_row['PM2.5'] if p_row['PM2.5']\u003e20 and diff\u003ep_row['PM2.5']/5: red = True dfMean.loc[pd.IndexSlice[iD, index.strftime('%Y-%m-%d %H:%M:%S')], pd.IndexSlice['red']] = red p_index = index p_row = row dfMean Implementation of the Device Ranking module To better understand the effectiveness of microsensors, we evaluate their reliability through a process we call Device Ranking (DR). This involves two key ideas:\nIf a sensor frequently reports data that seems unusual, either in terms of time or location, this could signal potential issues with the sensor’s hardware or its environment. Such cases warrant further investigation. Conversely, if a sensor rarely shows abnormal data, it suggests that its readings are in good agreement with those of nearby sensors, indicating high reliability. To assess this, we look at each sensor’s daily data and count how many times it records anomalies in time (marked as red=True) or space (marked as indoor=True or emission=True). We then compare these counts to the total data entries for that day. This comparison forms the basis for evaluating the sensor’s reliability.\ndevice_rank = pd.DataFrame() for iD in listId: dfId = dfMean.loc[iD] abnormal = {} num = {} for index, row in dfId.iterrows(): d = index.strftime('%Y-%m-%d') if d not in abnormal: abnormal[d] = 0 if d not in num: num[d] = 0 num[d] = num[d] + 1 if row['indoor'] or row['emission'] or row['red']: abnormal[d] = abnormal[d] + 1 for d in num: device_rank = device_rank.append(pd.DataFrame({'device_id': [iD], 'date': [d], 'rank': [1 - abnormal[d]/num[d]]})) device_rank.set_index(['device_id','date']) References Ling-Jyh Chen, Yao-Hua Ho, Hsin-Hung Hsieh, Shih-Ting Huang, Hu-Cheng Lee, and Sachit Mahajan. ADF: an Anomaly Detection Framework for Large-scale PM2.5 Sensing Systems. IEEE Internet of Things Journal, volume 5, issue 2, pp. 559-570, April, 2018. (https://dx.doi.org/10.1109/JIOT.2017.2766085) ",
    "description": "We use air quality data to demonstrate the anomaly detection framework commonly used in Taiwan's micro air quality sensing data. We learn by doing, step by step, from data preparation and feature extraction to data analysis, statistics, and induction. The readers will experience how to gradually achieve advanced and practical data application services by superimposing basic data analysis methods.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "6.2. Anomaly Detection",
    "uri": "/en/ch6/ch6.2/"
  },
  {
    "content": " Table Of Contents Goal Data Source Tableau Basic Operation Data Import Worksheet Introduction Tableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Time Series Graph Tableau Example 2: Disaster Notification Dashboard Disaster data format conversion Dashboard size Add worksheets to a dashboard Add interactions Interacting multiple worksheet Additional information Story Conclusion References Data visualization transforms data into graphical formats, making it easier to understand and interpret. Different graph types are better suited for various data sets. For instance, map-type charts are often used for data with latitude and longitude coordinates, while line charts or histograms are ideal for time series data. However, many real-world data sets have multiple characteristics. Take the sensor data from Civil IoT Taiwan, for example; it includes both geographical coordinates and time-series elements. Hence, to effectively convey the insights of such data, we might need to use multiple types of charts. In this chapter, we introduce Tableau, a widely-used software, to help us effectively present these complex types of data.\nTableau is a user-friendly platform for visual analysis. It enables quick creation of diverse charts, and its dashboard feature simplifies the presentation and integration of different data visualizations. Dashboards in Tableau also offer interactive elements with graphs, enhancing both the ease and speed of understanding data, as well as simplifying data analysis. Originally a paid commercial software, Tableau also offers Tableau Public, a free version accessible to everyone. It’s important to note, however, that any work created using the free version of Tableau must be publicly available.\nNote Please note: The version of Tableau Public used in this article is Tableau Desktop Public Edition (2022.2.2 (20222.22.0916.1526) 64-bit). However, we only utilize the basic functions of the software, which are generally consistent across different versions. Therefore, if you are using another version of the software, you should still be able to follow along and execute the processes described here without issues.\nGoal Load data into Tableau for visual representation. Use Worksheets in Tableau to design various data charts. Develop interactive visuals and reports by employing dashboards and stories within Tableau Public. Data Source Civil IoT Taiwan Historical Data: Academia Sinica - Micro Air Quality Sensors (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器) Civil IoT Taiwan Historical Data: National Fire Agency - Disaster Notifications (https://history.colife.org.tw/#/?cd=%2F災害示警與災情通報%2F消防署_災情通報) Tableau Basic Operation Data Import Tableau is capable of reading text files (like csv) and spatial information files (such as shp and geojson). To import data, you need to go to Data \u003e Add Data Source and then select the data file you wish to import.\nDepending on the type of file you’re working with, we have different examples to guide you:\nFor Geospatial Data Formats (shp, geojson):\nFirst, click on the data source in the lower left corner to view the currently imported data fields. ![Tableau screenshot](figures/7-2-3-2.png) Next, click the icon above the field name to modify the properties of that field.\nLastly, assign latitude and longitude coordinates to geographic roles to prepare for further operations.\nFor Text Data Formats (CSV):\nAfter importing the PM 2.5 records and station locations into Tableau, establish a relationship between these data tables.\nTo link the PM 2.5 station data with station coordinates, first double-click on the station data to enter the connection canvas. Then, drag another data table onto this canvas.\nNow, click the link icon in the middle to set the type of link and define how they interact.\nOnce set up, you’ll notice that the Station id in the lower right corner is sourced from the Location data table, linking the PM 2.5 data with the station location data.\nWorksheet Introduction A worksheet in Tableau is the space where you can create visual representations of your data. It’s not limited to traditional graph types like pie, bar, and line graphs; it can also be used to plot geographic information. We’ll guide you through how to create various visualizations.\nOnce you’ve prepared your data, click on ‘New Worksheet’ in the lower left corner. Upon entering the worksheet, you’ll be presented with the interface we’re about to discuss.\nIn the upper right corner of the worksheet in Tableau, there’s a “Show Me” button. When you click on it, a variety of chart types will be displayed. The system automatically identifies which types of charts can be created based on your data. Those that aren’t suitable for your data will be grayed out or highlighted. You can then click on the chart type you want to use to change the visualization accordingly.\nTableau Example 1: Spatio-temporal Distribution of Air Quality Data Spatial Distribution Graph Before you begin creating your visualization, it’s important to change the latitude and longitude fields from being a ‘measure’ to a ‘dimension’. Understanding the difference between a measure and a dimension is crucial in Tableau. For a more detailed explanation of these concepts, please refer to the reference materials. The specific steps to change these fields in Tableau are demonstrated in the following animation:\nNext, you’ll need to drag the latitude and longitude fields to the respective column and row areas in Tableau. After that, drag the PM 2.5 value to the indicated area for it to be included in the visualization. Then, change the mark type to ‘Color’ to create a distribution map showing PM 2.5 levels across Taiwan. This process is demonstrated in the animation below.\nYou have the option to click on the color section to alter the color of the dots on your map. Additionally, the filter tool can be used to narrow down the PM 2.5 values based on specific conditions. For instance, by selecting PM 2.5 \u003e Metric, you can choose to display the mean, sum, or median values of PM 2.5. The following steps will guide you on how to specifically display the PM 2.5 values for New Taipei City.\nDrag “Site Name” to filter Click on wildcards, click “Include” and enter New Taipei City Click “OK” Time Series Graph To create a time-based visualization, start by dragging “Timestamp” to the column area and both “Site Name” and the PM 2.5 value to the row area. Next, click on “Timestamp” to adjust the time interval. For instance, in the example below, the interval is set to 1 hour. Similar to plotting spatial distribution, you can also use the filter function to select specific stations for display. In the animation provided below, we illustrate how to showcase the daily monitoring values of the PM2.5 station in Kaohsiung.\nTableau Example 2: Disaster Notification Dashboard Dashboards in Tableau offer the ability to amalgamate various worksheets (charts) into a single view, presenting a more comprehensive set of information and enabling users to interact with the data. In the upcoming example, we’ll guide you through creating a simple dashboard utilizing rainstorm disaster data from the Civil IoT Taiwan project. This will demonstrate how you can effectively use dashboards to display and interact with complex data sets.\nDisaster data format conversion For our example, we’ll focus on the flood event that happened on August 23, 2018, known as the “823 flood.” The original data for this event is in XML format. To use this data in Tableau, we first need to convert it into a CSV file. You can do this conversion using an online tool available at https://www.convertcsv.com/xml-to-csv.htm. The process of converting the XML file to a CSV format using this website is demonstrated in the animation below.\nDashboard size Once you click on ‘Dashboard’ in the Tableau menu, you’ll have the option to set the size of your dashboard. This can be done using the tool list located on the left-hand side. This feature allows you to customize the layout and size of your dashboard to best fit the data visualizations you plan to include.\nAdd worksheets to a dashboard Next, you’ll see the worksheet(s) you created earlier listed in the tool list on the left side of the dashboard interface in Tableau. To add any of these worksheets to your dashboard, simply drag and drop them into the empty space of the dashboard. This intuitive feature allows you to easily organize and display the various data visualizations you’ve created in a cohesive and interactive dashboard layout.\nOnce you’ve added a worksheet to your Tableau dashboard, the software automatically processes the data from that worksheet. It reads the information contained within and automatically generates an initial chart based on that data. This feature simplifies the initial steps of data visualization, providing you with a starting point that you can then customize and refine according to your needs and preferences.\nThe worksheet you’ve just dragged into your Tableau dashboard comes with a preset size. If you want to adjust this size, look for a downward arrow at the top right corner of the worksheet within the dashboard. By clicking on this arrow and selecting the “Float” option, you gain the flexibility to resize the graph as needed. This feature allows you to customize the layout of your dashboard more dynamically, ensuring that each visual element fits and complements the overall design.\nAdd interactions Next, we’ll show you how to create an interactive interface that lets users choose a specific point in time from the disaster data for observation.\nBegin by clicking the downward arrow located in the upper right corner of the sheet. From the dropdown menu, select ‘Filters,’ and then choose ‘Case Time.’\nOnce you click, a selection field will appear in the worksheet. From there, you can choose the date you want to observe. After selecting the date, you’ll be able to view the disaster situation for that specific day.\nTo modify the date selection method, you can also click the downward arrow at the top right corner of the selection field. This will allow you to adjust the appearance of the selection field.\nAs an illustration, we can switch the selection method from the initial list format to the slider format in the following manner.\nInteracting multiple worksheet The interaction buttons mentioned above are applicable to a single worksheet only. If you wish to have multiple worksheets on the dashboard share the same interactive button simultaneously, consider the following steps:\nStart by creating an interactive button field using the method described above.\nNext, click the downward arrow at the top right corner of the interactive button field. From the dropdown menu, choose ‘Apply to worksheet’ and select the desired worksheet.\nFinally, apply the settings to the selected worksheet.\nAdditional information If you’d like to include additional information on the dashboard, look for the object column located at the bottom left of the toolbar. This column houses various elements such as text, images, and other objects. Simply drag and drop the necessary items to the top of the dashboard to display them there. You can then add text, pictures, and any other relevant objects.\nStory A story in this context is a collection of multiple dashboards or sheets that come together to create a slideshow-like presentation. Adding a story to a dashboard or sheet is quite similar to adding a sheet from a dashboard. You simply need to drag the sheet or dashboard you’ve created on the right and place it into the empty space designated for the story.\nIf you want to add a new page to your story, just click on ‘New Story’ at the top. This will create a fresh blank page. Alternatively, you can duplicate an existing page to create a new one.\nLastly, if you want the text information to better align with its content, you can modify the title of the page by double-clicking the box above the text area.\nConclusion In this chapter, we’ll provide a brief overview of the fundamental operations in Tableau. We’ll explore how to create interactive presentations and charts that engage users. However, it’s essential to note that Tableau offers a wealth of functionalities beyond what we cover here. Many classic examples showcasing Tableau’s capabilities can be found online. If you’re interested in diving deeper, consider exploring the additional resources listed below.\nReferences Tableau Public (https://public.tableau.com/) Tableau - About data field roles and types (https://help.tableau.com/current/pro/desktop/en-us/datafields_typesandroles.htm) Get Started with Tableau (https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-home.htm) Tableau Tutorial — Learn Data Visualization Using Tableau (https://medium.com/edureka/tableau-tutorial-71ef4c122e55) YouTube: Tableau For Data Science 2022 (https://www.youtube.com/watch?v=Wh4sCCZjOwo) YouTube: Tableau Online Training (https://www.youtube.com/watch?v=ttCDqyfrcEc) ",
    "description": "We introduce the use of Tableau tools to render Civil IoT Taiwan data and conduct two example cases using air quality data and disaster notification data. We demonstrate how worksheets, dashboards, and stories can be used to create interactive data visualizations for users to explore data. We also provide a wealth of reference materials for people to further study reference.",
    "tags": [
      "Air",
      "Disaster"
    ],
    "title": "7.2. Tableau Application",
    "uri": "/en/ch7/ch7.2/"
  },
  {
    "content": "\nTable Of Contents Goal Package Installation and Importing Data Access and Preprocessing Data Clustering Fast Fourier Transform Wavelet Transform References Cluster analysis is a commonly used data processing method in data science, and its primary purpose is to find similar clusters in the data. After cluster analysis, data with similar attributes are clustered together for ease of use, and researchers can perform deeper analysis and processing on data with similar characteristics. In Civil IoT Taiwan, since the data of each sensor is time series data, to properly group a large number of sensors for more in-depth data analysis, we introduce standard feature extraction methods and for clustering time series data.\nGoal Learn to use Fast Fourier Transform (FFT) and Wavelet Transform to extract features of time series data Use unsupervised learning method to cluster time series data Package Installation and Importing In this article, we will use the pandas, matplotlib, numpy, and pywt packages, which are pre-installed on our development platform, Google Colab, and do not need to be installed manually. However, we will also use one additional package that Colab does not have pre-installed, tslearn, which need to be installed by :\n!pip install --upgrade pip !pip install tslearn After the installation is complete, we can use the following syntax to import the relevant packages to complete the preparations in this article.\nimport numpy as np import pandas as pd import pywt import os, zipfile import matplotlib.pyplot as plt from datetime import datetime, timedelta from numpy.fft import fft, ifft from pywt import cwt from tslearn.clustering import TimeSeriesKMeans Data Access and Preprocessing Since we want to use long-term historical data in this article, we do not directly use the data access methods of the pyCIOT package, but directly download the 2021 data archive of “Academia Sinica - Micro Air Quality Sensors” in 2021 from the historical database of the Civil IoT Taiwan Data Service Platform and store in the Air folder.\n!mkdir Air CSV_Air !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" !unzip Air/2021.zip -d Air At the same time, since the downloaded data is in the format of a zip compressed file, we need to decompress it to generate a number of compressed daily file, and then decompress the compressed daily files in 2021/08 and store the content of the decompressed csv files in the air_month dataframe.\nfolder = 'Air/2021/202108' extension_zip = 'zip' extension_csv = 'csv' for item in os.listdir(folder): if item.endswith(extension_zip): file_name = f'{folder}/{item}' zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(folder) zip_ref.close() air_month = pd.DataFrame() for item in os.listdir(folder): if item.endswith(extension_csv): file_name = f'{folder}/{item}' df = pd.read_csv(file_name, parse_dates=['timestamp']) air_month = air_month.append(df) air_month.set_index('timestamp', inplace=True) air_month.sort_values(by='timestamp', inplace=True) At present, the format of air_month does not meet our needs. It needs to be sorted into a data format with site data as columns and time data as columns. Thus, we first find out the number of distinct sites in the data, and store the site information in a sequence.\nid_list = air_month['device_id'].to_numpy() id_uniques = np.unique(id_list) id_uniques array(['08BEAC07D3E2', '08BEAC09FF12', '08BEAC09FF22', ..., '74DA38F7C648', '74DA38F7C64A', '74DA38F7C64C'], dtype=object) Then we save the data of each site as a column and put them all in the air dataframe. Finally, we delete all downloaded data and unpacked data to save cloud storage space.\nair = pd.DataFrame() for i in range(len(id_uniques)): # print('device_id==\"' + id_uniques[i] + '\"') query = air_month.query('device_id==\"' + id_uniques[i] + '\"') query.sort_values(by='timestamp', inplace=True) query_mean = query.resample('H').mean() query_mean.rename(columns={'PM25': id_uniques[i]}, inplace=True) air = pd.concat([air, query_mean], axis=1) !rm -rf Air We can quickly view the contents of air using the following syntax.\nair.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 745 entries, 2021-08-01 00:00:00 to 2021-09-01 00:00:00 Freq: H Columns: 1142 entries, 08BEAC07D3E2 to 74DA38F7C64C dtypes: float64(1142) memory usage: 6.5 MB 08BEAC07D3E2 08BEAC09FF12 08BEAC09FF22 08BEAC09FF2A \\ timestamp 2021-08-01 00:00:00 2.272727 1.250000 16.818182 12.100000 2021-08-01 01:00:00 1.909091 1.285714 13.181818 14.545455 2021-08-01 02:00:00 2.000000 1.125000 12.727273 16.600000 2021-08-01 03:00:00 2.083333 3.000000 11.800000 12.090909 2021-08-01 04:00:00 2.000000 2.600000 10.090909 8.545455 08BEAC09FF34 08BEAC09FF38 08BEAC09FF42 08BEAC09FF44 \\ timestamp 2021-08-01 00:00:00 2.727273 0.000000 1.181818 NaN 2021-08-01 01:00:00 2.000000 0.545455 0.909091 NaN 2021-08-01 02:00:00 4.090909 1.583333 0.636364 NaN 2021-08-01 03:00:00 0.545455 1.454545 1.181818 NaN 2021-08-01 04:00:00 1.363636 1.363636 2.454545 NaN 08BEAC09FF46 08BEAC09FF48 ... 74DA38F7C62A \\ timestamp ... 2021-08-01 00:00:00 2.636364 2.545455 ... 6.777778 2021-08-01 01:00:00 1.636364 2.272727 ... 7.800000 2021-08-01 02:00:00 1.400000 3.100000 ... 7.300000 2021-08-01 03:00:00 2.181818 4.000000 ... 12.000000 2021-08-01 04:00:00 1.909091 2.100000 ... 9.000000 74DA38F7C630 74DA38F7C632 74DA38F7C634 74DA38F7C63C \\ timestamp 2021-08-01 00:00:00 9.800000 13.200000 5.0 6.200000 2021-08-01 01:00:00 13.000000 15.700000 5.2 6.800000 2021-08-01 02:00:00 12.800000 19.300000 5.0 7.300000 2021-08-01 03:00:00 8.444444 15.200000 5.1 6.777778 2021-08-01 04:00:00 6.500000 10.222222 4.9 6.100000 74DA38F7C63E 74DA38F7C640 74DA38F7C648 74DA38F7C64A \\ timestamp 2021-08-01 00:00:00 NaN 7.500000 5.000000 9.000000 2021-08-01 01:00:00 NaN 10.200000 4.900000 6.600000 2021-08-01 02:00:00 NaN 10.500000 3.666667 7.600000 2021-08-01 03:00:00 NaN 8.500000 8.400000 8.000000 2021-08-01 04:00:00 NaN 5.571429 6.200000 5.666667 74DA38F7C64C timestamp 2021-08-01 00:00:00 7.600000 2021-08-01 01:00:00 7.700000 2021-08-01 02:00:00 7.888889 2021-08-01 03:00:00 6.400000 2021-08-01 04:00:00 5.000000 Next, we delete the part of the data that has missing values (the value is Nan) and draw the data into a graph to observe the data distribution.\nair = air[:-1] air_clean = air.dropna(1, how='any') air_clean.plot(figsize=(20, 15), legend=None) Since the instantaneous value of the sensor in the original data is prone to sudden and dramatic changes due to environmental changes, we use a moving average method to average the sensing values every ten times so that the processed data can be smoother and more stable. It can also better reflect the overall trend around the sensor and facilitate the following cluster analysis.\nair_clean = air_clean.rolling(window=10, min_periods=1).mean() air_clean.plot(figsize=(20, 15), legend=None) Data Clustering Since the current data representation in the dataframe is to put each station’s data in a separate column, and each row stores the sensor values of all stations at a specific time point, this format is more suitable for time-series data processing and analysis. Still, it is not ideal for data grouping. Therefore, we need to exchange the columns and columns of the data first; that is, we need to transpose the existing data before proceeding to the data grouping.\nData clustering is a prevalent method in data science. Among many data clustering methods, we choose the KMeans method (TimeSeriesKMeans) provided by the tslearn package. In machine learning, this type of method is classified as an “unsupervised learning” method because, in the process of clustering, there is no specific standard to organize the data into a particular cluster, but only the similarity between the data is used to determine the clustering. Thus, it is beneficial for finding outliers or performing predictions.\nThe operation of the KMeans clustering method is roughly divided into the following steps:\nDetermine the value of k, that is, the final number of clusters; Randomly select k records as the center points of the initial k clusters (also called “cluster heads”); Calculate the distance from each record to each cluster head according to the distance formula, and select the nearest cluster head to attribute the record to this group; Recalculate its new cluster head for each cluster, and repeat the above steps until the cluster heads of k clusters no longer change. We first set k=10, and use TimeSeriesKMeans to divide the data into 10 clusters (0~9) as follows:\nair_transpose = air_clean.transpose() model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) # n_cluster:分群數量, max_iter: 分群的步驟最多重複幾次 pre = model.fit(air_transpose) pre.labels_ array([3, 3, 6, 3, 3, 3, 3, 6, 9, 6, 3, 6, 3, 3, 3, 3, 3, 1, 3, 3, 6, 3, 3, 3, 3, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 9, 3, 3, 3, 3, 3, 6, 3, 2, 6, 3, 2, 3, 3, 3, 6, 9, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 6, 3, 3, 3, 3, 3, 6, 3, 6, 3, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 6, 9, 6, 7, 3, 2, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 6, 6, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 6, 3, 3, 3, 0, 6, 0, 6, 3, 0, 3, 3, 3, 6, 6, 6, 6, 3, 3, 6, 3, 3, 9, 6, 3, 6, 3, 6, 6, 3, 2, 3, 3, 3, 6, 9, 6, 6, 3, 3, 6, 3, 0, 3, 3, 6, 6, 6, 3, 0, 0, 6, 3, 6, 6, 6, 3, 6, 6, 3, 0, 3, 3, 0, 3, 3, 6, 3, 6, 6, 6, 3, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 9, 9, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 9, 0, 9, 0, 0, 0, 9, 9, 0, 9, 0, 0, 0, 5, 0, 0, 0, 0, 9, 9, 0, 1, 0, 9, 2, 9, 9, 0, 0, 5, 0, 9, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 2, 3, 3, 0, 3, 0, 0, 0, 6, 3, 0, 8, 9, 3, 9, 9, 0, 5, 0, 3, 0, 9, 9, 5, 5, 5, 3, 5, 9, 3, 0, 0, 3, 0, 0, 0, 3, 5, 5, 3, 0, 5, 6, 5, 5, 5, 0, 9, 0, 6, 6, 6, 3, 0, 3, 5, 5, 9, 6, 3, 0, 0, 0, 0, 3, 5, 0, 5, 5, 0, 3, 3, 9, 5, 5, 9, 5, 9, 9, 9, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5]) The result of clustering is that a small number of sensors may form a cluster by themselves due to the particularity of the data. These sensors can be regarded as deviated stations with no value for further analysis. Therefore, our next step is first to count whether each separated cluster has only one internal sensor and discard the cluster. For example, in this example, we will discard clusters with only one sensor in them, as follows:\n# build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_clean.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() clusters_dropped [7, 4, 8] Finally, we display the remaining clusters graphically, and it can be found that the data in each cluster show a very high degree of similarity. In contrast, the data in different clusters show apparent differences. To quantify data similarity in clusters, we define a new variable quality. The value of this variable is equal to the average value of the correlation between each internal data and other data, and the lower the quality value, the more similar the internal data of this cluster.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_clean[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_clean[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() So far, we have demonstrated how to use air quality sensor data to cluster the data based on the characteristic information in the data. However, since the original data may contain noise interference, it will also affect the clustering results. To reduce noise interference in raw data, we introduce two commonly used advanced methods: Fourier transform, and wavelet transform. These two methods can extract advanced time series data features, so they can more effectively improve the effectiveness of clustering.\nFast Fourier Transform Fourier transform is a commonly used signal analysis method, which can convert the original data from the time domain to the frequency domain, which is convenient for further feature extraction and data analysis. It is commonly used in engineering and mathematics and in studying sound and time series data. Since the standard Fourier transform involves complicated mathematical operations, it is complex and time-consuming to implement. Therefore, a fast Fourier transform method to discretize the original data was developed later, which can significantly reduce the computational complexity of the Fourier transform and is especially suitable for large amounts of data. Therefore, in our next topic, we will use the Fast Fourier Transform method to extract the data features required for data grouping.\nWe first observe the temporal variation of a single station (’’08BEAC07D3E2’’) by plotting.\nair_clean['08BEAC07D3E2'].plot() Then we use the fast Fourier transform tool fft in the NumPy package to convert the station’s data input and observe the data distribution after conversion to the frequency domain by drawing.\nX = fft(air_clean['08BEAC07D3E2']) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] plt.figure(figsize = (12, 6)) plt.plot(f_oneside, np.abs(X[:n_oneside]), 'b') plt.xlabel('Freq (Hz)') plt.ylabel('FFT Amplitude |X(freq)|') plt.show() Then we extend the same transform step to all sensors and store the fast Fourier transform results for all sensors in the air_fft variable.\nair_fft = pd.DataFrame() col_names = list(air_clean.columns) for name in col_names: X = fft(air_clean[name]) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] air_fft[name] = np.abs(X[:n_oneside]) print(air_fft.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 12019.941563 11255.762431 13241.408211 12111.740447 11798.584351 1 3275.369377 2640.372699 1501.672853 3096.493565 3103.006928 2 1109.613670 1201.362571 257.659947 1085.384353 1128.373457 3 2415.899146 1631.128345 888.838822 2281.597031 2301.936400 4 1130.973327 446.032078 411.940722 1206.512460 1042.054041 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 12441.845754 11874.390693 12259.096742 3553.255916 13531.649701 ... 1 3262.357287 2999.042917 1459.720167 1109.764942 646.846038 ... 2 1075.877632 1005.445596 478.569869 368.572815 1163.425916 ... 3 2448.646527 2318.870954 956.029693 272.486798 553.409732 ... 4 1087.461354 1172.755489 437.920193 471.824176 703.557830 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 11502.841221 10589.689400 11220.068048 10120.220198 11117.146124 1 2064.762890 1407.105290 1938.647888 1126.088084 2422.787262 2 2163.528535 1669.014077 2054.586664 1759.326882 1782.523300 3 1564.407983 1157.759192 1253.849261 1244.799149 1519.477057 4 1484.232397 1177.909914 1318.704021 1106.349846 1373.167639 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 11243.094213 26.858333 9408.414826 11228.246949 8931.871618 1 2097.343959 15.020106 1667.485473 1687.251179 1395.239491 2 1806.524987 10.659603 1585.987276 1851.628868 1527.925427 3 1521.392873 6.021244 1217.547879 1240.173667 1022.239794 4 1393.469185 3.361938 1161.975844 1350.756178 1051.434944 [5 rows x 398 columns] Then we use the same method to divide the sensor data transformed into the frequency domain into 10 clusters using TimeSeriesKMeans, and delete the clusters with only a single sensor after clustering. In this example, there will be 9 clusters left, and we print the cluster code to which each sensor belongs.\nfft_transpose = air_fft.transpose() model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) pre = model.fit(fft_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_fft.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 0 3 08BEAC09FF42 7 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 0 8 08BEAC09FF8C 2 9 08BEAC09FF9C 8 Finally, we plot the sensor data for the nine clusters individually. It can be found that the data of sensors in each cluster are very similar in the frequency domain, and the frequency domain difference of sensors in different clusters is also more significant than that of sensors in the same cluster.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) for idx, cluster_number in enumerate(clusters_final): x_corr = air_fft[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_fft[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() Wavelet Transform Besides Fourier transform, wavelet transform is another commonly used method to convert raw data from the time domain to the frequency domain. The wavelet transform can provide more observation angles for frequency domain data than the Fourier transform. Therefore, recently, wavelet transform has been widely used in the analysis of video, audio, time series, and other related data and has achieved good results.\nWe use the pywt package for wavelet transform data processing. Since the wavelet transform will use the mother wavelet to extract the features in the time series data, we first use the following syntax to check the name of the mother wavelet that can be used.\nwavlist = pywt.wavelist(kind=\"continuous\") print(wavlist) ['cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'cmor', 'fbsp', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'shan'] Unlike the Fourier transform, which can only see changes in frequency, the wavelet transform can scale a finite mother wavelet to extract features from a data segment. When choosing the mother wavelet, you can draw it and observe it before deciding. If we take the morl mother wavelet as an example, we can use the following syntax to construct the graph.\nwav = pywt.ContinuousWavelet(\"morl\") scale = 1 int_psi, x = pywt.integrate_wavelet(wav, precision=10) int_psi /= np.abs(int_psi).max() wav_filter = int_psi[::-1] nt = len(wav_filter) t = np.linspace(-nt // 2, nt // 2, nt) plt.plot(t, wav_filter.real) plt.ylim([-1, 1]) plt.xlabel(\"time (samples)\") We first set the parameters of the wavelet transform and the mother wavelet as morl. For the selected sensor, we set the zoom range of the morl wavelet to 1~31 times and carried out wavelet transformation and plotting.\nF = 1 #Samples per hour hours = 744 nos = np.int(F*hours) #No of samples in 31 days x = air_clean['08BEAC09FF2A'] scales = np.arange(1, 31, 1) coef, freqs = cwt(x, scales, 'morl') # scalogram plt.figure(figsize=(15, 10)) plt.imshow(abs(coef), extent=[0, 744, 30, 1], interpolation='bilinear', cmap='viridis', aspect='auto', vmax=abs(coef).max(), vmin=abs(coef).min()) plt.gca().invert_yaxis() plt.yticks(np.arange(1, 31, 1)) plt.xticks(np.arange(0, nos/F, nos/(20*F))) plt.ylabel(\"scales\") plt.xlabel(\"hour\") plt.colorbar() plt.show() In general, we can find that the larger the scale in the picture, the more yellow-green the color, indicating that the extracted features are closer to the mother wavelet and vice versa. The characteristics of these comparison results will be used for subsequent data grouping.\nHowever, after wavelet transform, each site’s data are transformed into two-dimensional feature values. Before starting to group the data, we need to splice the fields of the original two-dimensional data into one dimension and store it in the air_cwt variable.\nair_cwt = pd.DataFrame() scales = np.arange(28, 31, 2) col_names = list(air_clean.columns) for name in col_names: coef, freqs = cwt(air_clean[name], scales, 'morl') air_cwt[name] = np.abs(coef.reshape(coef.shape[0]*coef.shape[1])) print(air_cwt.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 0.778745 6.336664 2.137342 1.849035 0.778745 1 0.929951 8.348031 2.977576 1.829540 0.958915 2 1.190476 11.476048 3.223773 1.832544 1.292037 3 1.227021 12.890554 4.488244 1.634717 1.338655 4 1.252126 14.103178 5.715656 1.430744 1.376562 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 0.555941 0.334225 8.110581 2.287378 0.409913 ... 1 0.596532 0.201897 8.774271 1.706411 0.311242 ... 2 0.771641 0.294242 8.327808 1.018296 2.754100 ... 3 0.713931 0.065011 8.669036 0.249029 3.278048 ... 4 0.670639 0.193083 8.795376 0.624988 3.628597 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 1.875873 1.090635 0.284901 1.111999 1.049112 1 0.643478 1.446061 0.420701 0.974641 0.478931 2 1.101801 2.404254 1.389941 1.190141 0.511454 3 2.370436 2.510728 1.927692 0.703424 1.054164 4 3.563873 2.387982 2.463458 0.098485 1.448078 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 1.107551 0.000918 0.080841 0.859855 0.415574 1 0.245802 0.005389 1.053260 1.762642 0.190458 2 1.162331 0.009754 2.667145 3.210326 0.516525 3 1.993138 0.014496 3.669712 3.889207 0.666492 4 2.672666 0.020433 4.482730 4.311888 0.677992 [5 rows x 398 columns] Due to the process of converting two-dimensional data into one-dimensional data, the number of characteristic values in each data will increase, significantly increasing the complexity of subsequent data calculations. Therefore, we only take the top 100 characteristic values to represent each sensor and apply the KMeans method for data grouping.\nSince wavelet transform can obtain more subtle data features, we preset 20 clusters in the data clustering process (readers can test different cluster number settings by themselves and observe what will happen to the results). In addition, we will also check whether there are small clusters with only a single sensor in the grouping results and remove them to prevent a few sensors with special conditions from affecting the overall data grouping results.\nair_cwt_less = air_cwt.iloc[:, 0:101] cwt_transpose = air_cwt_less.transpose() model = TimeSeriesKMeans(n_clusters=20, metric=\"dtw\", max_iter=10, verbose=1, n_jobs=-1) pre = model.fit(cwt_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_cwt.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 6 3 08BEAC09FF42 10 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 6 8 08BEAC09FF8C 15 9 08BEAC09FF9C 19 In our example, after the above procedure, there are 14 clusters left at the end. We draw the sensor raw data in each cluster one by one, and we can find that the consistency of the sensor data in the same cluster is more obvious. At the same time , the differences between the different clusters are more detailed and clear than when using the raw data or the Fourier transform before.\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_cwt[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_cwt[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() References Civil IoT Taiwan: Historical Data (https://history.colife.org.tw/) Time Series Clustering — Deriving Trends and Archetypes from Sequential Data | by Denyse | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) How to Apply K-means Clustering to Time Series Data | by Alexandra Amidon | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) Understanding K-means Clustering: Hands-On with SciKit-Learn | by Carla Martins | May, 2022 | Towards AI (https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81) Fast Fourier Transform. How to implement the Fast Fourier… | by Cory Maklin | Towards Data Science (https://towardsdatascience.com/fast-fourier-transform-937926e591cb) PyWavelets/pywt: PyWavelets - Wavelet Transforms in Python (https://github.com/PyWavelets/pywt) ",
    "description": "We introduce advanced data grouping analysis. We first present two time-series feature extraction methods, Fourier transform and wavelet transform, and briefly explain the similarities and differences between the two transform methods. We introduce two different time series comparison methods, Euclidean distance and dynamic time warping (DTW), and apply existing clustering algorithms accordingly. We then discuss data clustering with different temporal resolutions, as well as their representation and potential applications in the real world.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "4.3. Time Series Data Clustering",
    "uri": "/en/ch4/ch4.3/"
  },
  {
    "content": " Table Of Contents Package Installation and Importing Initialization and Data Access Data Preprocessing Calibration Model Establishment and Validation Best Calibration Model of the Day Calibration Results References In this article, we’re going to look at how air quality data from Civil IoT Taiwan can be used. We’ll explain how we can calibrate two types of air quality sensors using data science, so they work together better. This helps us get a more complete picture of air quality. We’ll focus on two kinds of air quality sensors:\nEnvironmental Protection Agency (EPA) Air Quality Monitoring Stations: These are the traditional, professional stations that are big, expensive, and very accurate. They’re not in every neighborhood because they’re costly to set up and maintain. Instead, they’re placed in specific spots by local environmental agencies. As of July 2022, Taiwan has 81 of these official monitoring stations. Micro Air Quality Sensors: These are smaller, cheaper sensors that are part of the Internet of Things (IoT) network. They’re easier to put up in lots of places because they’re less expensive and simpler to install and look after. These sensors can quickly send data, even every minute, which is great for keeping an eye on air quality in real-time and reacting fast to sudden pollution. But they’re not as accurate as the big EPA stations. The big question is, how can we make these small, affordable sensors as accurate as the professional ones? We’re going to show you how we can do this with data science techniques. By improving the accuracy of these micro sensors, we can better integrate different types of sensors and make more use of the data they collect in the future.\nPackage Installation and Importing In this article, we’re going to make use of several powerful Python packages that are already pre-installed on Google Colab, our development platform. This means we won’t need to go through the hassle of manually installing them. We’ll be using:\npandas: Great for data manipulation and analysis. numpy: Essential for numerical operations. datetime: Helpful for handling dates and times. sklearn: A go-to library for machine learning. scipy: Useful for scientific and technical computing. joblib: Handy for saving and loading Python objects. Since these packages are ready to use on Google Colab, we can simply import them into our project. We’ll do this with standard Python import statements, allowing us to quickly set up our working environment and dive into the exciting work we have planned in this article.\nimport pandas as pd import numpy as np from datetime import datetime, timedelta from sklearn import linear_model, svm, tree from sklearn import metrics as sk_metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split, cross_validate from sklearn.feature_selection import f_regression import scipy.stats as stats import joblib Initialization and Data Access For this example, we’ll focus on comparing air quality data from the Wanhua station of the EPA air quality monitoring network (wanhua) with data from two Micro Air Quality Sensors of Academia Sinica in the Civil IoT Taiwan network. These sensors, identified by their device IDs 08BEAC028A52 and 08BEAC028690, are located at the same place as the EPA Wanhua station, giving us a unique opportunity to compare their readings directly.\nWe’ll evaluate the data using three different machine learning models:\nLinear Regression: A straightforward approach for predicting a quantitative response. Random Forest Regression: A more complex model that uses multiple decision trees to make predictions. Support Vector Regression (SVR): An advanced technique that can capture complex relationships in data. Our analysis will focus on several key features of the air quality data: PM2.5 concentration, temperature, relative humidity, and the timestamp of each reading. We’ll also incorporate historical data over three different time spans - 3 days, 5 days, and 8 days - to see how the length of the historical data affects our models’ performance.\nTo get started, let’s set up our initial programming environment with these specifics in mind. This will involve loading the necessary data and preparing our analysis tools to handle this unique set of data and models.\nSITE= \"wanhua\" EPA= \"EPA-Wanhua\" AIRBOXS= ['08BEAC028A52', '08BEAC028690'] DAYS= [8, 5, 3] METHODS= ['LinearRegression', 'RandomForestRegressor', 'SVR'] METHOD_SW= { 'LinearRegression':'LinearR', 'RandomForestRegressor':'RFR', 'SVR':'SVR' } METHOD_FUNTION= {'LinearRegression':linear_model.LinearRegression(), 'RandomForestRegressor': RandomForestRegressor(n_estimators= 300, random_state= 36), 'SVR': svm.SVR(C=20) } FIELD_SW= {'s_d0':'PM25', 'pm25':'PM25', 'PM2_5':\"PM25\", 'pm2_5':\"PM25\", 's_h0':\"HUM\", 's_t0':'TEM'} FEATURES_METHOD= {'PHTR':[\"PM25\", \"HR\", \"TEM\", \"HUM\"], 'PH':['PM25','HR'], 'PT':['PM25','TEM'], 'PR':['PM25', 'HUM'], 'P':['PM25'], 'PHR':[\"PM25\", \"HR\", \"HUM\"], 'PTR':[\"PM25\", \"TEM\", \"HUM\"], 'PHT':[\"PM25\", \"HR\", \"TEM\"] } To determine the appropriate amount of data needed for system calibration and data fusion, let’s visualize the process. Imagine you want to create a calibration model for a specific day, which we’ll call the Nth day. The data from the day before that, the (N-1)th day, will be used to test and evaluate the accuracy of this model.\nNow, if you decide to use X days’ worth of data for training the model, you will need historical data from the (N-2)nd day going back to the (N-2-X)th day. This range of days will serve as the training dataset.\nIn our case, we’re setting N to represent the current day (today), and we’ve decided that the maximum length for X (our training data period) is 8 days. Therefore, to proceed with our calibration and data fusion, we need to prepare a total of 10 days of historical data. This timeframe includes 8 days for training and an additional day each for testing and the current day’s data.\nThis approach ensures that we have a comprehensive dataset, allowing our models to learn effectively from recent historical data and be tested against the most immediate past data for validation of their accuracy.\nTo set up our analysis, we need to define three key dates in our code: the date for calibrating the model (TODAY), the date for the test data (TESTDATE), and the end date of the training data (ENDDATE). These dates correspond to the Nth day, the N-1th day, and the N-2th day, respectively. Here’s how you can do this in Python:\nTODAY = datetime.today() TESTDATE = (TODAY - timedelta(days=1)).date() ENDDATE = (TODAY - timedelta(days=2)).date() In this case, we’re utilizing the data access API from Academia Sinica’s Micro Air Quality Sensors. This allows you to download sensing data in a CSV file format for a specific date (input as ) and device (using the device code ). However, it’s important to remember that this API only lets you access data from the past 30 days from your current download date. So, the date you choose must be within this 30-day window.\nhttps://pm25.lass-net.org/data/history-date.php?device_id=\u003cID\u003e\u0026date=\u003cYYY-MM-DD\u003e\u0026format=CSV If we’re looking to get the sensor data from the EPA Wanhua Station for September 21, 2022, we can do so by following these steps:\nhttps://pm25.lass-net.org/data/history-date.php?device_id=EPA-Wanhua\u0026date=2022-09-21\u0026format=CSV We’ve created a Python function named getDF. This function can retrieve the sensor data from the past 10 days for a specified device code. It then compiles this data and returns it as a single DataFrame object.\ndef getDF(id): temp_list = [] for i in range(1,11): date = (TODAY - timedelta(days=i)).strftime(\"%Y-%m-%d\") URL = \"https://pm25.lass-net.org/data/history-date.php?device_id=\" + id + \"\u0026date=\" + date + \"\u0026format=CSV\" temp_DF = pd.read_csv( URL, index_col=0 ) temp_list.append( temp_DF ) print(\"ID: {id}, Date: {date}, Shape: {shape}\".format(id=id, date=date, shape=temp_DF.shape)) All_DF = pd.concat( temp_list ) return All_DF Next, we’ll download the data from the first air quality monitoring device, known as an airbox, located at the EPA Wanhua station. This data will be saved in an object we’re calling AirBox1_DF:\n# First AirBox device AirBox1_DF = getDF(AIRBOXS[0]) AirBox1_DF.head() ID: 08BEAC028A52, Date: 2022-09-29, Shape: (208, 19) ID: 08BEAC028A52, Date: 2022-09-28, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-27, Shape: (225, 19) ID: 08BEAC028A52, Date: 2022-09-26, Shape: (230, 19) ID: 08BEAC028A52, Date: 2022-09-25, Shape: (231, 19) ID: 08BEAC028A52, Date: 2022-09-24, Shape: (232, 19) ID: 08BEAC028A52, Date: 2022-09-23, Shape: (223, 19) ID: 08BEAC028A52, Date: 2022-09-22, Shape: (220, 19) ID: 08BEAC028A52, Date: 2022-09-21, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-20, Shape: (215, 19) To proceed in the same way, we obtained the sensor data from the second airbox at the EPA Wanhua station as well as the sensor data from the EPA Wanhua station itself. These sets of data were then saved in two different objects: AirBox2_DF for the airbox data and EPA_DF for the station’s data.\n# Second AirBox device AirBox2_DF = getDF(AIRBOXS[1]) # EPA station EPA_DF = getDF(EPA) Data Preprocessing To ensure we don’t use too much memory, we’re streamlining our data by keeping only the essential information and removing any unnecessary details.\nCol_need = [\"timestamp\", \"s_d0\", \"s_t0\", \"s_h0\"] AirBox1_DF_need = AirBox1_DF[Col_need] print(AirBox1_DF_need.head()) AirBox2_DF_need = AirBox2_DF[Col_need] print(AirBox2_DF_need.head()) Col_need = [\"time\", \"date\", \"pm2_5\"] EPA_DF_need = EPA_DF[Col_need] print(EPA_DF_need.head()) del AirBox1_DF del AirBox2_DF del EPA_DF timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:03:28Z 9.0 29.75 71.0 1 2022-09-30T00:33:46Z 11.0 31.36 67.0 2 2022-09-30T00:39:51Z 10.0 31.50 67.0 3 2022-09-30T00:45:58Z 12.0 31.50 66.0 4 2022-09-30T00:52:05Z 12.0 31.86 66.0 timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:00:31Z 9.0 29.36 -53.0 1 2022-09-30T00:07:17Z 9.0 29.50 -52.0 2 2022-09-30T00:23:47Z 10.0 30.25 -45.0 3 2022-09-30T00:34:24Z 10.0 31.11 -36.0 4 2022-09-30T00:40:31Z 11.0 31.25 -35.0 time date pm2_5 index 0 00:00:00 2022-09-30 9.0 1 01:00:00 2022-09-30 10.0 2 02:00:00 2022-09-30 16.0 3 03:00:00 2022-09-30 19.0 4 04:00:00 2022-09-30 20.0 Next, we’re combining the date and time information from the EPA station data into a new field called timestamp. This helps us have a unified way of looking at when the data was collected.\nEPA_DF_need['timestamp'] = pd.to_datetime( EPA_DF_need[\"date\"] + \"T\" + EPA_DF_need[\"time\"], utc=True ) print(EPA_DF_need.head()) time date pm2_5 timestamp index 0 00:00:00 2022-09-30 9.0 2022-09-30 00:00:00+00:00 1 01:00:00 2022-09-30 10.0 2022-09-30 01:00:00+00:00 2 02:00:00 2022-09-30 16.0 2022-09-30 02:00:00+00:00 3 03:00:00 2022-09-30 19.0 2022-09-30 03:00:00+00:00 4 04:00:00 2022-09-30 20.0 2022-09-30 04:00:00+00:00 The airbox and EPA station data are collected at different times - the airbox data every five minutes and the EPA station data every hour. To make them match, we’re adjusting the airbox data to also be on an hourly basis.\ndef getHourly(DF): DF = DF.set_index( pd.DatetimeIndex(DF[\"timestamp\"]) ) DF_Hourly = DF.resample('H').mean() DF_Hourly.reset_index(inplace=True) return DF_Hourly AirBox1_DF_need_Hourly = getHourly( AirBox1_DF_need) AirBox2_DF_need_Hourly = getHourly( AirBox2_DF_need) EPA_DF_need_Hourly = getHourly( EPA_DF_need) # 可省略，原始數據已經是小時平均 del AirBox1_DF_need del AirBox2_DF_need del EPA_DF_need print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp s_d0 s_t0 s_h0 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp pm2_5 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 We’re renaming the data fields from both the airbox and EPA stations to names that are easier to understand and track.\nCol_rename = {\"s_d0\":\"PM25\", \"s_h0\":\"HUM\", \"s_t0\":\"TEM\"} AirBox1_DF_need_Hourly.rename(columns=Col_rename, inplace=True) AirBox2_DF_need_Hourly.rename(columns=Col_rename, inplace=True) Col_rename = {\"pm2_5\":\"EPA_PM25\"} EPA_DF_need_Hourly.rename(columns=Col_rename, inplace=True) print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp PM25 TEM HUM 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp EPA_PM25 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 Since we have two airboxes that are identical and in the same place, we’re treating them as a single data source. We’re combining their data into one object, which we’re calling AirBoxs_DF. After that, we’re merging this with the EPA station data based on the time they were recorded. This creates a new, combined data object named ALL_DF.\nAirBoxs_DF = pd.concat([AirBox1_DF_need_Hourly, AirBox2_DF_need_Hourly]).reset_index(drop=True) All_DF = pd.merge( AirBoxs_DF, EPA_DF_need_Hourly, on=[\"timestamp\"], how=\"inner\" ) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 NaN 5 2022-09-21 02:00:00+00:00 9.000000 30.418889 -59.222222 NaN 6 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 7 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 8 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 9 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 We’re making sure to remove any missing or incomplete data, marked as NaN (Not a Number), from our dataset.\nAll_DF.dropna(how=\"any\", inplace=True) All_DF.reset_index(inplace=True, drop=True) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 Lastly, we’re adding a new field called HR to ALL_DF. This field will show the hour when the data was collected, which is taken from the timestamp field. This hour information is important for the model we’re building.\ndef return_HR(row): row['HR'] = int(row[ \"timestamp\" ].hour) return row All_DF = All_DF.apply(return_HR , axis=1) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 HR 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 1 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 1 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 3 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 3 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 4 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 4 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 5 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 5 Calibration Model Establishment and Validation After we’re done preparing the data, we move on to create potential calibration models. In this case, we’re looking at 3 types of regression methods, 3 lengths of historical data, and 8 different combinations of features. This means we’ll end up with a total of 72 (3 x 3 x 8) candidate models.\nFirst up, we use a tool called SlideDay. This function helps us gather a specific amount of historical data needed for building a calibration model. It works by taking the data from Hourly_DF and selecting data from the enddate going back for the number of days specified by day.\ndef SlideDay( Hourly_DF, day, enddate ): startdate= enddate- timedelta( days= (day-1) ) time_mask= Hourly_DF[\"timestamp\"].between( pd.Timestamp(startdate, tz='utc'), pd.Timestamp(enddate, tz='utc') ) return Hourly_DF[ time_mask ] Then, we arrange the sensor data Training_DF for a particular monitoring station, referred to as site, covering the period from enddate to day days before. This data is organized based on the selected features (feature feature combination). After this, we apply one of the regression methods (method) to the data. Our goal here is to ensure that the predictions made by our calibration model are as close as possible to the EPA_PM25 values in the training data.\nTo evaluate the accuracy of our model, we use two methods: Mean Absolute Error (MAE) and Mean Squared Error (MSE). MAE measures how far off our predictions are on average, by calculating the average of the absolute differences between predicted and actual values. A lower MAE means better accuracy. MSE, on the other hand, calculates the average of the squares of these differences. Again, a lower MSE indicates better model performance.\ndef BuildModel( site, enddate, feature, day, method, Training_DF ): X_train = Training_DF[ FEATURES_METHOD[ feature ] ] Y_train = Training_DF[ \"EPA_PM25\" ] model_result = {} model_result[\"site\"], model_result[\"day\"], model_result[\"feature\"], model_result[\"method\"] = site, day, feature, method model_result[\"datapoints\"], model_result[\"modelname\"] = X_train.shape[0], (site + \"_\" + str(day) + \"_\" + METHOD_SW[method] + \"_\" + feature) model_result[\"date\"] = enddate.strftime( \"%Y-%m-%d\" ) # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) model_result['create_timestamp_utc'] = Now_Time ### training model ### print( \"[BuildR]-\\\"{method}\\\" with {day}/{feature}\".format(method=method, day=day, feature=feature) ) # fit lm = METHOD_FUNTION[ method ] lm.fit( X_train, Y_train ) # get score Y_pred = lm.predict( X_train ) model_result['Train_MSE'] = MSE = sk_metrics.mean_squared_error( Y_train, Y_pred ) model_result['Train_MAE'] = sk_metrics.mean_absolute_error( Y_train, Y_pred ) return model_result, lm Beyond just looking at the MAE and MSE for the training data, we also assess how well our model performs on data it hasn’t been trained on. So, for the lm model, we generate predictions based on the required feature data (feature) and test data, and then compare these predictions to the EPA_PM25 values in the test data to calculate both MAE and MSE. This helps us gauge the effectiveness of different calibration models.\ndef TestModel( site, feature, modelname, Testing_DF, lm ): X_test = Testing_DF[ FEATURES_METHOD[ feature ] ] Y_test = Testing_DF[ \"EPA_PM25\" ] # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) ### testing model ### # predict Y_pred = lm.predict( X_test ) # get score test_result = {} test_result[\"test_MSE\"] = round( sk_metrics.mean_squared_error( Y_test, Y_pred ), 3) test_result[\"test_MAE\"] = round( sk_metrics.mean_absolute_error( Y_test, Y_pred ), 3) return test_result To wrap up, we bring together the SlideDay, BuildModel, and TestModel processes to complete all 72 calibration models. We calculate the MAE and MSE for both the training and testing data for each model and compile all these results into an AllResult_DF object for further analysis.\nAllResult_list = [] for day in DAYS: for method in METHODS: for feature in FEATURES_METHOD: Training_DF = SlideDay(All_DF, day, ENDDATE)[ FEATURES_METHOD[feature] + [\"EPA_PM25\"] ] result, lm = BuildModel( SITE, TESTDATE, feature, day, method, Training_DF ) test_result = TestModel(SITE, feature, result[\"modelname\"], SlideDay(All_DF, 1, TESTDATE), lm) R_DF = pd.DataFrame.from_dict( [{ **result, **test_result }] ) AllResult_list.append( R_DF ) AllResult_DF = pd.concat(AllResult_list) AllResult_DF.head() [BuildR]-\"LinearRegression\" with 8/PHTR [BuildR]-\"LinearRegression\" with 8/PH [BuildR]-\"LinearRegression\" with 8/PT [BuildR]-\"LinearRegression\" with 8/PR [BuildR]-\"LinearRegression\" with 8/P [BuildR]-\"LinearRegression\" with 8/PHR [BuildR]-\"LinearRegression\" with 8/PTR [BuildR]-\"LinearRegression\" with 8/PHT [BuildR]-\"RandomForestRegressor\" with 8/PHTR [BuildR]-\"RandomForestRegressor\" with 8/PH [BuildR]-\"RandomForestRegressor\" with 8/PT [BuildR]-\"RandomForestRegressor\" with 8/PR [BuildR]-\"RandomForestRegressor\" with 8/P [BuildR]-\"RandomForestRegressor\" with 8/PHR [BuildR]-\"RandomForestRegressor\" with 8/PTR [BuildR]-\"RandomForestRegressor\" with 8/PHT [BuildR]-\"SVR\" with 8/PHTR [BuildR]-\"SVR\" with 8/PH [BuildR]-\"SVR\" with 8/PT [BuildR]-\"SVR\" with 8/PR [BuildR]-\"SVR\" with 8/P [BuildR]-\"SVR\" with 8/PHR [BuildR]-\"SVR\" with 8/PTR [BuildR]-\"SVR\" with 8/PHT [BuildR]-\"LinearRegression\" with 5/PHTR [BuildR]-\"LinearRegression\" with 5/PH [BuildR]-\"LinearRegression\" with 5/PT [BuildR]-\"LinearRegression\" with 5/PR [BuildR]-\"LinearRegression\" with 5/P [BuildR]-\"LinearRegression\" with 5/PHR [BuildR]-\"LinearRegression\" with 5/PTR [BuildR]-\"LinearRegression\" with 5/PHT [BuildR]-\"RandomForestRegressor\" with 5/PHTR [BuildR]-\"RandomForestRegressor\" with 5/PH [BuildR]-\"RandomForestRegressor\" with 5/PT [BuildR]-\"RandomForestRegressor\" with 5/PR [BuildR]-\"RandomForestRegressor\" with 5/P [BuildR]-\"RandomForestRegressor\" with 5/PHR [BuildR]-\"RandomForestRegressor\" with 5/PTR [BuildR]-\"RandomForestRegressor\" with 5/PHT [BuildR]-\"SVR\" with 5/PHTR [BuildR]-\"SVR\" with 5/PH [BuildR]-\"SVR\" with 5/PT [BuildR]-\"SVR\" with 5/PR [BuildR]-\"SVR\" with 5/P [BuildR]-\"SVR\" with 5/PHR [BuildR]-\"SVR\" with 5/PTR [BuildR]-\"SVR\" with 5/PHT [BuildR]-\"LinearRegression\" with 3/PHTR [BuildR]-\"LinearRegression\" with 3/PH [BuildR]-\"LinearRegression\" with 3/PT [BuildR]-\"LinearRegression\" with 3/PR [BuildR]-\"LinearRegression\" with 3/P [BuildR]-\"LinearRegression\" with 3/PHR [BuildR]-\"LinearRegression\" with 3/PTR [BuildR]-\"LinearRegression\" with 3/PHT [BuildR]-\"RandomForestRegressor\" with 3/PHTR [BuildR]-\"RandomForestRegressor\" with 3/PH [BuildR]-\"RandomForestRegressor\" with 3/PT [BuildR]-\"RandomForestRegressor\" with 3/PR [BuildR]-\"RandomForestRegressor\" with 3/P [BuildR]-\"RandomForestRegressor\" with 3/PHR [BuildR]-\"RandomForestRegressor\" with 3/PTR [BuildR]-\"RandomForestRegressor\" with 3/PHT [BuildR]-\"SVR\" with 3/PHTR [BuildR]-\"SVR\" with 3/PH [BuildR]-\"SVR\" with 3/PT [BuildR]-\"SVR\" with 3/PR [BuildR]-\"SVR\" with 3/P [BuildR]-\"SVR\" with 3/PHR [BuildR]-\"SVR\" with 3/PTR [BuildR]-\"SVR\" with 3/PHT Best Calibration Model of the Day When we talk about the “best calibrated model of the day” in data analysis, it’s important to understand that we can only identify this “best model” after the day has ended. This is because we need to collect a full day’s worth of data (24 hours) before we can accurately measure the performance of different models. We use two methods, Mean Absolute Error (MAE) and Mean Squared Error (MSE), to evaluate these models. Only after comparing these can we determine which model was the best for that day.\nHowever, in the real world, we often need a calibrated model during the day, not after it has ended. So, what we typically do is assume that the best model from yesterday will also work well for today. We use this model to correct and process today’s data. For instance, if we’re using MSE to judge our best model, we can identify yesterday’s top performer using a specific method.\nFIELD= \"test_MSE\" BEST= AllResult_DF[ AllResult_DF[FIELD]== AllResult_DF[FIELD].min() ] BEST To make this model work for today, we take the best parameters from yesterday’s model (things like how much historical data it looks at, the method of analysis it uses, and the types of data it considers) and apply them to today’s data. This helps us create a new model for today.\nBEST_DC= BEST.to_dict(orient=\"index\")[0] Training_DF= SlideDay(All_DF, BEST_DC[\"day\"], TESTDATE)[ FEATURES_METHOD[BEST_DC[\"feature\"]]+ [\"EPA_PM25\"] ] result, lm= BuildModel( SITE, TODAY, BEST_DC[\"feature\"], BEST_DC[\"day\"], BEST_DC[\"method\"], Training_DF ) result [BuildR]-\"SVR\" with 3/PHT {'site': 'wanhua', 'day': 3, 'feature': 'PHT', 'method': 'SVR', 'datapoints': 80, 'modelname': 'wanhua_3_SVR_PHT', 'date': '2022-09-30', 'create_timestamp_utc': '2022-09-30 11:19:48', 'Train_MSE': 3.91517342356589, 'Train_MAE': 1.42125724796098} For example, let’s say that the new model we create for today has a Train_MSE of 3.915. This is higher than yesterday’s 2.179, which means it’s not as accurate. However, since we can’t know today’s best model until the day is over, we use yesterday’s model as the next best thing.\nFinally, we save this model in a special file format (.joblib) and share it for others to use. You can find more details on how to do this in the reference materials provided.\nmodel_dumpname= result[\"modelname\"]+ \".joblib\" MODEL_OUTPUT_PATH= \"\" try: joblib.dump( lm, MODEL_OUTPUT_PATH+ model_dumpname ) print( \"[BuildR]-dump {}\".format( MODEL_OUTPUT_PATH+model_dumpname ) ) except Exceptionas e: print( \"ERROR! [dump model] {}\".format( result[\"modelname\"] ) ) error_msg(e) [BuildR]-dump wanhua_3_SVR_PHT.joblib Calibration Results Since May 2020, the method we discuss in this article has been put to use in the “Academia Sinica - Micro Air Quality Sensors” as part of the Civil IoT Taiwan initiative. We’ve shared the daily calibration models developed through this method on the Dynamic Calibration Model website.\nHere’s what we did: We picked 31 air quality monitoring stations run by the Environmental Protection Agency (EPA) and set up two airboxes at each. To find the most effective daily calibration model for each location, we experimented with a variety of factors. This included using three different lengths of historical data, combining eight types of data features, and applying seven different regression techniques. Altogether, this meant testing 168 different combinations (3 historical data lengths x 8 data features x 7 regression methods).\nOnce we determined the best calibration model for each station, we used it to calibrate the data from the nearest airbox. This means the air quality data from each airbox is adjusted using the most suitable model based on its location. After implementing this system, we observed a significant improvement: the discrepancy between the readings from our micro air quality sensors and those from the EPA’s monitoring stations was greatly reduced. The success of this approach not only improved our air quality data accuracy but also set a strong example of how data from different systems can be effectively integrated for air quality monitoring. The figure below illustrates the reduced data discrepancy after implementing this method.\nReferences Dynamic Calibration Model Status Report (https://pm25.lass-net.org/DCF/) scikit-learn: machine learning in Python (https://scikit-learn.org/stable/) Joblib: running Python functions as pipeline jobs (https://joblib.readthedocs.io/) Jason Brownlee, Save and Load Machine Learning Models in Python with scikit-learn, Machine Learning Mastery (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/) ",
    "description": "We use air quality category data of the Civil IoT Taiwan project to demonstrate the dynamic calibration algorithm for Taiwanese micro air quality sensors and official monitoring stations. In a learning-by-doing way, from data preparation, feature extraction, to machine learning, data analysis, statistics and induction, the principle and implementation process of the multi-source sensor dynamic calibration model algorithm are reproduced step by step, allowing readers to experience how to gradually realize by superimposing basic data analysis and machine learning steps to achieve advanced and practical data application services.",
    "tags": [
      "Python",
      "Air"
    ],
    "title": "6.3. Joint Data Calibration",
    "uri": "/en/ch6/ch6.3/"
  },
  {
    "content": "\nTable Of Contents Package Installation and Importing Data Access Leafmap Basics Basic Data Presentation Cluster Data Presentation Change Leafmap Basemap Integrate OSM Resources Heatmap Presentation Split Window Presentation Leafmap for Web Applications Conclusion References In earlier sections, we showed you how to analyze geographic data using programming and how to use GIS (Geographic Information System) software for basic analysis and display of this data. Now, we’re going to teach you how to use Leafmap in Python for GIS tasks, and how to develop websites with Streamlit. At the end, we’ll show you how to combine Leafmap and Streamlit to create your own basic web-based GIS system. This will allow you to display your data analysis results on web pages.\nPackage Installation and Importing In this chapter, we’re going to use a variety of packages including pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder, and pyCIOT. However, except for pandas, these packages are not pre-installed on our development platform, Google Colab. So, we need to install them ourselves. To make this process smoother and avoid flooding the screen with too much information, we’ve added the ‘-q’ parameter to each installation command. This will help keep the output on the screen more streamlined and easier to read.\n!pip install -q geopandas !pip install -q leafmap !pip install -q ipyleaflet !pip install -q osmnx !pip install -q streamlit !pip install -q geocoder !pip install -q pyCIOT Once the installation is done, you can use the syntax provided below to import the necessary packages. This will complete the setup required for the tasks in this chapter.\nimport pandas as pd import geopandas as gpd import leafmap import ipyleaflet import osmnx import geocoder import streamlit from pyCIOT.data import * Data Access In this article, we work with several datasets from the Civil IoT Taiwan Data Service Platform. This includes air quality data from the Environmental Protection Administration (EPA), and seismic data from the National Earthquake Engineering Research Center and the Central Weather Bureau.\nFor the EPA air quality data, we utilize the pyCIOT package to fetch the latest readings from all the EPA air quality monitoring stations. We then convert this data, which is initially in JSON format, into a more usable format called a DataFrame. This is done using the json_normalize() method from the pandas package. In our analysis, we’ll focus on specific data points: the station names, their latitude and longitude, and the concentration of ozone (O3). The code for gathering and processing this data is as follows:\nepa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) df_air['O3'] = 0 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] df_air = df_air[['name','location.latitude','location.longitude','O3']] df_air Similarly, we’ll extract data from the seismic monitoring stations managed by the National Earthquake Engineering Research Center and the Central Weather Bureau. In this case, we’ll only retain essential information such as the station names and their longitude and latitude for our upcoming tasks. The code used to collect and process this data is as follows:\nquake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") df_quake = pd.json_normalize(quake_station) df_quake = df_quake[['name','location.latitude','location.longitude']] df_quake We’ve successfully shown you how to read air quality (air) data and seismic (quake) data. In the next part of our discussion, we’ll use these datasets to demonstrate operations and applications using the Leafmap suite. It’s worth noting that these methods can also be applied to other datasets available on the Civil IoT Taiwan Data Service Platform. We encourage you to experiment with these techniques on your own.\nLeafmap Basics Basic Data Presentation With the air quality data df_air and seismic data df_quake that we’ve prepared, our next step is to convert these datasets from the DataFrame format (provided by the pandas package) to the GeoDataFrame format. The GeoDataFrame format, supported by the geopandas package, is more suitable for handling geographic information.\nOnce we have our data in the GeoDataFrame format, we can utilize Leafmap’s add_gdf() method. This method allows us to create a distinct presentation layer for each dataset. After these layers are created, we can easily add both of them to our map in a single step.\ngdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") m1 In the map generated by our program, you’ll notice the names of the two datasets in the top-right corner of the map. They are added as two separate layers, allowing users to select and view the layer they’re interested in. However, when trying to view both layers simultaneously, there’s an issue: both layers use the same icon for representation, leading to confusion on the map.\nTo address this, we introduce a different approach for displaying data. We use the GeoData layer format provided by the ipyleaflet suite. By utilizing Leafmap’s add_layer() method, we can add these GeoData layers to the map. To make each dataset distinct and easily identifiable, we represent air quality stations with small blue circle icons and seismic stations with small red circle icons.\ngeo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) m2 Cluster Data Presentation When dealing with maps that have a large number of data points, it can become difficult to clearly observe individual points. In such scenarios, clustering is a useful technique. Clustering groups nearby points together when viewing a map from a distance, displaying them as a single point that shows the number of clustered items. As the user zooms in on the map, these clusters start to separate, revealing the individual points. Once zoomed in enough, each point can be seen independently, allowing users to view information about specific data points.\nLet’s use the seismic station data as an example. By employing the add_points_from_xy() method from Leafmap, we can display the data from df2 on the map in a clustered format. This approach will make it easier to manage and view a large number of points on the map.\nm3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") m3 Change Leafmap Basemap Leafmap primarily uses OpenStreetMap as its default basemap, but it also offers a wide variety of over 100 other basemap options. This allows users to switch the basemap according to their personal preferences or specific requirements. To explore the basemaps currently supported by Leafmap, you can use the following syntax:\nlayers = list(leafmap.basemaps.keys()) layers From the array of basemaps available in Leafmap, let’s choose SATELLITE and Stamen.Terrain for our demonstration. We can add these basemaps to our map as new layers using the add_basemap() method from the Leafmap package. Once added, Leafmap automatically activates all layers and stacks them in the order they were added. You can then easily select the specific layer you wish to view using the layer menu located in the upper right corner of the map. This feature allows for flexible and customized map viewing, tailored to your specific needs or preferences.\nm4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m4.add_gdf(gdf_air, layer_name=\"EPA Station\") m4.add_basemap(\"SATELLITE\") m4.add_basemap(\"Stamen.Terrain\") m4 Apart from utilizing the basemaps provided by Leafmap, you can also integrate layers from Google Maps by using its XYZ Tiles service. This allows you to add Google’s satellite imagery as layers on your map. The method to do this is as follows:\nm4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) m4 Integrate OSM Resources Leafmap not only includes built-in resources but also incorporates a wealth of external geographic information sources. One of the most prominent among these is OSM (OpenStreetMap), a widely recognized and comprehensive open-source geographic information resource. OSM offers a variety of resources, all of which can be explored on the OSM website. For a detailed view of what’s available, you can refer to the complete list of properties provided on their website. This list outlines the diverse range of features and data types that OSM offers, making it a valuable resource for anyone working with geographic information in Leafmap.\nIn the next example, we’ll demonstrate how to obtain and display the outline of a city on the map using the add_osm_from_geocode() method from the Leafmap package. We’ll use Taichung City as our example. By combining this with the location information from the EPA air quality monitoring station data, we can easily identify which stations are located within Taichung City. This method effectively allows us to visually integrate specific geographic outlines with our existing dataset, offering a clearer understanding of the data’s geographical context.\ncity_name = \"Taichung, Taiwan\" m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) m5 Following the previous step, we’ll further enhance our map by using the add_osm_from_place() method in the Leafmap package. This method allows us to search for and add specific facilities within Taichung City to our map layer. As an example, let’s focus on factory facilities. We’ll utilize OSM’s land use data to identify the locations and areas of factories in Taichung City. This information can be very useful when analyzed in conjunction with the locations of the EPA air quality monitoring stations, providing insights into potential environmental impacts or correlations.\nFor a broader range of OSM facility types that you might want to explore and add to your map, you can refer to the complete properties list provided by OpenStreetMap. This extensive list offers a variety of options, enabling detailed and specific geographic analyses and visualizations.\nm5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") m5 The Leafmap package also offers a convenient feature for finding OSM facilities near a specific location, which is incredibly useful for data analysis and interpretation. For instance, in the following example, we’ll use the add_osm_from_address() method to search for religious facilities (with the attribute “amenity”: “place_of_worship”) within a 1,000-meter radius of Qingshui Station in Taichung. Additionally, we’ll employ the add_osm_from_point() method to look for school facilities (attributes “amenity”: “school”) within 1,000 meters of the GPS coordinates (24.26365, 120.56917) of Taichung Qingshui Station.\nWe will then superimpose the results from these two searches as separate layers onto our existing map. This layered approach allows for a detailed and nuanced visual representation of how different types of facilities are distributed in relation to a specific point, offering valuable insights for spatial analysis.\nm5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) m5 Heatmap Presentation A heatmap is a graphical representation that uses color variations to show the intensity of events in a two-dimensional format. When integrated with a map, a heatmap can effectively illustrate the intensity of events at various scales, depending on the map’s scale. It’s a widely used and powerful tool for visualizing data.\nHowever, it’s important to ensure that the characteristics of the data are suitable for heatmap representation. Otherwise, it can be easily confused with other graphical data interpolation methods like IDW (Inverse Distance Weighting) and Kriging, which we discussed in Chapter 5. As an example, let’s use the O3 concentration data from the EPA air quality dataset to create a corresponding heatmap. This approach will demonstrate how heatmaps can effectively visualize specific data points, like pollution levels, across a geographic area.\nm6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m6.add_layer(geo_data_air) m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) m6 The initial view of the heatmap might not reveal much detail, especially when looking at a larger area. However, if you zoom in on a specific region, like Taichung city, you’ll notice a significant change in the heatmap’s appearance. This zoomed-in view can reveal more nuanced patterns and concentrations, showing how the heatmap provides different insights at various scales.\nThis characteristic of heatmaps is particularly useful as it allows for both a broad overview when zoomed out and a detailed analysis when zoomed in. In the context of Taichung city, zooming in can help you better understand the distribution and intensity of O3 concentrations in more localized areas, offering a clearer picture of air quality in different parts of the city.\nThe previous example with the O3 concentration data actually illustrates a common pitfall in using heatmaps: not all types of data are suitable for this kind of representation. The O3 data reflects specific, local concentrations of ozone. These values don’t necessarily accumulate or disperse to adjacent areas, especially when the map scale changes. Therefore, using O3 concentration data for a heatmap can be misleading. Instead, a geographic interpolation method, as described in Chapter 5, would be more appropriate for this type of data.\nTo demonstrate an appropriate use of heatmaps, let’s consider the location data of seismic monitoring stations. We’ll add a field named num, assigning it a default value of 10. This data is more suitable for a heatmap because it represents discrete, quantifiable events (in this case, the presence of seismic stations) that can be aggregated over an area. Here’s the code we’ll use to create a heatmap that accurately represents the distribution of Taiwan’s Seismic Monitoring Stations. This will give a clearer and more accurate depiction of seismic monitoring coverage across different regions.\ndf_quake['num'] = 10 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m7.add_layer(geo_data_quake) m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) m7 Split Window Presentation In data analysis and interpretation, switching between various basemaps can be crucial to gain different geographic perspectives. The Leafmap package accommodates this need with the split_map() method. This method divides the original map display into two submaps, allowing each to use a different basemap. This feature is particularly useful for comparative analysis or for obtaining a more comprehensive understanding of the geographic context. Here’s an example of how you can use this method:\nm8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m8.add_gdf(gdf_air, layer_name=\"EPA Station\") m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) m8 Leafmap for Web Applications To facilitate the quick sharing of processed map information, the Leafmap suite offers an integrated approach with the Streamlit suite. This combination leverages Leafmap’s GIS expertise and Streamlit’s web development capabilities, enabling you to quickly build a Web GIS system. Below, we’ll demonstrate how this works through a simple example. You can use this as a foundation to develop and expand your own Web GIS service.\nBuilding a web system with the Streamlit package involves two key steps:\nPackaging the Python Program: First, you need to package the Python program you want to execute into a Streamlit object. This packaging process is written into an app.py file. Executing app.py: Once your app.py is ready, you execute this file on your system to run your web application. Since our operations are all conducted on the Google Colab platform, we can take advantage of its unique features. Google Colab allows us to write app.py directly into its temporary storage area using the %%writefile magic command. Colab can then read and execute the code directly from this temporary storage. So, for the file writing part of step 1, we can proceed as follows:\n%%writefile app.py import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * contnet = \"\"\" Hello World! \"\"\" st.title('Streamlit Demo') st.write(\"## Leafmap Example\") st.markdown(contnet) epa_station = Air().get_data(src=\"OBS:EPA\") from pandas import json_normalize df_air = json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') with st.expander(\"See source code\"): with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") m.to_streamlit() For the second part, we use the following instructions:\n!streamlit run app.py \u0026 npx localtunnel --port 8501 When you run it, you should see a result that looks something like this:\nNext, click on the link that appears after the phrase “your url is:”. This will open a page in your browser that should look like the following.\nLastly, click on “Click to Continue” to run the Python code contained in the file named app.py. In this example, you’ll see a map displaying the distribution of the EPA’s air quality monitoring stations, which is created using the leafmap package.\nConclusion This article has introduced the Leafmap package as a tool for displaying geographic data and incorporating external resources. We’ve shown how combining Leafmap with the Streamlit package can create a basic web-based GIS (Geographic Information System) service on the Google Colab platform. It’s important to mention that Leafmap has many additional advanced features that we haven’t covered here. For a more detailed and comprehensive understanding, you can refer to the following resources.\nReferences Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "We introduce the capability of leafmap package to use different types of data for geographic information representation and spatial analysis in Civil IoT Taiwan Data Service Platform, and demonstrate the combination of leafmap and streamlit packages to build Web GIS applications. Through cross-domain and cross-tool resource integration, readers will be able to expand their imagination of the future of data analysis and information services.",
    "tags": [
      "Python",
      "Air",
      "Quake"
    ],
    "title": "7.3. Leafmap Applications",
    "uri": "/en/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/en/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Air",
    "uri": "/en/tags/air/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/en/tags/api/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Authors",
    "uri": "/en/authors/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/en/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/en/categories/"
  },
  {
    "content": "We love comments on our articles and would love to hear what you have to say respectfully. Remember to:\nFriendly, polite and relevant; No profanity, abusive, personal attacks or inappropriate content; If you harass people or make fun of tragedies, you will be blocked; No spam posts or posts trying to sell anything. The main rule here is simple: treat others with respect for yourself. Trolling and posting inappropriate content can get you banned, and we don’t want that.\nOur moderators do their best not to delete comments, and we believe that communities have individual, and often strong, opinions and perspectives. However, we reserve the right to remove comments for the above reasons as well as for the following reasons:\nAny violation of our Privacy Policy; Anything that is derogatory and/or derogatory in any perceivable way based on race, religion, colour, national origin, disability, sexual orientation; Any other comments that are harmful to the community. Our moderators try to catch inappropriate content, but we also want our community to help us police it. If you see an inappropriate comment, you can report it to us by clicking on the banner. We will review comments and decide whether to keep or delete them.\nCan’t find your comment on the site? Occasionally we experience technical difficulties and your comment may not appear on our site. We may not have approved it because it violates one of the above rules. We may have removed it because it violated our commenting guidelines. Why am I blocked from commenting? While we do our best to allow everyone to have an opinion in our online communities, sometimes commenters cross the line. From time to time, we block and ban commenters who violate our rules. Blockers use a variety of techniques and information to perform blocking, including emails, IP addresses, and any other information available. This prevents abusive commenters from posting comments on the site in the future, even if future or other comments are not abusive. Depending on the software we are using at the time, the ban could also cause other previous comments from the abuser to disappear.\n",
    "description": "",
    "tags": null,
    "title": "Comment Policy",
    "uri": "/en/comment-policy/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Disaster",
    "uri": "/en/tags/disaster/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Huei-Jun Kao",
    "uri": "/en/authors/huei-jun-kao/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Hung-Ying Chen",
    "uri": "/en/authors/hung-ying-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "intermediate",
    "uri": "/en/levels/intermediate/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/en/tags/introduction/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Jen-Wei Huang",
    "uri": "/en/authors/jen-wei-huang/"
  },
  {
    "content": "Learning Civil IoT Taiwan Open Data and Its Applications The “Civil IoT Taiwan” initiative aims to tackle urgent public needs and improve public services in areas such as air quality, earthquake monitoring, water resources, and disaster prevention. It leverages big data, artificial intelligence, and Internet of Things (IoT) to create intelligent systems that enhance public services and help both the government and citizens adapt to environmental changes. Tailored for various users, including government bodies, academia, industry, and the public, the project strives to bolster smart governance, aid industrial and academic growth, and elevate people’s well-being.\nA significant component of the Civil IoT Taiwan initiative is the “Civil IoT Taiwan Data Platform.” This platform offers users real-time and historical data in a consistent format, enhancing the efficiency of data browsing and retrieval. It fosters the use of model-based scientific computing and artificial intelligence by offering a wealth of reliable, high-quality data collected through the Civil IoT project. With this platform, the public gains access to comprehensive environmental information, enabling real-time monitoring of environmental changes. The platform also spurs the creation of value-added applications, harnessing collective creativity to address widespread issues.\nLooking ahead, the “Learning Civil IoT Taiwan” project is committed to three core objectives to ensure the continued growth and impact of the Civil IoT Taiwan project:\nEducational Engagement: It aims to provide educational resources for college and high school students, fostering interdisciplinary learning in information technology, geography, earth science, and social studies. Application Enhancement: The project seeks to refine and enhance existing applications of Civil IoT Taiwan, lowering barriers to entry and encouraging innovative developments to cater to public needs effectively. Platform Expansion: Efforts will be made to broaden data access methods, including the integration of the popular Python programming language, to attract a diverse technical audience and expand the user base of the data platform. ",
    "description": "",
    "tags": null,
    "title": "Learning Civil IoT Taiwan Open Data and Its Applications",
    "uri": "/en/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/en/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ling-Jyh Chen",
    "uri": "/en/authors/ling-jyh-chen/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Ming-Kuang Chung",
    "uri": "/en/authors/ming-kuang-chung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/en/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quake",
    "uri": "/en/tags/quake/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Quen Luo",
    "uri": "/en/authors/quen-luo/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Sky Hung",
    "uri": "/en/authors/sky-hung/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/en/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tze-Yu Sheng",
    "uri": "/en/authors/tze-yu-sheng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Water",
    "uri": "/en/tags/water/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Chi Peng",
    "uri": "/en/authors/yu-chi-peng/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Yu-Shen Cheng",
    "uri": "/en/authors/yu-shen-cheng/"
  }
]
