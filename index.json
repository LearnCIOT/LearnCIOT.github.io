[
  {
    "content": "1. 教學網站簡介 Table Of Contents 環境感知 開放資料 民生公共物聯網 民生公共物聯網資料服務平台 參考資料 環境感知 自古以來，人類對自然界的好奇心始於農業文明時期，那時的人們會仰望天空，觀察星象。到了西元十六世紀的文藝復興時代，哥白尼根據當時的天文觀測資料提出「地動說」，伽利略和牛頓則接續其思路，為現代科學奠定了基石。在當今社會，隨著半導體技術的快速進步，我們使用的感知生活環境的工具日趨多樣、精確和微型。當這些微型感測器結合了資訊科技的演進，以及網際網路的即時資料傳輸，我們獲得了前所未有的大量觀測數據。\n面對這浩如瀚海的數據，科學家正努力從中分析環境變化的模式，研究這些模式與災害之間的連結。他們的目的是為了更好地預測災害、提高我們的生活品質、強化災害防救效果，並促進人與環境的和諧共生。這不僅是現今個人、社群、國家乃至全球共同關心的議題，也是努力追求美好、永續生活環境的核心目標。\n開放資料 「開放資料」是一種策略，將各式電子化資料 —— 無論是文字、數據、圖片、影片還是聲音等，都經過特定的程序和格式整理，然後公開提供，並鼓勵各方自由使用。依據 Open Knowledge Foundation 的定義，要被認定為開放資料，必須符合以下幾項要件：\n開放授權或開放狀態 (Open license or status)：開放資料應屬於公眾領域，且須以開放授權的方式釋出，確保不受額外的使用限制。 自由存取 (Access)：人們應能在網路上輕鬆取得開放資料。儘管在某些合理情境下，一次性的取用費用或特定的使用條款可能是允許的。 機器可讀 (Machine readability)：開放資料的格式應該容易讓電腦進行讀取、分析和修改。 開放格式 (Open format)：資料必須採用一種開放的標準格式，確保使用者可以透過免費或開放原始碼的軟體來取得和使用它。 隨著開放原始碼、開放政府以及開放科學等理念的普及，開放資料已經逐步成為政府和科學界制定政策和進行學術研究的重要原則。特別是近來受到廣大關注的環境感知物聯網，因為其廣泛分布於公共空間，加上所監測的環境信息與大眾的日常生活密切相關，更被視為最受期待的開放資料來源之一。\n目前網路上常見的開放資料多使用 JSON、CSV 或 XML 的資料格式：\nJSON：全名為 Javascript Object Notation，這是一個輕量化的資料交換格式。它由屬性和值所組成，不僅機器友善，人們也能輕鬆閱讀和解讀。這格式常被選用於網站的資料展示、傳輸和存儲。\nCSV：全名為 Comma-Separated Values，正如其名，它是一種以文字方式儲存表格式資料的方法。在這格式中，每筆資料都是一行，而每個屬性則是一列。當資料儲存時，所有的屬性都會按照特定的順序進行排列，並用特定的符號分隔。這種格式廣泛應用於軟體的資料匯入 / 匯出、網路資料傳送以及歷史資料存儲。\nXML：全名為 Extensible Markup Language，它基於標準通用標記語言 (Standard Generalized Markup Language, SGML) 而來，是一個可擴展的標記式語言。它允許使用者自訂他們需要的標籤，適合建立包含結構化資料的文件，並廣泛用於文件的呈現和網上資料交換。\n目前臺灣常用的開放資料，多彙整於下列平台：\n政府資料開放平臺 (https://data.gov.tw/) 氣象資料開放平臺 (https://opendata.cwb.gov.tw/devManual/insrtuction) 環保署環境資料開放平臺 (https://data.epa.gov.tw/) 民生公共物聯網 鑑於環境感知的關鍵性及相應技術的快速進展，政府為更貼近民眾的生活與提供綜合性的公共服務，確認了四大民生議題：空氣品質、地震、水資源和災防。於是，在民國 106 年的「前瞻基礎建設 - 數位建設」計畫中，集結國科會、交通部、經濟部、內政部、環保署、中央研究院、農委會的資源，共同推動了一個跨部門的大型政府計畫－「民生公共物聯網」。此計畫融合大數據、人工智慧和物聯網技術，開發各式智慧生活服務系統，協助政府及民眾一同迎戰環境的變遷挑戰。此外，考量到不同的使用者群像，如政府部門、學術界、產業和大眾，這計畫也旨在促進政府的智慧治理、支持學界和產業的發展，並提升民眾的生活質量。\n民生公共物聯網目前著重於以下四大領域：\n水資源：民生公共物聯網結合中央與地方政府的水利單位，研發佈建多元化的水資源感測器，並且建置各式水文觀測設施與農田灌溉水情感測器，有效整合地面、地下與新興水源資訊，以及相關監測資訊，建立水資源物聯網入口網與灌溉配水動態分析管理平台，透過大數據收集與雲端分析運算，強化各河川局防汛作業系統，並建立污水下水道雲端管理雲，提供各級相關單位進行遠端、自動化及智慧化管理，以達到聯合運用的目標，促進水源智慧調控管理、灌溉配水動態分析管理、污水下水道雲加值應用、路面淹水示警等多元應用，同時透過資訊公開與資料開放，帶動公私協力的水資源資料應用與決策輔助進展。 空氣品質：民生公共物聯網結合環保署、經濟部、國科會與中央研究院，從空氣品質感測的基礎設施佈建開始，研發空氣品質感測器，建立感測器性能測試驗證中心，並於全台廣佈大量不同目的用途的空氣品質感測器，透過大量更小時間與空間尺度的感測資料收集，建立空品物聯網運算營運平台與智慧環境感測數據中心，並搭建空品感測資料展示平台，一方面促進空氣品質感測物聯網的產業開展，另一方面提供智慧環保稽查的事證資料。藉由智慧化環境監測，提供國人即時且在地的周遭環境空氣品質感測資料，並且利用高解析度感測數據，協助執法人員鑑別污染熱區，有效進行環境稽查與環境治理，並於計畫期間同時強化國內自有技術研發能量，確立國產自主化空品感測技術的發展。 地震：民生公共物聯網針對台灣常見的地震活動，除了大幅增加現地型地震速報主站，以提供高密度且高品質的地震速報資訊外，也同時增設與升級地震與地球物理觀測站，提升包含全球導航衛星系統 (Global Navigation Satellite System, GNSS) 站、井下地震站、強震站、地磁站、地下水站等觀測資料的品質與解析度；此外，針對大屯火山的區域監測，也強化其全球導航衛星系統站、井下地震站及無人機 (Unmanned Aerial Vehicle, UAV) 觀測等項目，並且也針對台灣特有的海島特性，強化強震與海嘯測報效能，擴建海底纜線與相關海底和陸上測站。另外，透過大數據的整合運用，除了建立台灣地震與地球物理資料整合與查詢平台，並且建立整合現地型與區域型地震速報資訊的複合式地震速報平台，可以強化台灣斷層帶及大屯火山區的調查觀測，並且更快速完整地傳遞地震速報資訊，提供民眾強震即時警報，進而促進地震防災產業的開展。 防救災：透過民生公共物聯網的介接整合，將全台包含空、水、地、災與民生各類別共計達58種示警資料集結在單一「民生示警公開資訊平台」，提供民眾即時防救災資訊，並透過應變管理資訊雲端系統 (EMIC2.0)，以及相關的決策圖台，提供防災人員各項災情、通報與救災資源，輔助各項決策工作，同時將相關歷史資料利用統一個資料格式彙整與釋出，提供防災產業分析使用，促進災害情資產業鏈的發展。 這些創新措施不僅優化了資源和環境管理，也為政府、企業和公眾提供了即時、可靠的資訊，從而提升了人民的生活品質和安全保障。\n民生公共物聯網資料服務平台 為了管理和運用民生公共物聯網系統產生的龐大資料，民生公共物聯計畫中也創建了一個專門的資料服務平台，確保提供穩定且高品質的感測資料來支持各類環境管理活動。這平台的目的也在於減少環境資訊的落差，讓所有人都能獲得即時和全面的環境數據，掌握周遭環境的即時變化。\n「民生公共物聯網資料服務平台」將開放資料的理念融入其中，使用標準化的資料格式來提供即時資料連接和歷史資料查詢服務。該平台致力於增加使用者的資料瀏覽和搜索速度，並建立一個有效的感測資料存儲機制，以支持模擬分析和人工智慧的應用。這個平台不僅讓民眾能隨時隨地查詢到關於周邊環境的詳細資訊，也為產業提供了一個基礎，促進民間創新，並開發出能解決民生問題的優質服務，同時將資訊和技術資源開放給所有人，讓社會各界能共同參與，創建一個更美好、更可持續的生活環境。\n目前民生公共物聯網資料服務平台使用 OGC SensorThings API 的開放資料格式，相關的資料格式說明，以及各領域資料內容說明，可參考下列的投影片與影像說明：\n資料服務平台簡介 [PPT] 資料服務平台與SensorThings API簡介 [Video] 水資源領域 [PPT] 水資源物聯網 [Video] 空氣品質領域 [PPT] 環境品質感測物聯網 [Video] [PPT] PM2.5微型感測器布建 [Video] 地震領域 [PPT] 海陸地震聯合觀測 [Video] [PPT] 複合式地震速報 [Video] 防救災領域 [PPT] 民生示警公開資料 [Video] [PPT] 災害防救資訊系統整合 [Video] 為了擴大全民對於民生公共物聯網與其資料平台的參與，自 2018 年起，民生公共物聯網計畫也陸續舉辦了一系列的資料應用競賽、資料創新馬拉松、實體與虛擬展覽，同時也設計了一系列的訓練教材與商業輔導，從團隊的建立與養成，創意的發起到成形，應用服務的構思到落地，在過去這幾年間已累積多年的能量，並且成功發展出成功的案例，讓民生公共物聯網不僅僅只是政府單位的硬體佈建，更已成功轉化為民生公共的基礎資訊建設，提供源源不絕高品質的感測資料，便利民眾的生活，開創更多便民、有感、體貼的資訊服務。\n有關民生公共物聯網資料應用服務在各個領域的解決方案範例，可參考下列的網站資源：\n民生公共物聯網資料應用服務與解決方案－水資源 民生公共物聯網資料應用服務與解決方案－空氣品質 民生公共物聯網資料應用服務與解決方案－地震 民生公共物聯網資料應用服務與解決方案－防救災 參考資料 Open Definition: defining open in open data, open content and open knowledge. Open Knowledge Foundation. (https://opendefinition.org/od/2.1/en/)\n民生公共物聯網 (https://ci.taiwan.gov.tw)\n民生公共物聯網線上虛擬特展：在萬物相連中對話 (https://ci.taiwan.gov.tw/dialogue-in-civil-iot)\n民生公共物聯網資料應用服務與解決方案指南 (https://www.civiliottw.tadpi.org.tw)\n民生公共物聯網資料服務平台 (https://ci.taiwan.gov.tw/dsp/)\nXML - Wikipedia (https://en.wikipedia.org/wiki/XML)\nJSON - Wikipedia (https://en.wikipedia.org/wiki/JSON)\nComma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values)\nStandard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language)\nOGC SensorThings API Documentation (https://developers.sensorup.com/docs/)\n",
    "description": "教學網站簡介",
    "tags": [
      "概述"
    ],
    "title": "1. 教學網站簡介",
    "uri": "/ch1/"
  },
  {
    "content": "2. 整體課程前言 在這個主題中，我們介紹民生公共物聯網資料應用教材的整體架構，以及所使用到的程式語言 Python 和開發平台 Google Colab。除了概念上的描述外，我們也提供由淺入深的大量延伸學習資源，讓有興趣的讀者，可以依據需求自行進行更進一步地探索學習。\n2.1. 課程架構說明民生公共物聯網資料應用的整體架構介紹\n2.2. 課程軟體工具簡介民生公共物聯網資料應用所使用到的程式語言 Python 和開發平台 Google Colab 簡介\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "2. 整體課程前言",
    "uri": "/ch2/"
  },
  {
    "content": "3. 資料取用 在這個主題中，我們介紹如何透過本教材提供的教材工具，使用簡易的 Python 語法，直接取用民生公共物聯網開放資料平台的資料。為了更深入的透過範例介紹不同的資料存取方法，我們將此主題切分出兩個單元，進行更深入的介紹：\n3.1. 基本資料存取方法我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。\n3.2. 存取特定時空條件的資料我們介紹如何在民生公共物聯網資料平台中獲取特定時間或時間段的資料，以及特定地理區域的資料，並透過簡單的案例演示其應用。\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "3. 資料取用",
    "uri": "/ch3/"
  },
  {
    "content": "4. 時間維度資料分析 在這個主題中，我們針對物聯網資料所具備的時序特性，介紹一系列的時間維度資料分析方法。透過民生公共物聯網資料平台的使用，我們將發展下列三項單元，進行更深入的介紹。\n4.1. 時間序列資料處理我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與殘差波動，同時套用既有的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料，並探討其背後所可能隱含的意義。\n4.2. 時間序列資料預測我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件 (例如 scikit-learn、Kats 等)，用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。\n4.3. 時間序列屬性分群我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，並且探討不同的資料集與不同時間解析度的資料分群，在真實場域所代表的意義，以及可能衍生的應用。\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "4. 時間維度資料分析",
    "uri": "/ch4/"
  },
  {
    "content": "5. 空間維度資料分析 在這個主題中，我們針對物聯網資料所具備的地理空間特性，介紹一系列的空間維度資料處理與資料分析方法。透過民生公共物聯網資料平台的使用，我們將發展下列兩項單元，進行更深入的介紹。\n5.1. 地理空間篩選我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。\n5.2. 地理空間分析我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "5. 空間維度資料分析",
    "uri": "/ch5/"
  },
  {
    "content": "6. 資料應用 在這個主題中，我們將著重在存取民生公共物聯網開放資料後的衍生應用，透過其他函式庫套件與分析演算法的導入，強化民生公共物聯網開放資料的價值與應用服務。我們預計發展的單元內容包含：\n6.1. 機器學習初探我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行資料分類與資料分群的分析。我們將示範機器學習的標準流程，並且介紹如何透過資料分類進一步進行資料預測，以及如何透過資料分群進行對資料進一步的深入探究。\n6.2. 異常資料偵測我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。\n6.3. 感測器聯合校正我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "6. 資料應用",
    "uri": "/ch6/"
  },
  {
    "content": "7. 系統整合應用 在這個主題中，我們將著重在民生公共物聯網開放資料與其他應用軟體的整合應用，透過其他應用軟體的專業功能，進一步深化與發揮民生公共物聯網開放資料的價值。\n7.1. QGIS 應用我們介紹使用 QGIS 系統進行的地理資料呈現，並且以Civil IoT Taiwan 的資料當作範例，利用點擊拖拉的方式，進行地理空間分析。同時我們也討論 QGIS 軟體的優缺點與使用時機。\n7.2. Tableau 應用我們介紹使用 Tableau 工具呈現民生公共物聯網的開放資料，並使用空品資料和災害通報資料進行兩個範例演示。我們介紹如何使用工作表、儀表板和文本來建立互動式的資料視覺化系統，方便使用者近一步深入探索。我們同時提供豐富的參考資料供使用者學習參考。\n7.3 Leafmap 應用我們介紹 leafmap 套件在民生公共物聯網資料平台中使用不同類型數據進行地理信息表示和空間分析的能力，並演示了 leafmap 和 streamlit 套件的結合共同構建 Web GIS 的應用。透過跨域與跨工具的資源整合，將能拓展讀者對數據分析和信息服務未來的想像。\nPrevious Next     Page: /     Download ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "7. 系統整合應用",
    "uri": "/ch7/"
  },
  {
    "content": "上傳文章 資訊 介紹如何上傳文章\n其他參考資料 相關開放網課 哈爸 自學政府開放資料應用實戰-以LASS 總統盃水專案為例\n本釋出幾乎包含整個課程的所有東西，含（投影片，錄影，相關文件/素材，範例程式碼，QGIS包，學習歷程以及教自學心得）\n哈爸 高中生自學用 python 寫遊戲\n本釋出幾乎包含整個課程的所有東西，含（投影片，錄影，相關文件/素材，範例程式碼，復刻遊戲專案）\n",
    "description": "",
    "tags": null,
    "title": "附錄一：上傳文章",
    "uri": "/upload/"
  },
  {
    "content": "教學影片 Table Of Contents 民生公共物聯網 資料應用網站簡介影片 研習工作坊教學影片 (2022/10/22 台北) 研習工作坊教學影片 (2022/11/5 台中) 研習工作坊教學影片 (2022/11/27 台南) 民生公共物聯網 資料應用網站簡介影片 1 教學網站簡介 2 整體課程前言 3 資料取用 4 時間維度資料分析 5 空間維度資料分析 6 資料應用 7 系統整合應用 結尾 完整版 (53:25) 研習工作坊教學影片 (2022/10/22 台北) 整體課程與教學網站簡介 資料取用 時間維度資料分析 空間維度資料分析 研習工作坊教學影片 (2022/11/5 台中) 整體課程與教學網站簡介 資料取用 系統整合應用：Leafmap 系統整合應用：Tableau 研習工作坊教學影片 (2022/11/27 台南) 整體課程與教學網站簡介 資料取用 資料應用與機器學習 系統整合應用：QGIS ",
    "description": "各單元教學影片",
    "tags": [
      "概述"
    ],
    "title": "教學影片",
    "uri": "/video/"
  },
  {
    "content": "本套課程包含「教學網站簡介」、「整體課程前言」、「資料取用方法」、「時間維度資料分析」、「空間維度資料分析」、「資料應用案例」與「系統整合案例」等七大主題，其具體內容分別說明如下：\n教學網站簡介 我們介紹這整個計畫的核心 - 「民生公共物聯網」，介紹台灣過去幾年在「民生公共物聯網」的各項成果，以及「民生公共物聯網開放資料」涵蓋水、空、地、災四大面向資料，所能帶來在日常生活上的各項應用與服務。我們同時呈現各項既有的文章、影片與成功案例，讓學員與閱讀者能深刻體認「民生公共物聯網」對民生用途的重要性。\n整體課程前言 我們介紹這整套課程的架構，以及本套課程所使用的 Python程式語言與 Google Colab程式開發平台，透過淺顯的文字介紹，引導學員快速了解整體課程的架構，並且提供豐富的外部資源清單，讓有興趣與需求的學員，能更進一步深入探索相關的技術。\n資料取用方法 我們介紹如何透過我們開發的工具，可以使用簡易的 Python 語法，直接取用民生公共物聯網開放資料平台的資料，並且根據不同的需求，切割出兩個單元進行更深入的介紹：\n基本資料存取方法：我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。 存取特定時空條件的資料：我們介紹如何獲取民生公共物聯網資料平台中某測站特定時間或時間區段的資料、尋找最鄰近的測站當下最新的一筆資料，以及尋找某位置座標周圍固定區域所有測站當下最新的一筆資料等應用。 在這兩個單元中，除了介紹資料的存取方法外，我們也穿插基本的探索式資料分析 (Exploratory Data Analysis, EDA) 方法，藉由常用的統計方式描述資料不同面向的資料特性，並繪製簡易的圖表，讓讀者透過動手做、做中學的方式，體驗掌握資料與資料分析的成就感。\n時間維度資料分析 我們針對物聯網資料所具備的時序特性，設計三個單元介紹一系列的時間維度資料分析方法。\n時間序列資料處理：我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與隨機波動四大部分，同時套用既有常見的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料中，有關變點檢測與異常值檢測，其背後所可能隱含的意義。 時間序列資料預測：我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件（例如 scikit-learn、Kats 等），用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。 時間序列屬性分群：我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，探討資料集在不同時間解析度所代表的意義，以及可能衍生的應用。 空間維度資料分析 我們針對物聯網資料所具備的地理空間特性，根據不同的分析需求與應用，介紹一系列的空間維度資料處理與資料分析方法。\n地理空間篩選：我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。 地理空間分析：我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。 資料應用案例 我們在這個主題中，將著重在存取民生公共物聯網開放資料後的衍生應用，透過其他函式庫套件與分析演算法的導入，強化民生公共物聯網開放資料的價值與應用服務。我們依照由淺入深的原則，依序發展下列三個子主題：\n機器學習初探：我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行感測值的預測分析。我們將示範機器學習的標準流程，並且介紹預測分析的成效評估方法，以及如何避免機器學習容易產生的偏差 (Bias) 與過度學習 (Overfitting) 問題。 感測器異常偵測：我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。 感測器動態校正模型：我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。 系統整合案例 在這個主題中，我們著重在民生公共物聯網開放資料與其他應用軟體的整合應用，透過其他應用軟體的專業功能，進一步深化與發揮民生公共物聯網開放資料的價值。我們發展的單元內容包含：\nQGIS 應用：QGIS 是一套免費且開源的 GIS 軟體，使用者可以藉由這套軟體，整理、分析與繪製不同的地理空間資料與主題地圖，呈現地理現象和資訊型態的分佈狀況。我們示範導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，利用 QGIS 的圖資進行套疊，用點擊拖拉的方式，進行在「地理空間篩選」與「地理空間分析」進行過的各種分析，並且討論 QGIS 軟體的優缺點與使用時機。 Tableau Public 應用：Tableau 是一套操作簡單且功能強大的資料視覺化軟體，可以輕易地連結資料庫與各種格式資料檔案，並製作各種精美的統計圖表。我們示範如何利用拖拉點選的方式，導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，並且針對數值資料進行簡單的圖表製作與相互比對，對於空間資料則結合地圖圖資進行套疊與製圖，對於時間序列資料則透過圖表方式顯示數值變化的趨勢。此外，透過外部資料源的介接，我們可以將多元且相異來源的資料集，整合在單一地圖製圖中，在極短的時間內產製精美的報表。 使由 leafmap 與 Streamlit 自建簡易的 GIS 資訊服務：leafmap是一套Python 開發套件，用於和 Google Earth Engine 進行深度整合，並能在類似 Google Colab 或 Jupyter Notebook 平台上直接進行操作與視覺化呈現分析成果；Streamlit則是一個提供 Python 使用者迅速搭建架構網頁應用程式的套件。我們示範導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，藉由 leafmap 的地理資訊圖台與空間分析能力，搭配 Google Earth Engine 豐富的衛星影像，建構簡單的 GIS 資訊服務，同時借助 Streamlit 的友善介面操作，整合成網頁版的 GIS 資訊服務，擴展讀者對於資料分析與資訊服務的未來想像。 參考資料 民生公共物聯網 (https://ci.taiwan.gov.tw) 民生公共物聯網資料服務平台 (https://ci.taiwan.gov.tw/dsp/) QGIS - A Free and Open Source Geographic Information System (https://qgis.org/) Tableau - Business Intelligence and Analytics Software (https://www.tableau.com/) Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit - The fastest way to build and share data apps (https://streamlit.io/) Google Colaboratory (https://colab.research.google.com/) ",
    "description": "民生公共物聯網資料應用的整體架構介紹",
    "tags": [
      "概述"
    ],
    "title": "2.1. 課程架構說明",
    "uri": "/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents pyCIOT API 是什麼 pyCIOT API 使用方法 下載 pyCIOT 模組套件 使用 pyCIOT 模組 pyCIoT API 獲取資料方式 空氣資料存取 獲取專案代碼 Air().get_source() 獲取所有測站列表 Air().get_station() 獲取測站資料 Air().get_data() 水源資料存取 獲取專案代碼 Water().get_source() 獲取所有測站列表 Water().get_station() 獲取測站資料 Water().get_data() 地震資料存取 獲取專案代碼 Quake().get_source() 獲取地震監測站列表 Quake().get_station() 獲取地震資料 Quake().get_data() 獲取單一地震資料Quake().get_data() 天氣資料存取 獲取專案代碼 Weather().get_source() 獲取所有測站列表 Weather().get_station() 獲取測站資料 Weather().get_data() 影像資料存取 獲取專案代碼 CCTV().get_source() 獲取影像資料 CCTV().get_data() 災難警示資料存取 獲取災情示警 Disaster().get_alert() 獲取災情通報歷史資料 Disaster().get_notice() 參考資料 本章節涵蓋 pyCIOT API 的使用方法，以及空氣、水源、地震、天氣、影像資料、以及災難警示資料的基本存取方法。包含單一測站的最新一筆感測資料、獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。\n本章節需要讀者有基本的終端機操作能力及接觸過 Python 程式基本語法。\npyCIOT API 是什麼 政府開放資料現已有非常多種類的資料及窗口讓我們查詢，而不同的窗口有各自不同的資料存取方式。即便這些資料逐漸使用開放授權，但在進行資料搜集時，因為下載取得資料的方式各有差異，若要將這些資料整理會變得極為麻煩。本函式庫（python 模組）為了解決窗口不一的困難，將所有有提供 API 的政府民生開放資料的開放搜集至此，嘗試讓開放資料獲取門檻降低，降低自動化及二手資料處理成本。****\npyCIOT API 使用方法 下載 pyCIOT 模組套件 為了使用 pyCIOT API 服務，我們需要先將下載此服務的函式庫模組。pip 是一個以 Python 寫成的軟體包管理系統，用來安裝和管理 Python 軟體包。而這次使用的 pyCIOT 函式庫則是交由 Python Package Index (pypi) 管理，而我們可以在終端機上用這行指令將 pyCIOT 函式庫下載到本地，同時也會將其他必須的套件一起下載：\n# 使用 pip 安裝 pyCIOT 套件 !pip install pyCIOT 使用 pyCIOT 模組 欲在 python 引入本模組，僅需輸入語法引用 pyCIOT.data 即可：\n# 引入 pyCIOT 的 data 模組中所有功能 from pyCIOT.data import * 根據引入模組的方式不同，呼叫函式的方法也不一樣。若是使用 from ... import ... 的語法，呼叫函式時不需加上前綴；但若是使用 import ... 的話，則是要在每次呼叫其模組下的函式時加上前綴，而使用 import ... as ... 則是可以根據在 as 後自定義前綴方便撰寫程式。\n# 引入函式的三種方式 # 引入整個模組，使用時需加上模組名稱作為前綴 import pyCIOT.data a = pyCIOT.data.Air().get_source() # ~~~~~~~~~~~~ 加虛線上的文字 # 使用別名引入模組，可以縮短模組名稱 import pyCIOT.data as CIoT a = CIoT.Air().get_source() # ~~~~~ 可自行定義前綴 # 直接引入模組中的所有功能，使用時不需加任何前綴 # 注意：這種方法在引入大量模組時可能會引起名稱衝突 from pyCIOT.data import * a = Air().get_source() pyCIoT API 獲取資料方式 大部分的 API 資料格式都能夠透過下列方式獲取，包含空氣、水源、地震、天氣、影像資料等：\n.get_source() 獲取專案代碼回傳在民生公共物聯網開放資料平台中有的專案代碼，返回格式為 array。 .get_station(src='') 獲取回傳各測站資料的基本資訊及位置，返回格式為 array；可選擇性帶入 src 參數，指名所要查詢的專案代碼。 .get_data(src='', stationID='') 獲取所有測站列表返回格式為 array，回傳各測站的基本資訊，位置及感測資料；可選擇性帶入 src 參數，指名所要查詢的專案代碼，或選擇性帶入 stationID 參數，指名所要查詢的機器代碼。 災害警示資料則適用於以下：\n.get_alert() 獲取警示資料返回格式為 json，包括該事件的相關示警資訊。 .get_notice() 獲取通報歷史資料返回格式為 json，包括該事件的相關通報資訊。 若與 pyCIOT Package Document 內容相悖，以其為準。\n空氣資料存取 獲取專案代碼 Air().get_source() # 使用 Air 類別的 get_source 函式回傳所有空氣相關的專案代碼 a = Air().get_source() # 顯示所得的專案代碼 print(a) ['OBS:EPA', 'OBS:EPA_IoT', 'OBS:AS_IoT', 'OBS:MOST_IoT', 'OBS:NCNU_IoT'] 專案代碼轉換列表\nOBS:EPA: 環保署國家空品測站 OBS:EPA_IoT: 環保署智慧城鄉空品微型感測器 OBS:AS_IoT: 中研院校園空品微型感測器 OBS:MOST_IoT: 科技部智慧園區空品測站 OBS:NCNU_IoT: 暨南大學在地空品微型感測器 獲取所有測站列表 Air().get_station() # 使用 Air 類別的 get_station 函式，並指定資料來源為環保署的智慧城鄉空品微型感測，來獲取檢測站列表 b = Air().get_station(src=\"OBS:EPA_IoT\") # 顯示前五個檢測站項目資料 b[0:5] [ { 'name': '智慧城鄉空品微型感測器-10287974676', 'description': '智慧城鄉空品微型感測器-10287974676', 'properties': { 'city': '新北市', 'areaType': '社區', 'isMobile': 'false', 'township': '鶯歌區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10287974676', 'locationId': 'TW040203A0507221', 'Description': '廣域SAQ-210', 'areaDescription': '鶯歌區' }, 'location': { 'latitude': 24.9507, 'longitude': 121.3408416, 'address': None } }, ... ] 獲取測站資料 Air().get_data() # 使用 Air 類別的 get_data 函式，指定資料來源和檢測站ID，來獲取該檢測站的空氣品質資料 f = Air().get_data(src=\"OBS:EPA_IoT\", stationID=\"11613429495\") # 顯示所得的空氣品質資料 f [ {'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 30.6}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-28T06:23:08.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 9.8}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}} ] # 顯示空氣品質資料的 description 部分 print(f[0]['description']) # 遍歷資料集中的 data 部分，尋找 \"溫度\" 的數據 for f_data in f[0]['data']: # 如果找到 \"溫度\" 的數據，則顯示相對應的值和時間戳記 if f_data['description'] == '溫度': print(f_data['description'], ': ', f_data['values'][0]['value'], ' (', f_data['values'][0]['timestamp'], ')', sep='') 智慧城鄉空品微型感測器-11613429495 溫度: 30.6 (2022-08-28T06:22:08.000Z) 水源資料存取 獲取專案代碼 Water().get_source() 根據引數不同會回傳不同種類的專案代碼：\nwater_level_station: 回傳水位站的專案代碼（目前合法的代碼僅有 WRA, WRA2 和 IA)。 gate: 回傳閘門的專案代碼（目前合法的代碼僅有 WRA, WRA2 和 IA)。 pumping_station: 回傳抽水站的專案代碼（目前合法的代碼有 WRA2 和 TPE)。 sensor: 回傳各式感測器的專案代碼（目前合法的代碼有 WRA, WRA2, IA 和 CPAMI)。 `` (無輸入): 回傳所有水資源相關的專案代碼。 # 使用 Water 類別的 get_source 函式回傳所有水資源相關的專案代碼 wa = Water().get_source() # 顯示所得的專案代碼 wa ['WATER_LEVEL:WRA_RIVER', 'WATER_LEVEL:WRA_GROUNDWATER', 'WATER_LEVEL:WRA2_DRAINAGE', 'WATER_LEVEL:IA_POND', 'WATER_LEVEL:IA_IRRIGATION', 'GATE:WRA', 'GATE:WRA2', 'GATE:IA', 'PUMPING:WRA2', 'PUMPING:TPE', 'FLOODING:WRA', 'FLOODING:WRA2'] 專案代碼轉換列表：\nWRA: 水利署 WRA2: 水利署（與縣市政府合建） IA: 農田水利署 CPAMI: 營建署 TPE: 臺北市 獲取所有測站列表 Water().get_station() # 使用 Water 類別的 get_station 函式，並指定資料來源為 \"WATER_LEVEL:WRA_RIVER\"，來獲取水位檢測站列表 wa = Water().get_station(src=\"WATER_LEVEL:WRA_RIVER\") # 只顯示檢測站列表的第一筆資料 wa[0] { 'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None} } 獲取測站資料 Water().get_data() # 使用 Water 類別的 get_data 函式，指定資料來源和檢測站ID，來獲取該檢測站的水位資料 wa = Water().get_data(src=\"WATER_LEVEL:WRA_RIVER\", stationID=\"01790145-cd7e-4498-9240-f0fcd9061df2\") # 顯示所得的水位資料 wa [{'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'data': [{'name': '水位', 'description': ' Datastream_id=016e5ea0-7c7f-41a2-af41-eabacdbb613f, Datastream_FullName=延平.水位, Datastream_Description=現場觀測, Datastream_Category_type=河川水位站, Datastream_Category=水文', 'values': [{'timestamp': '2022-08-28T06:00:00.000Z', 'value': 157.41}]}], 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None}}] 地震資料存取 獲取專案代碼 Quake().get_source() # 使用 Quake 類別的 get_source 函式回傳所有地震相關的專案代碼 q = Quake().get_source() # 顯示所得的專案代碼 q ['EARTHQUAKE:CWB+NCREE'] 專案代碼轉換列表：\nEARTHQUAKE:CWB+NCREE: 中央氣象局與國震中心地震監測站（因資料儲存方式不同，CWB 及 NCREE 的測站資料無法分別查詢，因此在每個事件中，會回傳來源中所有測站的監測資料） 獲取地震監測站列表 Quake().get_station() # 使用 Quake 類別的 get_station 函式，並指定資料來源為 \"EARTHQUAKE:CWB+NCREE\"，來獲取地震檢測站列表 q = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") # 只顯示檢測站列表的前兩筆資料 q[0:2] [{'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'properties': {'authority': '中央氣象局', 'stationID': 'EGC', 'deviceType': 'FBA', 'stationName': 'Jiqi'}, 'location': {'latitude': 23.708, 'longitude': 121.548, 'address': None}}, {'name': '地震監測站-Xilin-ESL', 'description': '地震監測站-Xilin-ESL', 'properties': {'authority': '中央氣象局', 'stationID': 'ESL', 'deviceType': 'FBA', 'stationName': 'Xilin'}, 'location': {'latitude': 23.812, 'longitude': 121.442, 'address': None}}] 獲取地震資料 Quake().get_data() # 使用 Quake 類別的 get_data 函式，指定資料來源為 \"EARTHQUAKE:CWB+NCREE\"，來獲取地震資料 q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\") # 只顯示最後一筆地震資料 q[-1] {'name': '第2022083號地震', 'description': '第2022083號地震', 'properties': {'depth': 43.6, 'authority': '中央氣象局', 'magnitude': 5.4}, 'data': [ { 'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18815)' },{ 'name': '地震監測站-Taichung City-TCU', 'description': '地震監測站-Taichung City-TCU', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18816)' }, ... ] 'location': {'latitude': 22.98, 'longitude': 121.37, 'address': None}} 獲取單一地震資料Quake().get_data() # 使用 Quake 類別的 get_data 函式，指定資料來源和事件編號，來獲取該事件的地震資料 q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\", eventID=\"2022083\") # 顯示所得的地震資料 q # 獲得資料之格式和上述相同 天氣資料存取 獲取專案代碼 Weather().get_source() # 使用 Weather 類別的 get_source 函式回傳所有天氣相關的專案代碼 w = Weather().get_source() # 顯示所得的專案代碼 w ['GENERAL:CWB', 'GENERAL:CWB_IoT', 'RAINFALL:CWB', 'RAINFALL:WRA', 'RAINFALL:WRA2', 'RAINFALL:IA', 'IMAGE:CWB'] 根據引數不同會回傳不同種類的專案名稱：\nGENERAL: 回傳氣象站的專案代碼 RAINFALL: 回傳雨量站/雨量感測器的專案代碼 IMAGE: 回傳雷達回波圖的專案代碼 `` (無輸入): 回傳所有氣象相關的專案代碼。 專案代碼轉換列表：\nGENERAL:CWB: 中央氣象局局屬氣象站 GENERAL:CWB_IoT: 中央氣象局自動氣象站 RAINFALL:CWB:中央氣象局雨量站 RAINFALL:WRA: 水利署雨量感測器 RAINFALL:WRA2: 水利署（與縣市政府合建）雨量感測器 RAINFALL:IA: 農田水利署雨量感測器 IMAGE:CWB: 中央氣象局雷達整合回波圖 獲取所有測站列表 Weather().get_station() # 使用 Weather 類別的 get_station 函式，並指定資料來源為 \"RAINFALL:CWB\"，來獲取雨量站列表 w = Weather().get_station(src=\"RAINFALL:CWB\") # 顯示所得的雨量站列表 w [{'name': '雨量站-C1R120-上德文', 'description': '雨量站-C1R120-上德文', 'properties': {'city': '屏東縣', 'township': '三地門鄉', 'authority': '中央氣象局', 'stationID': 'C1R120', 'stationName': '上德文', 'stationType': '局屬無人測站'}, 'location': {'latitude': 22.765, 'longitude': 120.6964, 'address': None}}, ... ] 獲取測站資料 Weather().get_data() # 獲取測站 南投縣 雨量站-U2HA40-臺大內茅埔 的雨量資料 # 使用 Weather 類別的 get_data 函式，指定資料來源和測站ID，來獲取該測站的雨量資料 w = Weather().get_data(src=\"RAINFALL:CWB\", stationID=\"U2HA40\") # 顯示所得的雨量資料 w [{'name': '雨量站-U2HA40-臺大內茅埔', 'description': '雨量站-U2HA40-臺大內茅埔', 'properties': {'city': '南投縣', 'township': '信義鄉', 'authority': '中央氣象局', 'stationID': 'U2HA40', 'stationName': '臺大內茅埔', 'stationType': '中央氣象局'}, 'data': [{'name': 'HOUR_12', 'description': '12小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'MIN_10', 'description': '10分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'RAIN', 'description': '60分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_6', 'description': '6小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_3', 'description': '3小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_24', 'description': '24小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'NOW', 'description': '本日累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'ELEV', 'description': '高度', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 507.0}]}], 'location': {'latitude': 23.6915, 'longitude': 120.843, 'address': None}}] 影像資料存取 獲取專案代碼 CCTV().get_source() # 使用 CCTV 類別的 get_source 函式回傳所有與影像相關的專案代碼 cctv = CCTV().get_source() # 顯示所得的專案代碼 cctv ['IMAGE:EPA', 'IMAGE:WRA', 'IMAGE:COA'] 專案代碼轉換列表：\nIMAGE:EPA: 環保署空品監測即時影像器 IMAGE:WRA: 水利署水利防災用影像器 IMAGE:COA: 行政院農委會土石流觀測站影像 獲取影像資料 CCTV().get_data() # 使用 CCTV 類別的 get_data 函式，指定資料來源為 \"IMAGE:EPA\"，來獲取環保署空品監測即時影像資料 cEPA = CCTV().get_data(\"IMAGE:EPA\") # 顯示第三筆影像資料 cEPA[2] { 'name': '環保署空品監測即時影像器-萬里', 'description': '環保署-萬里-空品監測即時影像器', 'properties': { 'city': '新北市', 'basin': '北部空品區', 'authority': '環保署', 'stationName': '萬里', 'CCDIdentifier': '3'}, 'data': [ { 'name': '即時影像', 'description': '環保署-萬里-空品監測即時影像器', 'values': [ { 'timestamp': '2022-06-13T09:00:00.000Z', 'value': 'https://airtw.epa.gov.tw/AirSitePic/20220613/003-202206131700.jpg' } ] } ], 'location': {'latitude': 25.179667, 'longitude': 121.689881, 'address': None} } # 使用 CCTV 類別的 get_data 函式，指定資料來源為 \"IMAGE:COA\"，來獲取行政院農委會土石流觀測站影像資料 cCOA = CCTV().get_data(\"IMAGE:COA\") # 顯示第一筆影像資料 cCOA[0] {'name': '行政院農委會土石流觀測站影像-大粗坑下游攝影機', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'properties': {'city': '新北市', 'township': '瑞芳鎮', 'StationId': '7', 'authority': '行政院農委會', 'stationName': '大粗坑下游攝影機', 'CCDIdentifier': '2'}, 'data': [{'name': '即時影像', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'values': [{'timestamp': '2099-12-31T00:00:00.000Z', 'value': 'http://dfm.swcb.gov.tw/debrisFinal/ShowCCDImg-LG.asp?StationID=7\u0026CCDId=2'}]}], 'location': {'latitude': 25.090878, 'longitude': 121.837815, 'address': '新北市瑞芳鎮弓橋里大粗坑'}} 災難警示資料存取 民生公共物聯網資料平台上提供的災情示警有 58 個項目以上，而災情通報共計有 41 個項目以上；不同災情示警個由不同單位負責，而災情通報則為中央災害應變中心負責。\n因專案代碼轉換列表過長，可參考 pyCIOT Package Document。\n獲取災情示警 Disaster().get_alert() # 使用 Disaster 類別的 get_alert 函式，指定參數為 \"5\"，來獲取災害警報資訊 d = Disaster().get_alert(\"5\") # 顯示所得的災害警報資訊 d {'id': 'https://alerts.ncdr.nat.gov.tw/Json.aspx', 'title': 'NCDR_CAP-即時防災資訊(Json)', 'updated': '2021-10-12T08:32:00+08:00', 'author': {'name': 'NCDR'}, 'link': {'@rel': 'self', '@href': 'https://alerts.ncdr.nat.gov.tw/JSONAtomFeed.ashx?AlertType=5'}, 'entry': [ { 'id': 'CWB-Weather_typhoon-warning_202110102030001', 'title': '颱風', 'updated': '2021-10-10T20:30:05+08:00', 'author': {'name': '中央氣象局'}, 'link': {'@rel': 'alternate', '@href': 'https://b-alertsline.cdn.hinet.net/Capstorage/CWB/2021/Typhoon_warnings/fifows_typhoon-warning_202110102030.cap'}, 'summary': { '@type': 'html', '#text': '1SEA18KOMPASU圓規2021-10-10T12:00:00+00:0018.20,126.702330992150輕度颱風TROPICAL STORM2021-10-11T12:00:00+00:0019.20,121.902835980220輕度颱風 圓規（國際命名 KOMPASU）10日20時的中心位置在北緯 18.2 度，東經 126.7 度，即在鵝鑾鼻的東南東方約 730 公里之海面上。中心氣壓 992 百帕，近中心最大風速每秒 23 公尺（約每小時 83 公里），相當於 9 級風，瞬間最大陣風每秒 30 公尺（約每小時 108 公里），相當於 11 級風，七級風暴風半徑 150 公里，十級風暴風半徑 – 公里。以每小時21公里速度，向西進行，預測11日20時的中心位置在北緯 19.2 度，東經 121.9 度，即在鵝鑾鼻的南南東方約 320 公里之海面上。根據最新資料顯示，第18號颱風中心目前在鵝鑾鼻東南東方海面，向西移動，其暴風圈正逐漸向巴士海峽接近，對巴士海峽將構成威脅。巴士海峽航行及作業船隻應嚴加戒備。第18號颱風外圍環流影響，易有短延時強降雨，今(10日)晚至明(11)日基隆北海岸、宜蘭地區及新北山區有局部大雨發生的機率，請注意。＊第18號颱風及其外圍環流影響，今(10日)晚至明(11)日巴士海峽及臺灣附近各海面風浪逐漸增大，基隆北海岸、東半部（含蘭嶼、綠島）、西南部、恆春半島沿海易有長浪發生，前往海邊活動請特別注意安全。＊第18號颱風外圍環流影響，今(10日)晚至明(11)日臺南以北、東半部(含蘭嶼、綠島)、恆春半島、澎湖、金門、馬祖沿海及空曠地區將有9至12級強陣風，內陸地區及其他沿海空曠地區亦有較強陣風，請注意。＊第18號颱風外圍環流沉降影響，明(11)日南投、彰化至臺南及金門地區高溫炎熱，局部地區有36度以上高溫發生的機率，請注意。＊本警報單之颱風半徑為平均半徑，第18號颱風之7級風暴風半徑西南象限較小約60公里，其他象限約180公里，平均半徑約為150公里。'}, 'category': {'@term': '颱風'} },{ ... }, ... ] } 獲取災情通報歷史資料 Disaster().get_notice() # 使用 Disaster 類別的 get_notice 函式，指定專案代碼為 \"ERA2_F1\"，來獲取交通災情通報表（道路、橋梁部分） d = Disaster().get_notice(\"ERA2_F1\") # 顯示所得的交通災情通報資訊 d \"maincmt\":{ \"prj_no\":\"專案代號\", \"org_name\":\"填報機關\", \"rpt_approval\":\"核定人\", \"rpt_phone\":\"聯絡電話\", \"rpt_mobile_phone\":\"行動電話\", \"rpt_no\":\"通報別\", \"rpt_user\":\"通報人\", \"rpt_time\":\"通報時間\" }, \"main\":{ \"prj_no\":\"2014224301\", \"org_name\":\"交通部公路總局\", ... }, \"detailcmt\":{ \"trfstatus\":\"狀態\", ... }, ... } 參考資料 Python pyCIOT pypi package (https://pypi.org/project/pyCIOT/) Python pyCIOT Document (https://hackmd.io/@cclljj/pyCIOT_doc) ",
    "description": "我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。",
    "tags": [
      "Python",
      "API",
      "水",
      "空",
      "地",
      "災"
    ],
    "title": "3.1. 基本資料存取方法",
    "uri": "/ch3/ch3.1/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引用 讀取資料 空品資料 水位資料 氣象資料 資料視覺化 (Visualization) 重新採樣 (resample) 移動平均 (moving average) 多曲線圖 日曆熱力圖 資料品質檢測與處理 離群值偵測 (Outlier detection) 改變點偵測 (Change point detection) 缺失資料 (missing data) 處理 資料分解(Decomposition) 參考資料 時間序列資料是依照時間上發生的先後順序形成的資料，通常在資料上的時間間隔會一樣（例如：五分鐘一筆資料、一小時一筆資料），應用的領域相當廣泛，如：金融資訊、太空工程、訊號處理等，在分析上也有許多統計相關的工具可以使用。 同時也可以發現時序資料是很貼近日常生活的，隨著全球氣候變遷的日益加劇，這幾年全球的平均氣溫越來越高，在夏天時更是讓人熱到非常有感，也越來越難以忍受；又或是在一年中某些季節的空氣品質往往特別差，或者某些時間的空氣品質往往比其他時間來的差等。如果想要更加了解這些生活環境的改變，以及其對應的感測器數值是如何變化的，就會運用到時間序列資料的分析，也就是觀察資料與時間的關係，進而得出結果。本章節將會使用三種資料（空氣品質、水資源、氣象）示範。\n章節目標 使用作圖工具觀察時序資料 檢測與處理時序資料 分解時序資料得到趨勢與週期性 套件安裝與引用 在本章節中，我們將會使用到 pandas, matplotlib, numpy, seaborn, statsmodels, warnings 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 並未預先安裝好的套件：kats 和 calplot，需使用下列的方式自行安裝：\n# 升級 pip，確保使用最新版 !pip install --upgrade pip # 安裝 Kats 套件，版本指定為 0.1，以及相依的 ax-platform 和 statsmodels 套件 !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # 安裝 calplot 套件，用於繪製日曆熱圖 !pip install calplot 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport warnings # 引入警告處理模組，用於控制警告訊息 import calplot # 引入日曆圖繪製函式庫 import pandas as pd # 引入資料處理函式庫 pandas import numpy as np # 引入數學運算函式庫 NumPy import matplotlib as mpl # 引入繪圖函式庫 Matplotlib 的基礎設定 import matplotlib.pyplot as plt # 引入繪圖函式庫 Matplotlib 的 pyplot 模組 import statsmodels.api as sm # 引入統計模型函式庫 Statsmodels 的 API import os, zipfile # 引入檔案操作和壓縮函式庫 from datetime import datetime, timedelta # 引入日期時間處理模組 from dateutil import parser as datetime_parser # 引入日期時間解析模組 from statsmodels.tsa.stattools import adfuller, kpss # 引入時序數據的統計檢定函式 from statsmodels.tsa.seasonal import seasonal_decompose # 引入季節分解函式 from kats.detectors.outlier import OutlierDetector # 引入異常值檢測器 from kats.detectors.cusum_detection import CUSUMDetector # 引入 CUSUM 異常檢測器 from kats.consts import TimeSeriesData, TimeSeriesIterator # 引入 Kats 時序數據和迭代器類型 from IPython.core.pylabtools import figsize # 引入 IPython 繪圖尺寸設定工具 讀取資料 我們使用 pandas 進行資料的處理，pandas 是 Python 語言常使用到的資料科學套件，也可以想成是程式語言中類似 Microsoft Excel 的試算表，而 pandas 所提供的 dataframe 物件，更可以想成是一種二維的資料結構，可以用行、列的方式儲存資料，方便各式的資料處理與運算。\n本章節的探討主題為時序資料 (time series data) 的分析處理，因此我們將分別以民生公共物聯網資料平台上的空品、水位和氣象資料進行資料讀取的演示，接著再使用空品資料進行更進一步的資料分析。其中，每一類別的資料處理都將使用其中一個測站長期以來觀測到的資料作為資料集，而在 dataframe 的時間欄位名稱則設為 timestamp，由於時間欄位的數值具有唯一性，因此我們也將使用此欄位作為 dataframe 的索引 (index)。\n空品資料 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的歷史資料，並存入 Air 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Air 資料夾中。\n!mkdir Air CSV_Air # 建立「Air」和「CSV_Air」資料夾 # 從指定的網址下載四個 zip 檔案到「Air」資料夾 !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" # 初始化資料夾和檔案副檔名。 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' # 第一次解壓縮：遍歷「Air」資料夾，找到所有 ZIP 檔並解壓縮。 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) # 輸出目前正在解壓縮的檔案路徑。 zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) # 解壓縮到「Air」資料夾。 zip_ref.close() # 第二次和第三次解壓縮：遍歷資料夾，解壓縮內層 ZIP 檔。 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) # 輸出目前正在解壓縮的檔案路徑。 zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) # 解壓縮到同一資料夾。 zip_ref.close() # 第三次解壓縮和搬移 CSV 檔案。 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) # 輸出目前正在解壓縮的檔案路徑。 zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # 解壓縮到「CSV_Air」資料夾。 zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') # 將 CSV 檔搬移到「CSV_Air」資料夾。 現在 CSV_Air 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 74DA38C7D2AC 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 air 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。 test\n# 初始化資料夾名稱和 CSV 檔案的副檔名。 folder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' # 感測器 ID # 建立一個空的 DataFrame，名為「air」。 air = pd.DataFrame() # 遍歷「CSV_Air」資料夾中的每一個檔案。 for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 讀取 CSV 檔案到 DataFrame # 若有「pm25」這個欄位，改名為「PM25」。 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 過濾出指定設備 ID 的資料。 filtered = df.query(f'device_id==@id') # 合併過濾後的資料到「air」。 air = pd.concat([air, filtered], ignore_index=True) # 去掉 timestamp 是空值的行。 air.dropna(subset=['timestamp'], inplace=True) # 將 timestamp 的時區資訊移除。 for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) # 解析有時區的時間。 naive = aware.replace(tzinfo=None) # 移除時區資訊。 air.at[i, 'timestamp'] = naive # 更新 DataFrame。 # 將 timestamp 設為 DataFrame 的 index。 air.set_index('timestamp', inplace=True) # 刪除「Air」和「CSV_Air」資料夾。 !rm -rf Air CSV_Air 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 刪除「device_id」和「SiteName」這兩個欄位。 air.drop(columns=['device_id', 'SiteName'], inplace=True) # 根據「timestamp」欄位排序 DataFrame。 air.sort_values(by='timestamp', inplace=True) # 顯示 DataFrame 的基本資訊。 air.info() # 輸出 DataFrame 的前五行。 print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 水位資料 和空品資料的範例一樣，由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「水利署地下水位站」的歷史資料，並存入 Water 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Water 資料夾中。\n# 建立「Water」和「CSV_Water」資料夾。 !mkdir Water CSV_Water # 從指定網址下載四個年份的資料壓縮包。 !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" # 初始化變數。 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' # 第一次解壓縮：解壓縮下載的 zip 檔到「Water」資料夾。 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() # 第二次解壓縮：解壓縮「Water」資料夾中的 zip 檔。 # 第三次解壓縮：解壓縮嵌套在資料夾內的 zip 檔，排除以 'QC.zip' 結尾的檔。 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() # 移動 CSV 檔到「CSV_Water」資料夾。 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') 現在 CSV_Water 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 338c9c1c-57d8-41d7-9af2-731fb86e632c 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 water 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 設定資料夾和檔案副檔名變數 folder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' # 初始化一個空的 DataFrame 來儲存水質數據 water = pd.DataFrame() # 開始讀取「CSV_Water」資料夾中的每個 .csv 檔 for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 如果存在「pm25」欄位，改名為「PM25」 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 篩選出對應於特定 station_id 的資料 filtered = df.query(f'station_id==@id') # 將篩選出的資料加入到主 DataFrame 中 water = pd.concat([water, filtered], ignore_index=True) # 去除 timestamp 為空的資料 water.dropna(subset=['timestamp'], inplace=True) # 將 timestamp 轉換為 naive datetime（去除時區資訊） for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive # 將 DataFrame 的索引設為 timestamp water.set_index('timestamp', inplace=True) # 最後，刪除不再需要的資料夾 !rm -rf Water CSV_Water 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 移除不必要的欄位 water.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) # 依照 timestamp 欄位排序資料 water.sort_values(by='timestamp', inplace=True) # 顯示 DataFrame 的基本資訊 water.info() # 列印 DataFrame 的前五行來確認資料 print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 氣象資料 我們從民生公共物聯網資料平台的歷史資料庫下載「中央氣象局自動氣象站」的歷史資料，並存入 Weather 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Weather 資料夾中。\n# 創建 Weather 和 CSV_Weather 目錄 !mkdir Weather CSV_Weather # 下載 2019、2020、2021 年的氣象資料壓縮檔 !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" # 變數設定，用來存放目錄名和檔案副檔名 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' # 解壓縮在 Weather 目錄下的所有壓縮檔 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): # 檢查是否為 .zip 檔 print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) # 解壓縮到 Weather 目錄 zip_ref.close() # 處理 Weather 目錄下的子目錄和檔案 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): # 檢查是否為目錄 for item in os.listdir(path): if item.endswith(extension_zip): 檢查是否為 .zip 檔 file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) # 解壓縮到該目錄 zip_ref.close() # 移動 .csv 檔到 CSV_Weather 目錄 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): # 檢查是否為目錄 for it in os.listdir(path2): if it.endswith(extension_zip): # 再次檢查是否為 .zip 檔 file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # 解壓縮到 CSV_Weather 目錄 zip_ref.close() elif item.endswith(extension_csv): # 檢查是否為 .csv 檔 os.rename(path2, f'CSV_Weather/{item}') # 移動到 CSV_Weather 目錄 現在 CSV_Weather 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 C0U750 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 weather 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 設定目錄和檔案副檔名變數 folder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' # 目標氣象站的 ID # 創建一個空的 DataFrame 來存放氣象數據 weather = pd.DataFrame() # 讀取 CSV_Weather 目錄下的所有 .csv 檔案 for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 如果有 pm25 欄位，改名為 PM25 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 篩選出特定氣象站的數據 filtered = df.query(f'station_id==@id') # 將篩選後的數據加入到主 DataFrame weather = pd.concat([weather, filtered], ignore_index=True) # 將 obsTime 改名為 timestamp weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) # 去掉沒有時間戳記的列 weather.dropna(subset=['timestamp'], inplace=True) # 轉換時間戳記格式 for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive # 將 timestamp 設為 DataFrame 的索引 weather.set_index('timestamp', inplace=True) # 刪除暫存的資料夾 !rm -rf Weather CSV_Weather 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 刪除 station_id 這個欄位，因為我們已經知道這是特定站點的數據 weather.drop(columns=['station_id'], inplace=True) # 根據 timestamp 欄位對資料進行排序 weather.sort_values(by='timestamp', inplace=True) # 查看 DataFrame 的基本資訊，包括欄位名稱、非空值數量等 weather.info() # 印出排序後的前 5 筆資料，這樣可以快速檢查一下數據是否正確 print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN 以上我們已經成功示範空品資料 (air)、水位資料 (water) 和氣象資料 (weather) 的讀取範例，在接下來的探討中，我們將以空品資料示範初步的時間序列資料處理，相同的方法也可以輕易改成使用水位資料或氣象資料而得到類似的結果，大家可以自行嘗試看看。\n資料視覺化 (Visualization) 時間序列資料處理的第一個步驟，不外乎就是將資料依照時間順序一筆一筆的呈現出來，讓使用者可以看到整體資料的變化，並且衍生更多資料分析的想法與概念，其中使用折線圖進行資料的展示，是最常使用的一種資料視覺化方法。例如若以空品資料為例：\n# 設定繪圖區域的大小和解析度 plt.figure(figsize=(15, 10), dpi=60) # 繪製 PM2.5 的時序圖，用 Date 當 x 軸，PM2.5 當 y 軸 plt.plot(air[:][\"PM25\"]) # 設定 x 軸和 y 軸的標籤 plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") # 設定圖表的標題 plt.title(\"PM2.5 Time Series Plot\") # 自動調整子圖參數，使之填充整個圖像區域 plt.tight_layout() # 顯示圖表 plt.show() 重新採樣 (resample) 從上圖空品資料的時序圖中，可以看到其實資料的分佈是很密集的，同時資料數值的變化有時很小有時卻很劇烈，這是因為目前空品資料的採樣頻率約略是每五分鐘一筆，同時採集的環境是生活中的周遭環境資訊，因此資料密集與起伏不定其實是必然的。由於每五分鐘一筆資料的採樣過於頻繁，不易呈現環境空品的整體變化趨勢，因此我們採用重新採樣的方法，在固定的時間間隔內計算資料的平均值，便能呈現資料資料在不同時間尺度的整體變化狀況。例如我們根據現有空品資料的特性，利用下列的語法以較大尺度 (小時、日、月) 的採樣率重新取樣：\n# 用 resample('H') 來計算每小時的平均 PM2.5，並儲存到 air_hour air_hour = air.resample('H').mean() #每小時的平均 # 用 resample('D') 來計算每天的平均 PM2.5，並儲存到 air_day air_day = air.resample('D').mean() #每日的平均 # 用 resample('M') 來計算每月的平均 PM2.5，並儲存到 air_month air_month = air.resample('M').mean() #每月的平均 # 顯示每個時間範圍的平均值的前五行來檢查 print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 接著我們使用每小時平均的重新採樣後資料再次進行作圖，可以看到線條曲線變得較為清晰，但曲線的波動還是很大。\n# 設定畫布大小和解析度 plt.figure(figsize=(15, 10), dpi=60) # 繪製 PM2.5 時間序列圖，只取用 'PM25' 這一欄 plt.plot(air_hour[:][\"PM25\"]) # 設定 x 軸和 y 軸的標籤 plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") # 設定圖表標題 plt.title(\"PM2.5 Time Series Plot\") # 自動調整子圖參數，使之填充整個圖形區域 plt.tight_layout() # 顯示圖表 plt.show() 移動平均 (moving average) 針對原始資料的變化圖，如果想看到更平滑的曲線變化趨勢圖，可以套用移動平均的方法，其中最主要的觀念就是在原始資料的時間軸上設定一個取樣窗 (window)，並在取樣窗內計算所有數值的平均數，並且用平滑移動的方式滑動取樣窗的位置，計算當下資料與窗內前幾筆資料的平均值。例如，若取樣窗的大小為 10，就代表每次要將當下的資料和前面 9 次的資料做平均，經過這樣的處理後每筆資料所代表的意義就不只是某一個時間點，而是原本時間點加上前幾個時間點的平均，如此一來，可以消除突發的變化讓整體曲線的呈現更加圓滑，也更容易觀察整體的變化趨勢。\n# plt.figure(figsize=(15, 10), dpi=60) # MA 儲存每小時的平均 PM2.5 數值 MA = air_hour # 使用 rolling 函數計算窗口為 500 的移動平均，並儲存在 MA10 MA10 = MA.rolling(window=500, min_periods=1).mean() # 將原始資料和 500 窗口的移動平均資料結合，然後畫出圖表 # add_suffix 會在欄位名後加上「_mean_500」，以便於區分 MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # 如果單獨畫 MA10 的圖也可以用下面這行 # MA10.plot(figsize(15, 10)) 上圖中的藍色線為原始資料，橘色線則是經過移動平均後的曲線，可以清楚發現橘色線更能代表整體數值的變化趨勢，同時也存在某種程度的規律起伏，值得後續進一步的分析。\n多曲線圖 除了將原始資料用單純的折線圖方式呈現外，另一個常見的資料時覺化手法，則是依照時間維度上的週期性，將資料切割為數個連續的片段，分別繪製折線圖，並疊加在同一張多曲線圖中。例如，我們可以將前述的空品資料，依照年份的不同切割出 2019, 2020, 2021, 和 2022 四個子資料集，並分別繪製各自的折線圖於同一張多曲線圖中，如下圖所示。\n# 重新設定 air_month 的 index，並加入年份和月份兩個新欄位 air_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] # 取得唯一的年份資料 years = air_month['year'].unique() print(air) # 設定隨機顏色 np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) # 開始繪圖 plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: # 跳過第一個年份 # 繪製每一年的 PM25 折線圖 plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) # 在每一年折線圖的最後一點加上年份標籤 plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # 其他可選設定，例如 x 和 y 軸的範圍、刻度、標題等 # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) # 顯示圖表 plt.show() 在這張多曲線圖中，我們可以發現 2019 年的資料有大片段的缺失值，2022 年的資料只記載到本文寫作時的七月份，同時也可以發現在這四個年份的折線圖中，不同年份的曲線皆在夏季時達到最低點，秋季時 PM2.5 數值則開始攀升，並且在冬季時達到最高點，呈現約略相同的變化趨勢。\n日曆熱力圖 日曆熱力圖是一種結合日曆圖 (calendar map) 與熱力圖 (heat map) 的資料視覺化呈現方式，可以更直覺瀏覽資料的分佈狀況，並從中尋找不同時間尺度的規律性。我們使用 calplot 這個 Python 語言的日曆熱力圖套件，將每日的 PM2.5 平均值輸入後，再選擇搭配的顏色 (參數名稱為 cmap，在接下來的範例中，我們先設為 GnBu，詳細的顏色選項說明，可參閱參考資料)，便能得到下圖的效果，其中藍色代表數值較高，綠色或白色代表數值較低，如果沒有塗色或是數值為 0 則代表當天沒有資料。從產生的圖中，我們可以發現中間部分的月份（夏季）顏色較淡，左邊部分的月份（冬季）顏色#較深，恰與我們之前使用多曲線圖的觀察結果一致。\n# 使用 calplot 套件繪製日曆熱圖（Calendar Heatmap） # cmap: 設定呈現的顏色色盤 (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: 設定圖中數字呈現的樣式 pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") 資料品質檢測與處理 在時序資料的基本視覺化呈現後，我們接下來介紹資料品質的檢測與基本處理方法，我們將使用 kats 這個 Python 語言的資料處理與分析套件，並依序進行離群值偵測、改變點偵測與缺失資料處理。\n離群值偵測 (Outlier detection) 離群值指的是資料中某些數值與其他數值存在顯著的差異，這些差異可能會影響我們對資料的判斷與分析結果，因此需要將離群值找出來後予以標示、刪除、或特別處理。\n我們首先將原本使用 dataframe 資料格式的 air_hour 轉換成 kats 套件所使用的 TimeSeriesData 格式，並將轉換後的資料存成 air_ts 這個變數名稱。然後我們再次繪製原始時序資料的折線圖。\n# 使用 TimeSeriesData 類別將 air_hour 資料框轉換為時間序列資料 # time_col_name='timestamp' 指定時間欄位為 'timestamp' air_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') # 使用 plot 方法繪製 PM25 時間序列圖 # cols=[\"PM25\"] 表示只畫出 PM25 這一個欄位的資料 air_ts.plot(cols=[\"PM25\"]) 接著我們使用 kats 套件中的 OutlierDetector 工具偵測時序資料中的離群值。其中，離群值指的是小於第一四分位數 (Q1) 減 1.5 倍四分位距 (IQR) 或大於第三四分位數 (Q3) 加 1.5 倍四分位距的數值。\n# 創建 OutlierDetector 類別的實例，用於檢測異常值 # 使用 'additive' 方法來設定異常檢測的模型 outlierDetection = OutlierDetector(air_ts, 'additive') # 執行 detector 方法來開始異常檢測 outlierDetection.detector() # 查看檢測到的異常值，儲存在 outliers 屬性中 outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] 最後我們把偵測出來的離群值從原始資料中刪除，然後再次做圖並與一開始的折線圖進行比較，便可以很清楚地發現一些異常值（例如 2022-07 有一個異常的高峰) 都被移除了。\n# 使用 OutlierDetector 的 remover 函數移除異常值，這裡不進行內插（interpolate=False） outliers_removed = outlierDetection.remover(interpolate=False) # 顯示移除異常值後的數據 print(outliers_removed) # 繪製移除異常值後的數據，y_0 表示原始數據的第一個變數（通常是 y 軸上的數據） outliers_removed.plot(cols=['y_0']) 改變點偵測 (Change point detection) 改變點是資料中突然發生重大改變的時間點，代表的是事件的發生、資料狀態的轉變或資料分布的轉變，因此改變點偵測也常被視為資料分析與資料預測的重要前處理步驟。\n在以下的範例中，我們使用空品資料的日平均值來進行改變點偵測，同時也使用 kats 套件的 TimeSeriesData 資料格式來儲存資料，並使用 kats 提供的 CUSUMDetector 偵測器來進行偵測，並在作圖中用紅色的點來代表偵測到的改變點。很不湊巧的是，在本次的範例中一如肉眼觀察的結果，並無明顯的改變點存在，建議讀者可參考這次的範例，帶入其他的資料做更多的演練與偵測。\n# 將每日平均數據 air_day 轉換為 TimeSeriesData 物件，方便後續分析 air_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') # 使用 CUSUMDetector（累積和檢測器）來找出數據中的變化點 detector = CUSUMDetector(air_ts) # 執行 detector 方法，設定變化方向為「增加」和「減少」 change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # 可以用這行程式碼來打印出第一個變化點的時間 # print(\"The change point is on\", change_points[0][0].start_time) # 將結果繪製出來 # plot the results plt.xticks(rotation=45) # 旋轉 x 軸標籤方便觀看 detector.plot(change_points) plt.show() 缺失資料 (missing data) 處理 在我們進行資料分析時，常常不免會遇上缺失資料的問題，這些缺失資料有的是在資料收取時便已經缺失 （例如感測器故障、網路斷線等原因），有些則是在資料前處理時不得不把某些資料刪除（離群值或明顯異常值），但對於後續的資料處理與分析而言，我們又往往需要資料能維持固定的採樣率，以方便各項方法工具的套用，因此便衍生出各種不同的缺失資料填值方法。以下我們介紹三種常見的方法：\n將該筆缺失資料標示為 Nan (Not a number)：Nan 代表非數，用來表示未定義或不可表示的值，如果已知後續的資料分析會額外處理這些 Nan 的特例，便可採用此方法以維護資料的真實性。 Forward fill 法：如果 Nan 對後續的資料分析有困難，必須將缺失的值填補適當的數值資料，最簡單的方法就是 forward fill，亦即用前一個數值來填補當下的缺失值。 # 使用 forward fill（前向填充）方法填補空值，這個方法會用前一個非空數據點來填充後面的空值 df_ffill = air.ffill(inplace=False) # 繪製使用 forward fill 方法填補後的數據 df_ffill.plot() 3. K-Nearest Neighbor (KNN) 法：顧名思義，KNN 的方法是尋找距離缺失值最近的 k 個數值進行平均，並用來填補這個缺失值。\n# 定義 knn_mean 函數，使用 k-近鄰 (KNN) 方法來填補缺失的 PM25 值 def knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): # 檢查是否為 NaN n_by_2 = np.ceil(n/2) # k 的一半，方便找到左右鄰居 lower = np.max([0, int(i-n_by_2)]) # 算出左界 upper = np.min([len(ts)+1, int(i+n_by_2)]) # 算出右界 ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) # 擷取鄰近數據 out[i] = np.nanmean(ts_near) # 計算鄰近數據的平均值（忽略 NaN） return out # 使用 KNN 方法填補 PM25 缺失值 df_knn = air.copy() # 複製一份原始資料 df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) # 應用 KNN # 繪製圖表，顯示使用 KNN 填補後的 PM25 資料 df_knn.plot() 資料分解(Decomposition) 在前面的基本資料處理範例中，我們已能約略觀察資料數值的變化趨勢，並且發現可能的規律變化，為了能更近一步深入探討時序資料的變化規律性，我們接著介紹時序資料所常使用的資料分解分法，將原本的時序資料拆解成趨勢波 (trend)、週期波 (seasonal) 及殘差波(residual)。\n我們首先將空品資料的每日平均資料另外複製一份為 air_process，並採用 forward fill 方法進行缺失資料的處理，然後把原始資料直接用圖表的方式呈現。\n# 創建 air_process DataFrame，為 air_day 的複製版，方便後續操作 air_process = air_day.copy() # 使用前向填充（Forward Fill, ffill）來填補缺失的 PM25 值 # 前向填充就是用前一個有效數據點來填充後面的缺失值 air_process.ffill(inplace=True) # 繪製圖表，展示填充後的 PM25 數據 air_process.plot() 接著我們使用 seasonal_decompose 方法對 air_process 資料進行分解，其中我們需要設定一個 period 參數，指的是資料被拆解的週期，我們先設定為 30天，接著在執行後便會依序產出四張圖：原始資料、趨勢圖、週期性圖與殘差圖。\n# 使用 seasonal_decompose 進行季節分解，將 PM25 資料分為趨勢（trend）、季節性（seasonal）和殘差（residual）三部分 # model='additive' 指定使用加法模型，period=30 表示每 30 天為一個季節週期 decompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) # 繪製季節分解的結果，並設定圖表大小 decompose.plot().set_size_inches((15, 15)) # 顯示圖表 plt.show() 在趨勢圖 (trend) 中，我們也可以發現與原始資料的圖表有著十分雷同的特性，在一月附近的數值較高，七月附近的數值則較低；而在週期性圖 (Seasonal) 中，我們可以發現資料在每個週期 (30天) 內存在固定的週期變化，代表空品資料存在著以月為週期的變動。\n倘若我們將 period 變數改為 365，亦即以較大的時間尺度 (一年) 來進行資料分解，我們可以從週期性圖中發現一月附近數值較高，而七月附近數值較低的趨勢，而且這個趨勢變化是規律地週期性發生的；同時，在趨勢圖中也可以看到整體緩降的趨勢，說明 PM2.5 的濃度在大趨勢下是逐漸好轉降低的，從這邊也可以理解到為什麼在尋找 change point 時無法成功，因為 PM2.5 的趨勢變化是平順的遞減，並沒有發生突如其來的變化。\n# 使用 seasonal_decompose 進行季節分解，這次 period 設為 365，即一年的天數 # 這樣可以觀察到 PM25 在一整年內的變化趨勢和季節性 decompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) # 繪製季節分解的結果，並設定圖表大小為 15x15 decompose.plot().set_size_inches((15, 15)) # 顯示圖表 plt.show() 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與殘差波動，同時套用既有的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料，並探討其背後所可能隱含的意義。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "4.1. 時間序列資料處理",
    "uri": "/ch4/ch4.1/"
  },
  {
    "content": "\nTable Of Contents 交集 (Intersect) 緩衝區 (Buffer) 多重緩衝區 (Multi-ring buffer) 距離矩陣 (Distance Matrix) 小結 民生公共物聯網的測站都有其空間位置，由於同一區域內常有類似的環境因子，所以它們的感測數值也會具有類似的起伏趨勢，而這也就是地理學的第一定律：“All things are related, but nearby things are more related than distant things.” (Waldo R. Tobler)\n此外，單一測站的感測數據有可能因為局部干擾因子的影響，而產生較大的起伏，所以為進一步確認數據的可信度，我們會需要以單一測站為中心，依照其所屬的行政區或是指定距離 (半徑)，選取鄰近的測站ID與數值，並將其以表單或地圖的方式呈現，以便進行比對。\n這個章節中，我們會利用環保署的空品測站、氣象局的局屬測站以及水利署在各縣市佈建的淹水感測器，示範如何利用地理空間篩選測站，並以其「位置」與「數值」為基礎，轉化成可利用的空間資訊。\n# 引入 matplotlib、seaborn 等繪圖模組 import matplotlib.pyplot as plt import seaborn as sns # 引入 pandas 和 numpy 進行數據分析 import pandas as pd import numpy as np # 引入 urllib.request 來處理網路資源 import urllib.request # 引入 ssl 和 json 來處理安全連線和 JSON 格式 import ssl import json # 安裝 geopython 相關套件，主要是用在地理資料處理 !apt install gdal-bin python-gdal python3-gdal # 安裝 python3-rtree，這是 Geopandas 的需求 !apt install python3-rtree # 安裝 Geopandas，用於處理地理資料 !pip install geopandas # 安裝 descartes，也是 Geopandas 的需求 !pip install descartes # 引入 geopandas 模組 import geopandas as gpd # 安裝 pyCIOT 模組，這是專門用於 CIoT 數據處理的 !pip install pyCIOT # 引入 pyCIOT 的數據處理模組 import pyCIOT.data as CIoT # 前往政府開放資料庫下載 縣市界線(TWD97經緯度) 的資料，並解壓縮到名為 shp 的資料夾 # 使用 wget 指令下載一個名為 \"shp.zip\" 的壓縮檔，檔案來源網址是內政部開放資料平台 !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" # 使用 unzip 指令解壓縮 \"shp.zip\"，並將解壓縮的內容存到 \"shp\" 資料夾 !unzip shp.zip -d shp # 從 pyCIOT 取得水利署淹水感測器資料和天氣資料 wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") wea_list = CIoT.Weather().get_station('GENERAL:CWB') # 讀取台灣縣市邊界的 shapefile county = gpd.read_file('county.shp') # 篩選出嘉義縣和嘉義市的資料作為底圖 basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] # 合併淹水感測器的資料並轉成 DataFrame 格式 flood_list = wa + wa2 flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) # 迴圈處理每個感測器的資料 for i in flood_list: if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else: df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) # 去除重複站點並排序 result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) # 轉成 GeoDataFrame 格式，方便地理資訊處理 gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") # 同樣地，整理天氣資料 weather_df = pd.DataFrame([],columns = ['name','lon', 'lat']) for i in wea_list: df = pd.DataFrame([[i['name'],i['location']['longitude'],i['location']['latitude']]],columns = ['name','lon', 'lat']) weather_df = pd.concat([weather_df,df]) # 去除重複站點並排序，然後轉成 GeoDataFrame result_df = weather_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) gdf_weather = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") 交集 (Intersect) 一般而言，手邊有很多的測站點位時，我們可以利用村里或鄉鎮等行政區界為範圍，利用資料的在空間上的交集 (intersect) 以篩選出特定行政區內的測站ID，並以API擷取這些測站的：瞬時值、小時平均值、日平均值、週平均值，這樣我們就可以檢視同一行政區內的測站，是否有類似的數值趨勢，抑或哪些測站的數值與其他測站有明顯的差異。\n# 引入 matplotlib 和 seaborn 用於繪圖 import matplotlib.pyplot as plt import seaborn as sns # 設置畫布和子圖大小 fig, ax = plt.subplots(figsize=(6, 10)) # 使用 seaborn 的 scatterplot 功能，繪製氣象站的經緯度 ax = sns.scatterplot(x='lon', y='lat', data=gdf_weather) # 使用 Geopandas 的 plot 功能，繪製嘉義縣和嘉義市的邊界 basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # 調整畫布的排版 plt.tight_layout(); # 將 basemap 的坐標系統設為 EPSG:4326，這樣才能和 gdf_weather 進行操作 basemap = basemap.set_crs(4326, allow_override=True) # 使用 Geopandas 的 overlay 函式來找出和嘉義縣、嘉義市邊界重疊的氣象站 intersected_data = gpd.overlay(gdf_weather, basemap, how='intersection') # 設置畫布和子圖大小 fig, ax = plt.subplots(figsize=(6, 10)) # 繪製過濾後，只在嘉義縣和嘉義市內的氣象站位置 ax = sns.scatterplot(x='lon', y='lat', data=intersected_data) # 繪製嘉義縣和嘉義市的邊界 basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # 調整畫布的排版 plt.tight_layout(); 緩衝區 (Buffer) 此外，部分測站也有可能是位在兩個行政界的邊界處，所以用行政界當成選取的標準，就有可能出現偏誤。所以這種狀況下，我們可以利用緩衝區 (buffer) 的概念，以測站的座標位置為中心，並指定一個距離半徑以建立一個虛擬的圓形 (圖1)，並以此範圍去查找有幾何相交的測站位置。\n當我們掌握了緩衝區的概念後，我們也可以某個地標 (例如：學校、公園、工廠…）標為中心，去查找鄰近的測站。甚至，我們可以用一個線段 (Line 例如：道路、河流…) 或一個區域 (polygon 例如：公園、工業區)，去建立一個查找範圍，以更具體地找出自己想要的測站點位。在這個範例中，我們以氣象局的局屬氣象站為中心，去建立緩衝區。\n# 設置畫布和子圖大小 fig, ax = plt.subplots(figsize=(6, 10)) # 使用 Geopandas 的 buffer 函式建立緩衝帶，這邊緩衝帶的寬度設為 0.05 buffer = intersected_data.buffer(0.05) # 繪製緩衝帶，透明度設為 0.5 buffer.plot(ax=ax, alpha=0.5) # 繪製過濾後的氣象站位置，顏色設為紅色，透明度為 0.5 intersected_data.plot(ax=ax, color='red', alpha=0.5) # 繪製嘉義縣和嘉義市的邊界 basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # 調整畫布的排版 plt.tight_layout(); 多重緩衝區 (Multi-ring buffer) 當然，我們也可以設定不同的距離半徑 ，進而把緩衝區畫成多個同心圓 (圖2)，這樣就可以利用不同的遠近/階層，以將鄰近的測站分組，從而檢視越鄰近的測站是否有越相近的數值趨勢。在這個案例中，我們可以用氣象局的局屬氣象站去建立多重緩衝區，並檢視不同的緩衝區內的水利署淹水感測器，從而探勘：雨量、水平距離與淹水高度之間的關係。\n# 利用不同的經緯度作為半徑以建立多重緩衝區，並依照 0.05度=藍色；0.1度=綠色；0.2度=橘色；0.3=紅色 進行顏色設定 # 設置畫布和子圖大小 fig, ax = plt.subplots(figsize=(6, 10)) # 建立不同寬度的緩衝帶，並分別以不同顏色繪製 buffer_03 = intersected_data.buffer(0.3) buffer_03.plot(ax=ax, color='red', alpha=1) buffer_02 = intersected_data.buffer(0.2) buffer_02.plot(ax=ax, color='orange', alpha=1) buffer_01 = intersected_data.buffer(0.1) buffer_01.plot(ax=ax, color='green', alpha=1) buffer_005 = intersected_data.buffer(0.05) buffer_005.plot(ax=ax, alpha=1) # 繪製原始交集點，用黑色表示 intersected_data.plot(ax=ax, color='black', alpha=0.5) # 建立最大緩衝帶（0.3）的 GeoDataFrame，然後和淹水感測器進行空間交集運算 buffer = gpd.GeoDataFrame(buffer_03, geometry=buffer_03) buffer = buffer.to_crs(4326) intersected_flood = gpd.overlay(gdf_flood, buffer, how='intersection') # 繪製交集後的淹水感測器位置，用淺灰色表示 intersected_flood.plot(ax=ax, color='lightgray', alpha=0.5) # 繪製嘉義縣和嘉義市的邊界 basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # 調整畫布的排版 plt.tight_layout(); 距離矩陣 (Distance Matrix) 最後，因為每一個測站都有具體的座標數值，所以我們也可以三角函式結合座標數值，取得兩兩測站間的絕對距離，從而建立所有測站間的距離矩陣 (distance matrix)，而這樣的矩陣可以協助我們快速檢視兩兩測站間的距離關係，並從中確認測站是否具有鄰近性，從而以更一步確認感測間的數值趨勢是否與其距離具有相關性 (圖3)。在這個範例中，我們可以利用台北市內的淹水感測器位置，並將其轉換成為距離矩陣，以協助我們掌握淹水是否具有鄰近關係。\n# 為了製作距離矩陣，先將 gdf_weather 資料整理一下，新增一個 'ID' 欄位來存放 index gdf_weather[\"ID\"] = gdf_weather.index # 挑選出需要的欄位（'ID', 'lon', 'lat'），並建立一個新的 DataFrame df = pd.DataFrame(gdf_weather, columns=['ID','lon', 'lat']) # 將 'ID' 設定為 DataFrame 的 index df.set_index('ID', inplace=True) # 引入 scipy 的 distance_matrix 函式來計算距離 from scipy.spatial import distance_matrix # 使用 distance_matrix 函式計算 df（感測器的經緯度）彼此之間的距離 # 輸出會是一個距離矩陣，然後再轉成 DataFrame 格式 distance_df = pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index) 小結 透過前述的方式，我們可以從地理空間 (行政區或鄰近性) 進行測站的篩選機制，並從而能從地理空間的特性檢視測站間的數值相關性。\nReferences Geopanda Documentation (https://geopandas.org/en/stable/docs.html) Shpfile 格式介紹 (https://en.wikipedia.org/wiki/Shapefile) 臺灣的大地基準及座標系統 (https://wiki.osgeo.org/wiki/Taiwan_datums) WGS 84 (EPSG:4326) 參數介紹 (https://epsg.io/4326) TWD 97 (EPSG:3826) 參數介紹 (https://epsg.io/3826) ",
    "description": "我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "5.1. 地理空間篩選",
    "uri": "/ch5/ch5.1/"
  },
  {
    "content": "\nTable Of Contents 基本介紹 分類問題 (Classification) 分群問題 (Clustering) 套件安裝與引用 案例一：空品資料的場所型態分類 資料清理 移除離群值 區別訓練資料與測試資料 使用 Sklearn 預建模型 案例二：空品資料的分群 資料下載與前處理 動態時間校正 (Dynamic Time Warping, DTW) 使用 K-Mean 分群演算法 探究資料分群與地理位置的關係 探究資料分群與風場風向的關係 案例三：結合氣象與水資源資料的分類與分群 資料下載與前處理 計算特定淹水感測器與雨量測站資料的相似度 資料分群並探究相關性高的雨量站 資料分類並由雨量計資料預測淹水可能性 參考資料 基本介紹 我們在之前的章節中，已經介紹民生公共物聯網豐富的開放資料內容，同時介紹從時間維度與空間維度的角度出發，進行各種不同的資料分析與處理，在這個章節中，我們將進一步初探機器學習的應用，介紹兩個經典的機器學習問題，分別是分類問題 (Classification) 與分群問題 (Clustering)。\n分類問題 (Classification) 分類問題是機器學習理論中的一個經典問題，若用比較數學的方式來描述這個問題，我們可以假設有一組已經分類好的數據 X，以及每一筆資料在分類之後所得到的標籤集合 Y，而分類問題就是希望能透過這組分類好的數據與標籤，建構一個有效的分類器 (Classifier)，可以將尚未分類的數據 X’，並找到其中每一筆資料相對應的標籤 Y’。\n因此，分類問題的重點，就是要構造一個有效的分類器 (Classifier)，為了達到這個目的，我們會先建立一個模型，利用已經標籤好 (labeled) 的數據進行訓練，並且讓這個模型盡可能地貼近與適應 (fit) 這些數據分佈狀態，完成訓練後，我們再利用此模型作為分類器，用來推測未知數據的標籤。\n這個建立分類器的過程，在機器學習中被稱為監督式學習 (supervised learning)，而常見的分類器模型有 Nearest Neighbors、SVM Classifier、Decision Tree、*Random Forest *****等，在我們稍後的文章中，我們並不會針對每一種模型進行深入的講解，只會把這些模型直接拿來當作工具使用，對於這些模型有興趣的讀者，可以參考相關資源，自行再做更深入的探究。\n分群問題 (Clustering) 分群問題和分類問題非常相似，主要差異在於分類問題是用已知的標籤數據資料推論未知的數據資料，而分群問題則是完全「無中生有」，在沒有事先標籤的情況下，根據數據的特徵自動將它們歸類出不同的群組。\n若以比較數學的方式來描述這個問題，我們可以假設有一組完全沒有標記的數據 X，而分類問題就是希望能透過某種演算法，將 X 的資料區分為 k 個群組，其中每一個群組內的資料彼此相似度大，而不同群組的資料彼此相異度大。\n因此，分群問題的演算法主要就是根據數據的特性，不斷地判斷資料間的相似與相異度，並且讓相似的資料群聚在一起，讓相異的資料在分佈中彼此互斥，而常見的分群演算法有 K-Means、DBSCAN、Hierarchical Clustering、BIRCH ******等，我們一樣不會針對每一種演算法進行深入的講解，只會把這些演算直接拿來當工具使用，對於這些模型有興趣的讀者，可以參考相關資源，自行再做更深入的探究。\n套件安裝與引用 在本章節中，我們除了使用 pyCIOT 套件獲取相關的民生公共物聯網開放資料外，將會使用到 pandas, numpy, matplotlib, json, os, glob, math, seaborn, warnings, tqdm, datetime, geopy, scipy 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 上並未預先安裝好的套件：fastdtw 和 sklearn，以及為了讓輸出的圖形美觀而使用 TaipeiSansTCBeta-Regular 字型，需使用下列的方式進行安裝：\n# 安裝 fastdtw 套件，用於快速動態時間扭曲。 !pip3 install fastdtw --quiet # 安裝 scikit-learn 套件，用於機器學習。 !pip3 install scikit-learn --quiet # 安裝 pyCIOT 套件。 !pip3 install pyCIOT --quiet # 從 Google Drive 下載 TaipeiSansTCBeta-Regular.ttf 字型檔案。 !wget -q -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\u0026export=download 待安裝完畢後，即可使用下列的語法先行引入相關的套件模組，完成本章節的準備工作：\n# 引入 pyCIOT 模組中的 data 模組。 from pyCIOT.data import * # 引入常用的模組。 import json, os, glob, math import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt # 引入字型管理相關模組，用於設定圖表字型。 from matplotlib.font_manager import fontManager from tqdm import tqdm_notebook as tqdm # 引入日期時間相關模組。 from datetime import datetime, timedelta # 引入 seaborn 資料視覺化套件，並設定字型大小。 import seaborn as sns sns.set(font_scale=0.8) # 新增字型，並設定 matplotlib 的預設字型。 fontManager.addfont('TaipeiSansTCBeta-Regular.ttf') mpl.rc('font', family='Taipei Sans TC Beta') # 隱藏警告訊息。 import warnings warnings.simplefilter(action='ignore') # 引入距離計算模組及 fastdtw 套件。 import geopy.distance from scipy.spatial.distance import euclidean from fastdtw import fastdtw # 引入 scikit-learn 相關模組，用於機器學習。 import sklearn from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.cluster import KMeans 安裝與引入所需模組完畢後，我們接下來依照民生公共物聯網的空氣品質資料與水資源資料兩個案例解析，介紹資料分類與分群。\n案例一：空品資料的場所型態分類 在這個案例中，我們使用民生公共物聯網資料平台中，環保署智慧城鄉空品微型感測器 (‘OBS:EPA_IoT’) 的資料，進行資料分類的示範。\n我們首先使用 pyCIOT 所提供的 Air().get_data() 方法，下載取得所有環保署智慧城鄉空品微型感測器的最近一筆感測資料。注意，由於內容數量龐大，因此這個步驟可能需要等待較長的時間。\n# 定義資料來源為環保署智慧城鄉空品微型感測器 (OBS:EPA_IoT)。 Source = 'OBS:EPA_IoT' # 從 Air() 中獲取資料。 Data = Air().get_data(src=Source) # 過濾 Data 資料，只保留 data 長度為 3 且 properties 中有 'areaType' 的資料點。。 Data = [datapoint for datapoint in Data if len(datapoint['data']) == 3 and 'areaType' in datapoint['properties'].keys()] # 輸出過濾後的第一筆資料，以 JSON 格式顯示，且縮排為 4。 print(json.dumps(Data[0], indent=4, ensure_ascii=False)) { \"name\": \"智慧城鄉空品微型感測器-10287974676\", \"description\": \"智慧城鄉空品微型感測器-10287974676\", \"properties\": { \"city\": \"新北市\", \"areaType\": \"社區\", \"isMobile\": \"false\", \"township\": \"鶯歌區\", \"authority\": \"行政院環境保護署\", \"isDisplay\": \"true\", \"isOutdoor\": \"true\", \"stationID\": \"10287974676\", \"locationId\": \"TW040203A0507221\", \"Description\": \"廣域SAQ-210\", \"areaDescription\": \"鶯歌區\" }, \"data\": [ { \"name\": \"Relative humidity\", \"description\": \"相對溼度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 70.77 }, { \"name\": \"Temperature\", \"description\": \"溫度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 33.78 }, { \"name\": \"PM2.5\", \"description\": \"細懸浮微粒 PM2.5\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 9.09 } ], \"location\": { \"latitude\": 24.9507, \"longitude\": 121.3408416, \"address\": null } } 我們可以發現每一個感測器的資料中，都同時有溫度、相對濕度、細懸浮微粒濃度等感測資料，同時也記載了感測器的基本資訊，例如所在城市、所在鄉鎮、機器編號、位置編號、場所型態等。在我們這個範例中，我們將以「場所型態」為標籤資料，並搭配感測資料（溫度、相對濕度、細懸浮微粒濃度）進行分類器的訓練。我們首先觀察目前「場所型態」的內容狀態。\n# 從 Data 資料集中取得所有不重複的 'areaType' 標籤。 Label = list(dict.fromkeys([datapoint['properties']['areaType'] for datapoint in Data if datapoint['properties']['areaType']])) # 初始化一個計數器，用於計算每個 'areaType' 的出現次數。 count = dict.fromkeys(Label, 0) # 迴圈遍歷 Data 資料集，計算每個 'areaType' 的出現次數。 for datapoint in Data: count[datapoint['properties']['areaType']] += 1 # 輸出資料清理前的記錄總數。 print(\"Before data cleaning, There are {} records.\".format(len(Data))) # 以 JSON 格式輸出每個 'areaType' 的計數結果，且縮排為 4。 print(json.dumps(count, indent=4, ensure_ascii=False)) Before data cleaning, There are 8620 records. { \"社區\": 223, \"交通\": 190, \"一般社區\": 2021, \"工業\": 12, \"測站比對\": 66, \"工業區\": 3333, \"交通區\": 683, \"鄰近工業區社區\": 948, \"輔助區\": 165, \"特殊區(民眾陳情熱區)\": 143, \"特殊區(敏感族群聚集區)\": 200, \"特殊區(測站比對)\": 32, \"輔助區(無測站區)\": 4, \"工業感測\": 196, \"特殊感測\": 4, \"輔助感測\": 295, \"交通感測\": 102, \"機動感測\": 2, \"社區感測\": 1 } 資料清理 由於目前資料中共有 8620 筆資料散佈在 19 種場所型態中，為了符合資料的意義，我們首先將類似的場所型態進行合併，同時也為了讓分類器的示範能更順利，我們只專注在一般社區、交通區、工業區、鄰近工業區社區等四大場所型態。因此，我們用下列的程式進行資料的重新整理：\n# 迴圈遍歷 Data 資料集，對 'areaType' 進行資料清理和重新命名。 for datapoint in Data: if datapoint['properties']['areaType'] == '社區': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '社區感測': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '交通': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '交通感測': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '工業': datapoint['properties']['areaType'] = '工業區' elif datapoint['properties']['areaType'] == '工業感測': datapoint['properties']['areaType'] = '工業區' # 若 'areaType' 不在指定的四種類別中，則設為 None。 if not datapoint['properties']['areaType'] in ['一般社區', '交通區', '工業區', '鄰近工業區社區']: datapoint['properties']['areaType'] = None # 過濾資料，去除 'areaType' 為 None 的資料點。 Data = [datapoint for datapoint in Data if datapoint['properties']['areaType'] != None] # 定義我們希望計數的標籤。 Label = ['一般社區', '交通區', '工業區', '鄰近工業區社區'] # 初始化一個計數器，用於計算每個 'areaType' 的出現次數。 count = dict.fromkeys(Label, 0) # 迴圈遍歷 Data 資料集，計算每個 'areaType' 的出現次數。 for datapoint in Data: count[datapoint['properties']['areaType']] += 1 # 輸出資料清理後的記錄數量。 print(\"After data cleaning, There are {} records.\".format(len(Data))) # 以 JSON 格式顯示每個 'areaType' 的計數結果，且縮排為 4。 print(json.dumps(count, indent=4, ensure_ascii=False)) After data cleaning, There are 7709 records. { \"一般社區\": 2245, \"交通區\": 975, \"工業區\": 3541, \"鄰近工業區社區\": 948 } 經過資料清理後，總共剩下 7709 筆資料，並且散佈在四大類的場所型態中。針對這些資料，我們接著考慮每筆資料的溫度、相對濕度、細懸浮微粒濃度感測值，並用不同的顏色代表不同場所型態的資料，繪製成一張三維的資料分佈圖。\n# 初始化存放資料的列表。 DataX, DataY = [], [] # 迴圈遍歷 Data 資料集，從每一個資料點中提取特徵和標籤。 for datapoint in Data: TmpX = [None]*3 # 初始化暫存特徵列表。 TmpY = None # 初始化暫存標籤值。 for rawdata_array in datapoint['data']: # 提取資料點名稱為 'Temperature' 的值。 if(rawdata_array['name'] == 'Temperature'): TmpX[0] = rawdata_array['values'][0].get('value') # 提取資料點名稱為 'Relative humidity' 的值。 if(rawdata_array['name'] == 'Relative humidity'): TmpX[1] = rawdata_array['values'][0].get('value') # 提取資料點名稱為 'PM2.5' 的值。 if(rawdata_array['name'] == 'PM2.5'): TmpX[2] = rawdata_array['values'][0].get('value') # 將 'areaType' 轉換為整數索引。 TmpY = Label.index(datapoint['properties']['areaType']) # 將暫存的特徵和標籤加入至 DataX 和 DataY 列表。 DataX.append(TmpX) DataY.append(TmpY) # 將 DataX 和 DataY 列表資料轉換為 Numpy 陣列。 DataX_Numpy = np.array(DataX) DataY_Numpy = np.array(DataY) # 設定圖例字體大小。 plt.rc('legend',fontsize=\"xx-small\") # 創建一個 8x6 大小，150 dpi 的圖形。 fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') # 迴圈遍歷每一種標籤，並在 3D 空間中繪製對應的資料點。 for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) # 顯示圖例。 ax.legend() # 設定 X 軸標籤為 Temperatur， y 軸標籤為 Relative humidity， z 軸標籤為 PM2.5。 ax.set_xlabel('Temperature') ax.set_ylabel('Relative humidity') ax.set_zlabel('PM2.5') # 顯示圖形。 plt.show() 從資料分布圖中，我們可以發現絕大多數的資料都群聚在某一個特定的空間內，但仍有少數的資料零散地分布在很外圍的地方，這些遠離群體的資料我們稱為離群值 (Outlier)，對於資料分類或資料分群而言，離群值很容易導致我們的模型或演算法走向極端化，因而失去其通用性，因此我們需要先將這些資料予以移除。\n移除離群值 移除離群值的方法，不外乎運用一些資料的統計特徵，可以依據不同的應用情境需求自行定義。在我們的範例中，我們定義如果一筆資料在其中一種感測資料的數值，距離平均值超過兩個標準差以上，該筆資料便歸類為離群值。我們將這些離群值的資料移除後，照例繪製一張三維空間的分佈圖觀察其分布狀況。\n\"\"\" 函數用於過濾多維陣列中的極端值。 輸入: arr: Numpy 二維陣列，其中每一行代表一個數據點，每一列是一個特徵。 k: 一個常數，用於設定極端值過濾的閾值。 輸出: Boolean_Arr: 一個布林陣列，True 表示對應的數據點不是極端值，False 表示它是極端值。 \"\"\" def Outlier_Filter(arr, k): Boolean_Arr = np.ones(arr.shape[0], dtype=bool) for i in range(arr.shape[1]): Boolean_Arr = Boolean_Arr \u0026 (abs(arr[:,i] - np.mean(arr[:,i])) \u003c k*np.std(arr[:,i])) return Boolean_Arr # 使用 Outlier_Filter 函數找出不是極端值的數據點。 OutlierFilter = Outlier_Filter(DataX_Numpy, 2) DataX_Numpy = DataX_Numpy[OutlierFilter] DataY_Numpy = DataY_Numpy[OutlierFilter] # 輸出過濾後的數據點數量。 print(\"After removing Outliers, there are {} records left.\".format(DataX_Numpy.shape[0])) # 設定圖例字體大小並畫出 3D 散點圖。 plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature') ax.set_ylabel('Relative humidity') ax.set_zlabel('PM2.5') plt.show() After removing Outliers, there are 7161 records left. 從最後呈現的結果中，我們共移除了 7709 - 7161 = 548 筆離群值資料，而最後留下的資料在三維空間的分佈也較為集中，不再有偏移在外圍的狀況發生。為了更容易觀察，我們以每次挑選兩個維度的方式，分別繪製三張資料在不同維度間的分佈狀況。\n# 設定圖例的字體大小為大號。 plt.rc('legend',fontsize=\"large\") # 創建一個1x3的子圖，每個子圖大小為24x6。 fig, axes = plt.subplots(1,3,figsize=(24, 6)) # 定義各特徵的標籤。（ChatGPT移到這裡？） # Axis_label = ['Temperature', 'Relative humidity', 'PM2.5'] # 迴圈遍歷每一個特徵。 for i in range(DataX_Numpy.shape[1]): # 迴圈遍歷每一種標籤。 for j in range(len(Label)): # 畫出2D散點圖。 axes[i].scatter(DataX_Numpy[DataY_Numpy==j][:,i%3],DataX_Numpy[DataY_Numpy==j][:,(i+1)%3], s=1, label=Label[j]) # 設定圖例的位置在左上角。 axes[i].legend(loc=2) # 定義各特徵的標籤。（ChatGPT移到迴圈外？） Axis_label = ['Temperature', 'Relative humidity', 'PM2.5'] # 設定X軸和Y軸的標籤。 axes[i].set_xlabel(Axis_label[i%3]) axes[i].set_ylabel(Axis_label[(i+1)%3]) # 調整子圖之間的間隔。 plt.tight_layout() 從這三張圖中，我們已可發現不同顏色（場所型態）的資料隱約中似乎存在某種關係，雖然用肉眼很難直接敘明，但接下來我們將帶入分類模型來建構專屬的分類器。\n區別訓練資料與測試資料 在進入分類器的模型訓練前，我們還有一個步驟需要處理，那就是拆分數據，將現有的資料集分為訓練資料和測試資料。顧名思義，訓練資料將被用於調校分類器的模型，而測試資料則是用來測試所建構出來的分類器，在處理新資料時的效果。我們使用下列的範例程式，將資料集按照 4:1 的比例，切割成訓練資料與測試資料。\n# 隨機排列 DataX_Numpy 的索引。 indices = np.random.permutation(DataX_Numpy.shape[0]) # 取得訓練集的索引（佔總數的 80%），測試集的索引（從第 80 個開始，佔總數的 20%）。 Train_idx, Test_idx = indices[:int(DataX_Numpy.shape[0]*0.8)], indices[80:(DataX_Numpy.shape[0] - int(DataX_Numpy.shape[0]*0.8))] # 根據上述索引，從 DataX_Numpy 選取訓練集和測試集的特徵 TrainX, TestX = DataX_Numpy[Train_idx,:], DataX_Numpy[Test_idx,:] # 根據上述索引，從 DataY_Numpy 選取訓練集和測試集的標籤 TrainY, TestY = DataY_Numpy[Train_idx], DataY_Numpy[Test_idx] 使用 Sklearn 預建模型 我們直接使用 Scikit learn (sklearn) 這個 Python 套件所提供的分類器模型來進行訓練與測試，在系列的程式範例中，我們總共使用 Nearest neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, Neural Net, Adaboost, Naive Bayes, QDA 共計九種模型，我們依次帶入訓練資料進行調校後，接著帶入測試資料進行預測，並且將測試資料與預測結果中的標籤內容進行比對，並用混淆矩陣 (confusion matrix) 的方式，呈現不同標籤組合的分類結果。\n# 定義分類器的名稱列表。 classifier_names = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] # 定義對應的分類器模型。 classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] # 建立 3x3 的子圖，用於顯示每個模型的混淆矩陣。 fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) # 迴圈遍歷每一個分類器模型。 for i, model in enumerate(classifiers): # 使用訓練集資料訓練模型。 model.fit(TrainX, TrainY) # 使用測試集資料進行預測。 Result = model.predict(TestX) # 計算混淆矩陣。 mat = confusion_matrix(TestY, Result) # 以熱度圖顯示混淆矩陣。 sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=Label, yticklabels=Label, ax = axes[i//3][i%3]) # 設定子圖標題，顯示模型名稱及準確率。 axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(classifier_names[i], round(accuracy_score(Result, TestY), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) # 調整子圖之間的間距。 plt.tight_layout() 在這九種分類模型的分類結果中，我們發現 RBF SVM 可以達到將近 7成的分類成功率，事實上，這只是我們使用原始資料尚未進行更進一步處理分析的結果，讀者們若對於分類器有興趣，可以參考相關資源，進行更深入的探究，將能更加提升分類器對不同種類資料的分類能力。\n案例二：空品資料的分群 在這個案例中，我們使用民生公共物聯網開放資料中的環保署國家空品測站的感測資料，並且透過歷史資料的分析，利用資料分群的方式，將這些空品測站依照感測資料的變化趨勢關係予以分群，讓每一群的測站具有類似的感測資料變化趨勢。\n資料下載與前處理 我們使用下列的程式碼，從民生公共物聯網開放資料平台的歷史資料庫中下載 2021 年環保署國家空品測站的所有感測資料，並將下載回來的壓縮檔解開後，置於 /content 的目錄下。\n# 下載感測資料。 !wget -O 'EPA_OD_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FnkrDkv53nvbJf5ZyL5a6256m65ZOB5ris56uZL0VQQV9PRF8yMDIxLnppcA%3D%3D\" # 解開壓縮檔，置於 '/content' 的目錄下。 !unzip -q 'EPA_OD_2021.zip' \u0026\u0026 rm 'EPA_OD_2021.zip' !unzip -q '/content/EPA_OD_2021/EPA_OD_202112.zip' -d '/content' !rm -rf '/content/EPA_OD_2021' 我們先選用 2021 年 12月的資料，先把不需要的欄位 Pollutant、SiteId、Status、SO2_AVG 刪除，並且將感測資料的數值資料型態改為浮點數，以利後續的處理。\n# 讀取 CSV 檔案，並解析 'PublishTime' 欄位為日期時間格式。 Dataframe = pd.read_csv(\"/content/EPA_OD_202112.csv\", parse_dates=['PublishTime']) # 移除不需要的欄位。 Dataframe = Dataframe.drop(columns=[\"Pollutant\", \"SiteId\", \"Status\", \"SO2_AVG\"]) # 取得所有數值型態的欄位名稱。 Numerical_ColumnNames = list(Dataframe.columns.values) # 從列表中移除非數值型態的欄位名稱。 for ColumnName in ['SiteName', 'County', 'PublishTime']: Numerical_ColumnNames.remove(ColumnName) # 將數值型態的欄位轉換為浮點數格式，並對無法轉換的值設為 NaN。 for Numerical_ColumnName in Numerical_ColumnNames: Dataframe[Numerical_ColumnName] = pd.to_numeric(Dataframe[Numerical_ColumnName], errors='coerce').astype('float64') # 移除所有含有 NaN 的資料列。 Dataframe = Dataframe.dropna() # 顯示數據的前五行。 Dataframe.head() 由於一個月的資料量十分龐大，為了精簡範例程式的執行時間，我們抽取其中 2021-12-13 至 2021-12-17 共計五天的資料 (FiveDay_Dataframe) 作為接下來的範例，並且把 Country 與 SiteName 兩個欄位合併，同時根據合併後的欄位和資料發佈時間進行排序。\n# 篩選出 'PublishTime' 在 2021-12-13 到 2021-12-17 之間的數據。 FiveDay_Dataframe = Dataframe.loc[(Dataframe['PublishTime'] \u003c= '2021-12-17 23:00:00') \u0026 (Dataframe['PublishTime'] \u003e= '2021-12-13 00:00:00')] # 將 'County' 和 'SiteName' 的內容結合，形成一個新的欄位 'CountyAndSiteName'。 FiveDay_Dataframe['CountyAndSiteName'] = FiveDay_Dataframe['County'] + FiveDay_Dataframe['SiteName'] # 移除 'County' 和 'SiteName' 這兩個欄位。 FiveDay_Dataframe = FiveDay_Dataframe.drop(columns=[\"County\", \"SiteName\"]) # 根據 'CountyAndSiteName' 和 'PublishTime' 進行排序。 FiveDay_Dataframe = FiveDay_Dataframe.sort_values(by=['CountyAndSiteName','PublishTime']) # 將 'CountyAndSiteName' 設為索引。 FiveDay_Dataframe = FiveDay_Dataframe.set_index(keys = ['CountyAndSiteName']) # 顯示 DataFrame。 FiveDay_Dataframe 動態時間校正 (Dynamic Time Warping, DTW) 接下來，我們必須判斷兩個測站之間的「相似度」，並且將其量化成一個數字。最基本的相似度量測方式，是直接將兩測站的資料，按照感測時間對齊後，計算兩兩之間空氣汙染物質感測數據的差距；但是，若考量測站資料具備時間序列的特性，空氣污染在各個測站間可能發生的順序不一，影響的時間長度亦不一定相同，需要更有彈性的推估兩個測站之間的相似度，因此我們先扣除掉資料中的風向、感測時間、經緯度等資訊後，選用動態時間校正 (DTW) 方法進行相似度量測，若兩個測站資料的 DTW 距離越小，代表兩者的相似度越高。\n# 初始化一個字典，用來儲存每個測站的時間序列資料。 Site_TimeSeriesData = dict() # 針對每個測站提取其時間序列資料。 for Site in np.unique(FiveDay_Dataframe.index.values): tmp = FiveDay_Dataframe[FiveDay_Dataframe.index == Site] tmp = tmp.groupby(['CountyAndSiteName', 'PublishTime'], as_index=False).mean() tmp = tmp.loc[:,~tmp.columns.isin(['CountyAndSiteName', 'PublishTime'])] Site_TimeSeriesData[Site] = tmp.to_numpy() # 獲取測站的名稱。 DictKeys = Site_TimeSeriesData.keys() # 初始化一個字典，用來儲存每兩個測站之間的 DTW 距離。 Sites_DTW = dict() for i, key1 in enumerate(DictKeys): for j, key2 in enumerate(DictKeys): if i \u003e= j: continue else: # 計算扣除風向、感測時間、經度、緯度後資料的 DTW 距離。 Sites_DTW[str(key1)+\" \"+str(key2)] = fastdtw(Site_TimeSeriesData[key1][:,:-4], Site_TimeSeriesData[key2][:,:-4], dist=euclidean)[0] # 將測站之間的 DTW 距離轉換為 numpy 數組形式。 Sites_DTW_keys = np.array(list(Sites_DTW.keys())) Site_DTW_Numpy = np.array([[value] for _, value in Sites_DTW.items()]) 我們將所有測站兩兩間的 DTW 距離畫在數線上，可以得到下方的圖形；其中 DTW 距離從小到大皆有，若要進一步處理，便需要開始使用分群演算法來分析。\n# 創建一個 4x3 大小，解析度為 150 dpi 的圖片。 fig = plt.figure(figsize=(4, 3), dpi=150) # 在圖片中加入一個子圖。 ax = fig.add_subplot(1,1,1) # 在子圖上繪製散點圖，其中 x 軸為測站間的 DTW 距離，y 軸都設定為 1。每個點的大小設為 0.05。 ax.scatter(Site_DTW_Numpy[:,0], [1]*len(Sites_DTW.items()), s=0.05) # 隱藏 y 軸。 ax.get_yaxis().set_visible(False) 使用 K-Mean 分群演算法 我們使用 sklearn 套件中的 K-Means 模組來進行資料分群，由於分群演算法需事先設定最後要產生的群組數目，我們先設定為 3。我們使用下列的程式碼進行分群，並將結果依照資料所歸屬的群組組別 (Y 軸) 以及與其他資料的 DTW 相似度值 (X 軸) 繪製出來。\n# 從 sklearn 引入 KMeans 模組。 from sklearn.cluster import KMeans # 使用 KMeans 演算法對測站間的 DTW 距離進行分群，並設定群數為 3。 model = KMeans(n_clusters=3, random_state=0).fit([[value] for _, value in Sites_DTW.items()]) # 取得分群的結果。 Result = model.labels_ # 輸出每個群的數量。 for i in np.unique(Result): print(\"Number of Cluster{} : {}\".format(i,len(Result[Result==i]))) # 將分群結果繪製成散點圖。 fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): ax.scatter(Site_DTW_Numpy[Result==i][:,0],[i]*len(Site_DTW_Numpy[Result==i]), s=0.05) # 隱藏 y 軸。 ax.get_yaxis().set_visible(False) Number of Cluster0 : 1165 Number of Cluster1 : 994 Number of Cluster2 : 542 從分群結果中，我們可以看到 K-Means 演算法將原始資料分為三個群組，分別有 1165、994、542 筆資料，為了更進一步了解這三個群組的成因，我們繼續追查每個群組形成的可能原因。\n探究資料分群與地理位置的關係 我們首先假設空氣品質的變化具有地域性，因此探究資料分群的結果，是否和空品測站的地理位置有關。我們首先擷取測站的 GPS 經緯度座標，並換算出兩倆地理位置的實體距離，接著按照資料分群的結果，把不同群組的實體距離進行簡單的統計分析，並繪製在下方圖片中。\n# 初始化一個列表來儲存每個分群的實體距離。 Dist_for_Clusters = [None]*len(np.unique(Result)) # 迴圈遍歷所有的分群。 for i in np.unique(Result): # 初始化一個列表來儲存此分群的實體距離。 Dist_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] # 迴圈遍歷分群中的每對站點組合。 for Sites in Cluster: Site1, Site2 = Sites.split(' ') # 獲取兩個測站的經緯度座標。 coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] # 使用 geopy.distance.geodesic 函式計算兩個測站的實體位置距離，並將結果加到 Dist_for_Cluster 列表。 Dist_for_Cluster.append(geopy.distance.geodesic(coord1, coord2).km) Dist_for_Cluster = np.array(Dist_for_Cluster) Dist_for_Clusters[i] = Dist_for_Cluster Dist_for_Clusters = np.array(Dist_for_Clusters) # for Dist_for_Cluster in Dist_for_Clusters: # print(np.mean(Dist_for_Cluster)) # 創建一個新的散點圖，用於展示每個分群的距離數據。 fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) # 迴圈遍歷所有的分群，並為每個分群繪製散點和平均距離線。 for i in np.unique(Result): gtMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003enp.mean(Dist_for_Clusters[i])] ltMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003cnp.mean(Dist_for_Clusters[i])] # 輸出當前分群的平均距離和其上下偏差的百分比。 print(\"Mean Distance between Sites for Cluster{} : {}\".format(i, np.mean(Dist_for_Clusters[i]))) print(\"In Cluster{} there are {:.2%} less than mean, and {:.2%} greater than mean.\\n\".format(i, len(ltMean)/len(Dist_for_Clusters[i]), len(gtMean)/len(Dist_for_Clusters[i]))) # 用不同顏色繪製大於和小於平均距離的點。 ax.scatter(gtMean, [i]*len(gtMean), s=0.05, color=\"orange\") ax.scatter(ltMean, [i]*len(ltMean), s=0.05, color=\"pink\") # 畫出該分群的平均距離的紅色線。 ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=0.5) # 隱藏 y 軸。 ax.get_yaxis().set_visible(False) Mean Distance between Sites for Cluster0 : 84.34126465234523 In Cluster0 there are 60.09% less than mean, and 39.91% greater than mean. Mean Distance between Sites for Cluster1 : 180.26230465399215 In Cluster1 there are 54.53% less than mean, and 45.47% greater than mean. Mean Distance between Sites for Cluster2 : 234.89206124762546 In Cluster2 there are 39.48% less than mean, and 60.52% greater than mean. 從分析的結果中，我們可以發現對於 DTW 數值較高（時間序列相似度較低）的群組，其測站間的距離平均值亦較高，反之則距離平均值較低，由此可知測站資料的相似度確實與地理位置的差異有關，也應證了我們的假設，確認了空氣污染物的擴散確實會受到地理位置距離的影響。\n探究資料分群與風場風向的關係 我們接著假設空氣品質的變化受到環境風場的影響，因此探究資料分群的結果，是否和空品測站所在地的風向有關。我們首先擷取測站的 GPS 經緯度座標，並換算出兩倆地理位置的方位角關係，接著按照資料分群的結果，根據地理位置方位角與現場風向計算兩者的相關性，並將所獲得的數值進行簡單的統計分析，再繪製於下圖。\n# 計算兩點之間的方位角。 def get_bearing(lat1, long1, lat2, long2): dLon = (long2 - long1) x = math.cos(math.radians(lat2)) * math.sin(math.radians(dLon)) y = math.cos(math.radians(lat1)) * math.sin(math.radians(lat2)) - math.sin(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.cos(math.radians(dLon)) brng = np.arctan2(x,y) brng = np.degrees(brng) return brng # 檢查方位角和風向之間的關聯性。 def Check_Wind_Dirc(brng, wind_dirc): # 確定方位角和風向之間的差距是否在45度之內。 if brng \u003e 180: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng - 180 \u003c wind_dirc + 45) and (brng - 180 \u003e wind_dirc - 45)) else: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng + 180 \u003c wind_dirc + 45) and (brng + 180 \u003e wind_dirc - 45)) # 初始化兩個列表來儲存每個分群的方位角和風相關布爾值。 Brng_for_Clusters = [None]*len(np.unique(Result)) Boolean_WindRelated_for_Clusters = [None]*len(np.unique(Result)) # 迴圈遍歷所有分群。 for i in np.unique(Result): Brng_for_Cluster = [] Boolean_WindRelated_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] # 迴圈遍歷分群中的每對站點組合。 for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Brng_Between_Site = get_bearing(coord1[0], coord1[1], coord2[0], coord2[1]) Brng_for_Cluster.append(Brng_Between_Site) # 取得兩個站點的平均風向。 MeanWindDirc1 = np.mean(Site_TimeSeriesData[Site1][:,-3]) MeanWindDirc2 = np.mean(Site_TimeSeriesData[Site2][:,-3]) Boolean_WindRelated_for_Cluster.append(Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc1) or Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc2)) Brng_for_Cluster = np.array(Brng_for_Cluster) Boolean_WindRelated_for_Cluster = np.array(Boolean_WindRelated_for_Cluster) Boolean_WindRelated_for_Clusters[i] = Boolean_WindRelated_for_Cluster Brng_for_Clusters[i] = Brng_for_Cluster Brng_for_Clusters = np.array(Brng_for_Clusters) Boolean_WindRelated_for_Clusters = np.array(Boolean_WindRelated_for_Clusters) # 顯示和風向相關或不相關的測站間的距離。 fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): print(\"Relevance for Cluster{} : {:.2%}\".format(i, len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True])/len(Dist_for_Clusters[i]))) # 以不同的顏色顯示與風向相關或不相關的測站 ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True]), s=2, color=\"green\") ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False]), s=0.05, color=\"violet\") # 畫出該分群的平均距離的紅色線 ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=2) ax.get_yaxis().set_visible(False) Relevance for Cluster0 : 54.08% Relevance for Cluster1 : 39.24% Relevance for Cluster2 : 22.69% 從分析的結果中，我們可以發現對於 DTW 數值較低 （時間序列相似度較高）的群組，其測站間方位角與風向的相關性較高，反之則相關性較低，由此可知測站資料的相似度確實與環境風場風向有關，也印證了我們的假設，確認空氣污染物的擴散確實會受到環境風場風向的影響。\n案例三：結合氣象與水資源資料的分類與分群 在這個案例中，我們結合資料分群與資料分類同時進行演練。我們使用民生公共物聯網開放資料中的中央氣象局雨量站和水利署（與縣市政府合建）淹水感測器的感測資料，並且透過歷史資料的分析，利用資料分群的方式，尋找和降雨變化最為相關的河川水位站群組，並且利用資料分類的方式，在僅有雨量資料的狀況時，得以進行特定地區是否造成淹水的預測。\n資料下載與前處理 我們使用下列的程式碼，從民生公共物聯網開放資料平台的歷史資料庫中下載 2021 年中央氣象局雨量站和水利署（與縣市政府合建）淹水感測器的所有感測資料，並將下載回來的壓縮檔解開後，置於 /content 的目錄下。\n# 下載中央氣象局雨量站歷史資料。 !wget -O 'Rain_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZLzIwMjEuemlw\" !wget -O 'Rain_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZL3JhaW5fc3RhdGlvbi5jc3Y%3D\" !unzip -q 'Rain_2021.zip' \u0026\u0026 rm 'Rain_2021.zip' !find '/content/2021' -name '*.zip' -exec unzip -q {} -d '/content/Rain_2021_csv' \\; !rm -rf '/content/2021' # 下載水利署（與縣市政府合建）淹水感測器資料。 !wget -O 'Flood_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoLzIwMjEuemlw\" !wget -O 'Flood_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoL3N0YXRpb25f5rC05Yip572y77yI6IiH57ij5biC5pS%2F5bqc5ZCI5bu677yJX%2Ba3ueawtOaEn%2Ba4rOWZqC5jc3Y%3D\" !unzip -q 'Flood_2021.zip' \u0026\u0026 rm 'Flood_2021.zip' !find '/content/2021' -name '*_QC.zip' -exec unzip -q {} -d '/content/Flood_2021_csv' \\; !rm -rf '/content/2021' 接下來我們先處理降雨量資料，將所下載的 2021 年所有測站的資料逐一讀取，並把接下來不會使用到的 MIN_10、HOUR_6、HOUR_12、NOW 欄位刪除，同時移除 11 月以後的資料後，彙整為 Rain_df 物件，同時讀入測站資訊成為 Rain_Station_df 物件。由於這個步驟所處理的資料量十分龐大，因此會花費較多的時間，請耐心等候。\n# 從 Rain_2021_csv 資料夾中找到所有 csv 檔案的路徑，並將這些路徑排序。 csv_files = glob.glob(os.path.join(\"/content/Rain_2021_csv\", \"*.csv\")) csv_files.sort() # 建立一個空的 DataFrame，用於後續儲存資料。 Rain_df = pd.DataFrame() # 迴圈遍歷所有 csv 檔案。將所有資料逐一讀取，並把接下來不會使用到的 MIN_10、HOUR_6、HOUR_12、NOW 欄位刪除 for csv_file in tqdm(csv_files): # 讀取 csv 檔案，並將 obsTime 欄位解析為日期時間格式。 tmp_df = pd.read_csv(csv_file, parse_dates=['obsTime']) # 移除不必要的欄位。 tmp_df.drop(['MIN_10','HOUR_6', 'HOUR_12', 'NOW'], axis=1, inplace=True) try: # 保留在每小時整點觀測的資料。 tmp_df = tmp_df.loc[tmp_df['obsTime'].dt.minute == 00] # 將處理過的資料加入到 Rain_df 中。 Rain_df = pd.concat([Rain_df, tmp_df]) except: # 若有異常，輸出有問題的 csv 檔名。 print(csv_file) continue # 只保留 2021 年 10 月 31 日之前的資料。 Rain_df = Rain_df.loc[Rain_df['obsTime'] \u003c \"2021-11-01 00:00:00\"] num = Rain_df._get_numeric_data() # 將數值小於 0 的資料設為 0。 num[num \u003c 0] = 0 # 去除有缺失值的資料行。 Rain_df.dropna(inplace=True) # 根據站點 ID 和觀測時間排序資料。 Rain_df.sort_values(by=['station_id','obsTime'], inplace=True) # 讀取雨量站資料。 Rain_Station_df = pd.read_csv('/content/Rain_Stataion.csv') # 最後顯示整理後的雨量資料。 Rain_df 我們接下來處理淹水感測器的資料，將所下載的 2021 年所有測站的資料逐一讀取，並移除 11 月以後的資料，以及含有缺失值的資料後，將資料儲存為 Flood_df 物件，同時讀入測站資訊成為 Flood_Station_df 物件。這個步驟所處理的資料量一樣十分龐大，因此會花費較多的時間，請務必耐心等候。\n# 取得 \"/content/Flood_2021_csv\" 目錄下所有的結尾是 \"_QC.csv\" 的文件路徑。 csv_files = glob.glob(os.path.join(\"/content/Flood_2021_csv\", \"*_QC.csv\")) csv_files.sort() # 創建一個空的 DataFrame 來存放所有的淹水感測器資料。 Flood_df = pd.DataFrame() # 迴圈遍歷每一個 CSV 文件，將所有資料逐一讀取，並彙整成 Flood_df for csv_file in tqdm(csv_files): # 讀取 CSV 文件到暫時的 DataFrame。 tmp_df = pd.read_csv(csv_file, parse_dates=['timestamp']) # 選取單位為 'cm' 的資料。 tmp_df = tmp_df.loc[(tmp_df['PQ_unit'] == 'cm')] # 將 tmp_df 的資料加到主 DataFrame。 Flood_df = pd.concat([Flood_df,tmp_df], axis=0, ignore_index=True) # 選取 timestamp 小於 \"2021-11-01 00:00:00\" 的資料。 Flood_df = Flood_df.loc[Flood_df['timestamp'] \u003c \"2021-11-01 00:00:00\"] # 將數值為 -999.0 的資料替換成 0.0。 Flood_df.replace(-999.0,0.0, inplace=True) # 去除 NaN 的資料。 Flood_df.dropna(inplace=True) # 根據 timestamp 來排序資料。 Flood_df.sort_values(by=['timestamp'], inplace=True) # 讀取淹水感測器的站點資料。 Flood_Station_df = pd.read_csv('/content/Flood_Stataion.csv') # 輸出整理好的 Flood_df。 Flood_df 計算特定淹水感測器與雨量測站資料的相似度 由於淹水感測器的資料量十分龐大，因此我們先挑選一個位於雲林縣且編號為 43b2aec1-69b0-437b-b2a2-27c76a3949e8 的淹水感測器，將其資料取出後存在 Flood_Site_df 物件中，作為後續處理的範例。\n# 從 Flood_df 中選取 station_id 為 '43b2aec1-69b0-437b-b2a2-27c76a3949e8' 的資料。 Flood_Site_df = Flood_df.loc[Flood_df['station_id'] == '43b2aec1-69b0-437b-b2a2-27c76a3949e8'] # 輸出 Flood_Site_df 的前五行資料。 Flood_Site_df.head() 我們接著計算雨量站資料與選定的淹水感測器之間的相似度，我們一樣使用動態時間校正 (Dynamic Time Warping, DTW) 進行量測，由於 DTW 的值越小代表相似度越大，為了能更直觀表達相似度，我們在這個案例中，將相似度定義為 DTW 值的倒數，並且計算這個選定的淹水感測器與所有雨量站資料的相似度。\n# 將 Flood_Site_df 的 'value' 欄位的值重複三次，製作成一個新的 NumPy 陣列。 Flood_Sensor_np = np.array([[v,v,v] for v in Flood_Site_df['value'].to_numpy()]) # 初始化一個空字典，用來儲存每個雨量站與淹水感測器資料的 DTW 距離。 Site_dtw_Dist = dict() # 從 Rain_df 中選取小時為 1 的資料。 Rain_tmp_df = Rain_df.loc[(Rain_df['obsTime'].dt.hour == 1)] # 迴圈遍歷臺南市的所有雨量站。 for Site in tqdm(np.unique(Rain_Station_df.loc[Rain_Station_df['city']=='臺南市']['station_id'].to_numpy())): tmp_df = Rain_tmp_df.loc[(Rain_tmp_df['station_id'] == Site)] if tmp_df.empty: continue tmp_np = tmp_df[['RAIN','HOUR_3','HOUR_24']].to_numpy() # 使用 fastdtw 計算 Flood_Sensor_np 和 tmp_np 之間的 DTW 距離。 # 計算完的值取倒數，儲存到 Site_dtw_Dist 字典中。 Site_dtw_Dist[Site] = (1/fastdtw(Flood_Sensor_np, tmp_np, dist=euclidean)[0]) # 根據 DTW 距離值排序 Site_dtw_Dist 字典。 Site_dtw_Dist = dict(sorted(Site_dtw_Dist.items(), key=lambda item: item[1])) # 輸出 Site_dtw_Dist 字典的內容。 print(json.dumps(Site_dtw_Dist, indent=4, ensure_ascii=False)) { \"88K590\": 4.580649481044748e-05, \"C0K560\": 4.744655320519647e-05, \"C0K490\": 4.79216996101994e-05, \"C0K520\": 5.038234963332513e-05, \"C0K420\": 5.0674082994877385e-05, \"C0K250\": 5.1021366345465985e-05, \"C0K280\": 5.118406054309105e-05, \"A0K420\": 5.1515699268157996e-05, \"C0K400\": 5.178763059243615e-05, \"O1J810\": 5.2282255279259976e-05, \"C0K240\": 5.2470804312991397e-05, \"01J960\": 5.334885670256585e-05, \"C0K470\": 5.438256498969844e-05, \"81K580\": 5.45854441959214e-05, \"C0K410\": 5.5066753408217084e-05, \"A2K570\": 5.520214274022887e-05, \"01J970\": 5.529887546186233e-05, \"C0K480\": 5.5374254960644355e-05, \"C0K460\": 5.5657892623955056e-05, \"72K220\": 5.5690175197363816e-05, \"C0K510\": 5.5742273217039165e-05, \"C1K540\": 5.618025674136218e-05, \"C0K550\": 5.621240903075098e-05, \"C0K450\": 5.62197509062689e-05, \"C0K291\": 5.6380522616008906e-05, \"C0K330\": 5.638960953991442e-05, \"C0K530\": 5.6525582441919285e-05, \"C0K500\": 5.6825555408648244e-05, \"C0K440\": 5.692254536595e-05, \"C0K430\": 5.697351917955081e-05, \"01J930\": 5.7648109890427854e-05, \"C0K580\": 5.770344946580767e-05, \"C0K390\": 5.782553930260475e-05, \"01J100\": 5.7933240408734325e-05, \"01K060\": 5.8343415644572526e-05 } 資料分群並探究相關性高的雨量站 如同前面的作法，我們藉由分群演算法來幫我們依照相似度的關係，將雨量計分為三個群組，並且找出其中相似度最高的群組，以及群組中的雨量計代碼。在我們的範例中，我們找到的三個群組各自有 9、23、3 個雨量計，同時第二個群組的時間序列資料與淹水感測器的資料相似度最高。\n# 使用 KMeans 演算法進行分群，並將分成3個群組的結果保存到 cluster_model 中。 cluster_model = KMeans(n_clusters=3).fit([[value] for _, value in Site_dtw_Dist.items()]) Result = cluster_model.labels_ # 迴圈遍歷每一個群組，輸出該群組中的站點數量。 for i in np.unique(Result): print(\"Number of Cluster {} : {}\".format(i,len(Result[Result==i]))) # 初始化一個繪圖畫布，大小為 4x3，解析度為 150dpi fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) # 將 Site_dtw_Dist 轉換成 numpy 陣列，方便後續操作。 Site_DTW_Numpy = np.array([value for _, value in Site_dtw_Dist.items()]) Site_Name_Numpy = np.array([key for key, _ in Site_dtw_Dist.items()]) # 初始化一個陣列，用來儲存每一群的平均 DTW 距離。 Mean_Dis_For_Cluster = [None] * len(np.unique(Result)) # 迴圈遍歷每一群，計算該群的平均 DTW 距離，並在散佈圖上畫出該群的所有點。 for i in np.unique(Result): # 計算並儲存該群組的平均 DTW 距離。 Mean_Dis_For_Cluster[i] = (np.mean(Site_DTW_Numpy[Result==i])) # 繪製散佈圖，將該群組中的所有點都畫出來。 ax.scatter(Site_DTW_Numpy[Result==i],[i]*len(Site_DTW_Numpy[Result==i]), s=10) # 輸出該群組的平均 DTW 距離。 print(\"Mean Distance of Cluster {} : {}\".format(i,Mean_Dis_For_Cluster[i])) # 由於 y 軸僅代表群組標籤，因此隱藏 y 軸以提高可讀性。 ax.get_yaxis().set_visible(False) # 找出所有群組中，平均 DTW 距離最大的群組。 Best_Cluster = np.where(Mean_Dis_For_Cluster == np.amax(Mean_Dis_For_Cluster))[0] # 從該最大平均距離群組中，取得所有站點的名稱。 Best_Site = Site_Name_Numpy[Result == Best_Cluster] # 輸出該群組中的站點名稱。 print(Best_Site) Number of Cluster 0 : 9 Number of Cluster 1 : 23 Number of Cluster 2 : 3 Mean Distance of Cluster 0 : 5.1629678408018994e-05 Mean Distance of Cluster 1 : 5.6307994901628334e-05 Mean Distance of Cluster 2 : 4.7058249208614454e-05 ['C0K470' '81K580' 'C0K410' 'A2K570' '01J970' 'C0K480' 'C0K460' '72K220' 'C0K510' 'C1K540' 'C0K550' 'C0K450' 'C0K291' 'C0K330' 'C0K530' 'C0K500' 'C0K440' 'C0K430' '01J930' 'C0K580' 'C0K390' '01J100' '01K060'] 為了更進一步了解淹水感測器與這 23 個雨量計之間的相互關係，我們將這個選定的淹水感測器的感測資料按照時間順序繪製成圖。\n# 從 Flood_Site_df 的 'value' 欄位中提取數值，並將其轉換成 numpy 陣列格式。 tmp= Flood_Site_df['value'].to_numpy() # 初始化一個繪圖畫布，設定其大小為 6x3，解析度為 150dpi。 fig= plt.figure(figsize=(6, 3), dpi=150) # 在畫布上新增一個子圖。 ax= fig.add_subplot(1,1,1) # 使用 ax.plot 函數繪製 tmp 的曲線圖 # X 軸的數值是 tmp 陣列的索引，Y 軸的數值是 tmp 陣列的實際數值 # 設定線的寬度為 0.5 ax.plot(range(len(tmp)),tmp, linewidth=0.5) 接著我們將最相似群組的這 23 個雨量計的每小時降雨資料按照時間順序分別繪製成 23 張圖，由圖中所示，可以發現在淹水感測器出現高值時，雨量計的數值確實也都有增加的狀態發生，兩者的變化趨勢確實具備極高的相似度，符合我們的常理預期。\n# 建立一個新的繪圖區域，設定其大小和解析度。 fig = plt.figure(figsize=(8, 2*(len(Best_Site)//4+1)), dpi=150) # 迴圈遍歷 Best_Site 內的所有站點，並繪製對應的降雨數據。 for i, Site in enumerate(Best_Site): # 取得指定站點的降雨數據 tmp = Rain_df.loc[Rain_df['station_id']==Site]['RAIN'] # 在繪圖區域中新增子圖，排列方式依賴於 Best_Site 的長度。 ax = fig.add_subplot(len(Best_Site)//4+1,4,i+1) # 繪製降雨數據。 ax.plot(range(len(tmp)),tmp, linewidth=0.5) 資料分類並由雨量計資料預測淹水可能性 接下來我們利用資料分類的方法，以所選定的淹水感測器所記載的淹水資料作為標籤，搭配最佳相似群組的雨量計，建構一個簡單的分類器，預測淹水感測器所在地是否發生淹水現象。我們將原有 2021 年 1 到 10 月的資料區分為前 7 個月的訓練資料和後 2 個月的測試資料，並將前七個月中淹水感測器中數值大於 0 的資料標記為淹水事件（註：本範例乃基於方便使用的原則，採取最寬鬆的標準將水位大於 0 的事件皆認定為淹水事件；然而，有關淹水事件的判定其實有更嚴謹的相關規定，在正式使用時，建議仍應遵照相關法規的規範判定），數值等於 ０的資料標記為無淹水事件，並將整理完畢的資料，儲存在訓練資料 Train_DataSet 物件中。\n# 取得2021年8月1日前有發生洪水（淹水值\u003e0）的時間區段和淹水值。 Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']].values # 從非洪水的時間區段中隨機選取10倍的洪水時間區段數量，以作為平衡的樣本。 Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=10*len(Flooding)).values # 初始化訓練資料集的結構。 Train_DataSet = {'x':[], 'y':[]} # 處理發生洪水的時間區段資料。 for timestamp, _ in tqdm(Flooding): tmp_x = [] # 找出距離該洪水事件1到60小時內的降雨資料。 tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] # 如果找到相關站點的資料，則取最近24小時的降雨和3小時的累積雨量。 if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) # 若某些站點缺少資料，則以第一個站點的資料作為填充。 while len(tmp_x) \u003c len(Best_Site): # 若資料不足，補充資料。 tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() # 只有當資料長度符合期望的長度時，才加入到訓練資料集中。 if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(1) # 洪水的標籤為 1 # 處理非洪水的時間區段資料，流程與上面類似。 for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(0) 運用相同的方法，我們將 2021 年 8 到 10 月中淹水感測器中數值大於 0 的資料標記為淹水事件，數值等於 ０的資料標記為無淹水事件，並將整理完畢的資料，儲存在測試資料 Test_DataSet 物件中。\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=2*len(Flooding)).values Test_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(0) 最後我們使用 Scikit learn (sklearn) 這個 Python 套件所提供的九個分類器模型 (Nearest neighbors, Linear SVM, RBF SVM, Decision tree, Random forest, Neural net, Adaboost, Naive Bayes, QDA) ，並且依次帶入訓練資料進行調校後，接著帶入測試資料進行淹水與否的預測，並且將預測結果與測試資料中的標籤內容進行比對，再用混淆矩陣 (confusion matrix) 的方式，呈現不同標籤組合的分類結果。\n# 定義分類器的名稱列表。 names = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] # 定義對應的分類器實例。 classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] # 創建 3x3 的子圖，用於顯示混淆矩陣。 fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) # 迴圈遍歷每一個分類器。 for i, model in enumerate(classifiers): # 使用訓練資料集來訓練該分類器。 model.fit(Train_DataSet['x'], Train_DataSet['y']) # 使用測試資料集來進行預測。 Result = model.predict(Test_DataSet['x']) # 計算混淆矩陣。 mat = confusion_matrix(Test_DataSet['y'], Result) # 繪製混淆矩陣。 sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=[\"不淹水\",\"淹水\"], yticklabels=[\"不淹水\",\"淹水\"], ax = axes[i//3][i%3]) # 設置子圖的標題，顯示分類器名稱和精確度。 axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(names[i], round(accuracy_score(Result, Test_DataSet['y']), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) # 調整子圖之間的間距。 plt.tight_layout() 在這個案例中，我們發現 Nearest Neighbors 方法可以達到將近 7.3 成的分類成功率，事實上，這只是我們使用原始資料尚未進行更進一步處理分析的結果，倘若我們對於資料本身進行更多的分析，擷取更多的特徵，還有可能將分類成功率繼續提升，讀者們若對於分類器有興趣，可以參考相關資源，進行更深入的探究，將能更加提升分類器對不同種類資料的分類能力。\n參考資料 Ibrahim Saidi, Your First Machine Learning Project in Python, Medium, Jan, 2022, (https://ibrahimsaidi.com.au/your-first-machine-learning-project-in-python-e3b90170ae41) Esmaeil Alizadeh, An Illustrative Introduction to Dynamic Time Warping, Medium, Oct. 2020, (https://towardsdatascience.com/an-illustrative-introduction-to-dynamic-time-warping-36aa98513b98) Jason Brownlee, 10 Clustering Algorithms With Python, Machine Learning Mastery, Aug. 2020, (https://machinelearningmastery.com/clustering-algorithms-with-python/) Alexandra Amidon, How to Apply K-means Clustering to Time Series Data, TOwards Data Science, July 2020, (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) scikit-learn Tutorials (https://scikit-learn.org/stable/tutorial/index.html) FastDTW - A Python implementation of FastDTW, (https://github.com/slaypni/fastdtw) Nearest Neighbors - Wikipedia, (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) SVM Classifier - Wikipedia, (https://en.wikipedia.org/wiki/Support-vector_machine) Decision Tree - Wikipedia, (https://en.wikipedia.org/wiki/Decision_tree) Random Forest - Wikipedia, (https://en.wikipedia.org/wiki/Random_forest) K-Means - Wikipedia, (https://en.wikipedia.org/wiki/K-means_clustering) DBSCAN - Wikipedia, (https://en.wikipedia.org/wiki/DBSCAN) Hierarchical Clustering - Wikipedia, (https://en.wikipedia.org/wiki/Hierarchical_clustering) BIRCH - Wikipedia, (https://en.wikipedia.org/wiki/BIRCH) ",
    "description": "我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行資料分類與資料分群的分析。我們將示範機器學習的標準流程，並且介紹如何透過資料分類進一步進行資料預測，以及如何透過資料分群進行對資料進一步的深入探究。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "6.1. 機器學習初探",
    "uri": "/ch6/ch6.1/"
  },
  {
    "content": " Table Of Contents 章節目標 QGIS操作說明 範例一：空品測站分佈圖 資料匯入 產生 GeoJSON 檔案 篩選資料與改變資料點位顏色 將成果輸出成主題式地圖 範例二：避難所分佈圖 QGIS總結 Reference QGIS 是一套免費的地圖資料管理系統，除了可以將使用者蒐集到的資料以地理資續的方式呈現外，使用者也可透過 QGIS 來處理、分析及整合地理空間資料，並繪製主題地圖。在這個章節中，我們將利用 QGIS 來協助我們分析並呈現從民生公共物聯網上所得到的 PM2.5 資料，並在分析完成後將其結果輸出成主題式地圖以供判讀。我們同時也示範如何結合民生公共物聯網的災防資料，透過 QGIS 系統繪製防災避難所分佈圖，方便民眾自行查詢離家最近的防災避難所。\n註釋 本文所操作之 QGIS 版本為 3.16.8，惟本文所使用之功能皆為該軟體之基本功能，若使用其他版本軟體，應仍可正常操作。\n章節目標 如何將取得的資料導入 QGIS 在 QGIS 使用前面章節所介紹的地理空間分析功能 (Intersection、Buffer) 將結果輸出成一張主題地圖供別人觀賞 QGIS操作說明 在執行 QGIS 軟體後，可以看到下方的操作介面，除了中間區域的資料框外，上方為標準工具列，提供各式不同的基本操作工具與功能；左方有兩個子區域，分別為資料目錄視窗與圖層；右邊則為分析工具列，提供各式不同的分析工具。\n以下，我們將使用兩個簡單的範例，透過民生公共物聯網的空品資料與災防資料，介紹 QGIS 的基本操作。\n範例一：空品測站分佈圖 資料匯入 資料來源：民生公共物聯網歷史資料 https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器\n在這個部分會先介紹如何將資料成功導入到 QGIS 內。由於有些資料在儲存時，會把一張資料表拆開成多張不同的資料表，因此我們在進行分析時，必須要特別留意是否手邊的資料是否有這種情況，並將資料重新結合成原來的一張資料表，以下介紹資料結合的方法：\n資料導入\n首先，我們要先介紹如何將從民生公共物聯網歷史資料直接下載下來的 csv 檔導入到 QGIS。由於資料中有中文，導入時會顯示亂碼，所以採用導入方法為 Layer（位於最上方選單） \u003e Add Layer \u003e Add Delimited Text Layer，匯入介面如下圖： 資料結合 (Join)\n由於原始資料將 PM2.5 與測站經緯座標分成兩個檔案，故須先將 PM2.5 與經緯座標進行 Join。 Join方法如下，對要Join的檔案右鍵 \u003e Properties \u003e joins \u003e 點+號進入 Add Vector Join，如下圖 進入之後有四個主要的選項分別是：\nJoin Layer：想要 Join 的 Layer Join field：與 Target field 所對應，為 Join 時的參考 (類似 Primary key) Target field：與 Join field 所對應，為 Join 時的參考 (類似 Foreign key) Joined field：選擇想要 Join 的欄位 產生 GeoJSON 檔案 接著我們利用 QGIS 內建的功能來將原本的 csv 資料轉換成 GeoJSON 檔，操作程序為點選Processing \u003e Toolbox \u003e Create points from table。\n請注意在點選後選擇要輸入的 Table，在 X 選擇 lon、在 Y 輸入 lat，並且在 Target CRS 指定為 WGS 84（經緯度座標）然後輸入要輸出的檔案名稱，如下圖。\n接著選擇所要輸出的檔案格式後，點選「存檔」即可。\n篩選資料與改變資料點位顏色 接下來我們示範如何使用 QGIS 來篩選所需要的測站，並讓測站的呈現顏色隨著 PM 2.5 數值而改變。\n利用 Intersection 來根據縣市篩選所需要的測站\n在進行篩選前，需要先從政府資料開放平台下載直轄市、縣市界線的 shp 檔，之後將各縣市界線的 shp 檔匯入QGIS，如下圖： 註釋 由於民生公共物聯網計畫中，將彰化和南投縣市的校園空品微型感測器佈建劃分由暨南大學負責，因此本次所取用的資料中，並無彰化和南投的資料。\n接著我們點選 Select Feature 的圖示，並點選 Country 的 Layer 並選取想要的縣市，被選取的縣市將會呈現黃色，這邊以新北為例，如下圖： 在 Processing Toolbox 中找尋 Intersection 功能，點擊後會出現如下圖介面，其中有三個主要的輸入，分別是：\nInput Layer Overlay Layer Intersection (Output) 接著請在 Input Layer 中放入 PM2.5 的 layer，在 Overlay Layer 中放入縣市界線的 layer 並勾選Select features only，代表只篩選出和上一步驟中選取的新北市有交集的測站，接著在 Intersection 中輸入要輸出的檔案名稱，其中可支援輸出的檔案格式選項和之前可選擇的選項相同。\n執行過後將會得到下圖成果 根據 PM2.5 的數值來顯示不同的顏色\n接著我們在 PM2.5 的 layer 上點選右鍵 \u003e Properties \u003e Symbology 便可以看到圓點的顏色設定，有關各 PM2.5 測站的顏色設定步驟如下：\n在最上方原本的設定為 No Symbols，將其改為 Graduated 如下 Value 部分選 PM25 點擊下方 Classify 按鈕 在 Classes 設定顏色數目（註：建議設定類別不宜過多，以不超過7類為原則） 到 Color ramp 設定各數值之顏色 完成後點擊 OK 當一切都設定完成後，將可以得到下圖，其中圓點的顏色會隨著PM 2.5數值的不同而改變，而右邊的 Layer 則顯示不同顏色所代表的 PM 2.5 數值。 註釋 新版的QGIS (3.16 後)裡已經有 OpenStreetMap 的底圖，可以點擊下圖中的 XYZ Tiles → OpenStreetMap 來添加 OSM 底圖\n將成果輸出成主題式地圖 在完成了上述的設定後，接下來要將 QGIS 專案輸出成 JPG 圖片。我們點擊 Project \u003e New Print Layout 後會跳出 Layout 名稱設定，設定完成後會出現如下畫面：\n點擊左方的 Add map，並在繪圖區上框選範圍，來加入 PM 2.5 的地圖，如下圖\n接下來點選左邊的 Add Legend 並選取一範圍來匯入標籤，而在右方的 Item Properties 可以改變標籤中的字體大小、顏色等；最後再匯入標題、比例尺、指北針來完成主題圖。\n最後點選左上角的 Layout \u003e Export as Image 便可將主題圖輸出成圖片檔案。\n範例二：避難所分佈圖 資料來源：https://data.gov.tw/dataset/73242\n在政府的開放資料中，已將全台灣的緊急避難所整理成電子檔案，方便民眾下載使用，在這個範例中，我們將使用這些資料，介紹如何透過 QGIS 尋找離家裡最近的避難所。\n我們先去上面的網址取得避難所資料，接下來照前面所講的方法載入資料，如下圖：\n由於全台灣的避難所數量眾多，我們在本文中只分析台北市的避難所，其餘縣市的部分也可以用相同的方法分析，歡迎讀者自行嘗試。我們先用前面所述的 intersection 方法把台北市的避難所找出來。接下來，我們使用旁邊工具列的 Voronoi Polygons 來繪製Voronoi Diagram，如下圖：\n在 Voronoi Polygons 填入避難所的圖層，並按「執行」\n根據 Voronoi Diagram 的特性 (第 5.2 章節)，我們可以得知離自己家最接近的避難所位置，如下圖：\n在完成分析後，可以按照前面的方法將分析結果出成主題式地圖來供別人觀賞。\nQGIS總結 在本章節中我們介紹了如何將資料導入 QGIS，以及如何利用 QGIS 中的分析工具，來協助我們對資料進行分析。最後我們介紹了圖表匯出的方法，可以把分析好的資料製成主題圖供別人觀賞。當然，QGIS 中仍有許多功能是在這個章節中來不及介紹的，如果對 QGIS 還有更多的興趣，可以參考下方的一些其他資源。\nReference QGIS: A Free and Open Source Geographic Information System (https://qgis.org/) YouTube：GIS 網上小教室系列：第1集 — 從零開始學 QGIS QGIS 英文說明：QGIS Tutorials and Tips QGIS Documentation (https://www.qgis.org/en/docs/index.html) ",
    "description": "我們介紹使用 QGIS 系統進行的地理資料呈現，並且以Civil IoT Taiwan 的資料當作範例，利用點擊拖拉的方式，進行地理空間分析。同時我們也討論 QGIS 軟體的優缺點與使用時機。",
    "tags": [
      "空",
      "災"
    ],
    "title": "7.1. QGIS 應用",
    "uri": "/ch7/ch7.1/"
  },
  {
    "content": "本網站為基於臺灣民生公共物聯網各項開放資料，所撰寫之資料應用說明與範例。本網站所使用之各項開放資料，其授權規定請參考原始網站之說明內容；本網站所提供程式範例中，所使用到的 Python 語言套件與套裝軟體，其使用授權，請參考各套件與軟體之相關網頁的說明；本網站所提供之文章內容，採用 CC-BY (https://creativecommons.org/licenses/by/4.0/) 的創用 CC 授權。\n本網站的指導單位、執行單位、主持人與相關團隊資訊如下：\n指導單位：國家實驗研究院 科技政策研究與資訊中心 執行單位：中央研究院 資訊科學研究所 專案主持人：陳伶志 專家顧問團隊： 洪智傑，國立中興大學資訊管理系 汪立本，國立台灣大學土木工程系 黃仁暐，國立成功大學電機工程系 黃維嘉，LASS 社群 劉致灝，國家災害防救科技中心 洪翠屏，臺北市立育成高級中學 高慧君，臺北市立南港高級中學 編輯團隊：吳姃家、沈姿雨、洪軾凱、高慧君、陳宏穎、彭昱齊、黃仁暐、鄭宇伸、鍾明光、羅泉恆 審查團隊： 劉嘉凱，智庫驅動股份有限公司 許武龍，LASS社群 謝欣成，國立中央大學化學系 劉育宏，高雄市立前鎮高中 柯建華，基隆市立基隆高中 洪挺晏，國立臺灣師範大學附屬高中 ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "關於我們",
    "uri": "/about/"
  },
  {
    "content": " Table Of Contents 程式語言 - Python 開發平台 – Google Colab 參考資料 程式語言 - Python 本課程專注於民生公共物聯網資料應用，並以資料科學界廣泛採用的 Python 程式語言為教學基礎。我們透過直觀和易於理解的示範，引領學員透過實踐來學習，逐漸深入各個章節的核心主題。學員將有機會獲得民生公共物聯網資料應用的實際經驗，並培養出對其他資料科學主題的應用能力。\nPython 之所以能迅速成為資料科學界的首選程式語言，主要歸因於其三大優勢：\n低學習門檻：Python 的語法結構相對簡單直觀。與 C 或 Java 等語言相比，Python 少了許多複雜的語法和特殊符號，使其更像是閱讀日常英文文章。只需掌握基本語法和邏輯以及一些基礎英文能力，學員就能輕易理解和掌握 Python 程式碼。 豐富的套件庫：Python 有著廣泛而多樣的套件庫，這得益於其三十多年的發展歷程和龐大的開源社群。使用者可以根據需求，安裝各種套件以擴展Python的功能，使其能夠適應各種不同的應用場景。在本課程中，我們會使用一個特別定制的Python套件（pyCIOT），幫助學員更快地突破學習的瓶頸，並掌握民生公共物聯網資料的應用技巧。 適合自學的語言：隨著套件的多樣化，學會閱讀和理解程式碼變得尤為重要。學員需要學會閱讀使用手冊，然後學會如何將不同的套件組合起來寫出解決特定問題的程式碼。本課程鼓勵學員建立在現有程式基礎上進行學習和創新，而不是從零開始。透過持續的實踐和閱讀，學習Python將變得像閱讀一本引人入勝的故事書。 由於 Python 語言具備上述的優勢，它已經成為資料科學領域最常用的程式語言，也是許多初學者的首選。除了傳統的教學書籍，網際網路上也充斥著大量有用的學習資源，這些資源對於有興趣深入學習 Python 的學員來說是一個寶貴的學習資產。\n免費教學課程 用 Python 做商管程式設計（一），孔令傑，Coursera (https://zh-tw.coursera.org/learn/pbc1) Python 入門教學課程，彭彭的課程 (https://www.youtube.com/watch?v=wqRlKVRUV_k\u0026list=PL-g0fdC5RMboYEyt6QS2iLb_1m7QcgfHk) Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) 免費電子書資源 Python 教學 (https://docs.python.org/zh-tw/3/tutorial/index.html) Python 教學 (學習導讀)，STEAM 教育學習網 (https://steam.oxxostudio.tw/category/python/info/start.html) Python 程式設計，李明昌 (http://rwepa.blogspot.com/2020/02/pythonprogramminglee.html) Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) 開發平台 – Google Colab Python 作為一種直譯式語言，不同於 C 語言等需要事先編譯的語言，它在執行時才將代碼轉換成機器語言。這意味著 Python 代碼在執行時會被一行一行地轉換和執行，這樣的運作方式類似於一位即時翻譯員在我們與外國人交流時幫助我們翻譯語言。\n這種直譯的特點使得 Python 擁有多樣性的開發環境選擇。其中，Jupyter 以其類似筆記本的設計，允許用戶將自然語言文字和 Python 代碼組合在一起，成為眾多開發者和數據分析師的首選。基於 Jupyter，Google 推出了 Colab 平台，允許用戶在雲端上編寫和執行 Python 代碼，這不僅減輕了本地計算和存儲的壓力，還方便了協作和分享。\nGoogle Colab 的出現為 Python 開發者帶來了以下便利：\n無需安裝：Colab 已預先安裝了大多數常用的 Python 套件，用戶無需自行配置和解決可能出現的依賴和衝突問題。 雲端儲存：Colab 代碼和數據都存儲在 Google 雲端，用戶可以在任何地方、任何設備上訪問和編輯自己的代碼，免去了數據傳輸和儲存的麻煩。 協作共創：Colab 支持多人在線協作，用戶可以實時共享和協作代碼，這對於團隊合作和項目共建具有重要價值。 免費運算資源：Colab 提供免費的 GPU 和 TPU 資源，用戶可以利用這些高性能運算資源執行複雜和計算密集型的任務，無需擔心本地計算資源的限制。 這些特點使得 Google Colab 成為了 Python 和資料科學愛好者的理想選擇。無論是初學者還是專業開發者，都可以在這個平台上找到合適的工具和資源，快速開始和深化自己的 Python 和資料科學學習和開發。有關 Google Colab 的相關學習資源，可參考下列的連結：\nGoogle Colab 使用教學，MeDA School，國立台灣大學 (https://www.youtube.com/watch?v=OyS6K2XdgbQ) 使用 Google Colab，STEAM 教育學習網 (https://steam.oxxostudio.tw/category/python/info/online-editor.html) Getting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) 參考資料 Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "民生公共物聯網資料應用所使用到的程式語言 Python 和開發平台 Google Colab 簡介",
    "tags": [
      "概述"
    ],
    "title": "2.2. 課程軟體工具簡介",
    "uri": "/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents 獲取特定時間序列之資料 獲取特定區域之資料 實作：所在地有比附近的空氣糟嗎？ 獲取檢測站資料 去除無效資料 計算距離 Pandas 函式庫 顯示結果 參考資料 本章節將以時間、空間的角度存取民生公共物聯網的資料，並以空氣品質監測作為題目進行簡易實作。\n本章節會涵蓋到的技術：\ndatetime, math, numpy, pandas 等函式庫應用 json 資料格式處理 Pandas DataFrame 資料處理 獲取特定時間序列之資料 在 pyCIOT 中在執行 get_data() 時，能夠根據時間起始及結束時間獲取資料。格式以字典 (Dict) 傳入 time_range，分別為 start, end 及 num_of_data。\nstart 與 end 指資料搜集的開始及結束時間，格式為 ISO8601 或 Datetime。num_of_data 則是會控制獲取資料的筆數不會超過此數字。若在範圍內的資料超過 num_of_data 則會隔一段時間搜集，使資料與資料之間的時間間隔趨於平均。\n以空氣資料為例，獲取之資料最多能夠回溯一天。因此當將 end 變數設定為一天之前不會獲得任何資料，請留意。此外，因為民生物聯網中各個感測器的更新頻率不同，所以不同感測器每「天」的資料筆數會有不同，詳可參閱：https://ci.taiwan.gov.tw/dsp/dataset_air.aspx\n# 從 datetime 模組中引入 datetime 和 timedelta 類別 from datetime import datetime, timedelta end_date = datetime.now() # 獲取現在時間 isodate_end = end_date.isoformat().split(\".\")[0]+\"Z\" # 將時間格式轉為 ISO8601，去除毫秒並加上\"Z\" start_date = datetime.now() + timedelta(days = -1) # 獲取前一天的時間 isodate_start = start_date.isoformat().split(\".\")[0]+\"Z\" # 將時間格式轉為 ISO8601，去除毫秒並加上\"Z\" # 定義時間區間及要取得的資料數量 time = { \"start\": isodate_start, \"end\": isodate_end, \"num_of_data\": 15 } # 從「智慧城鄉空品微型感測器-11613429495」感測器獲取一天內的 15 筆資料 data = Air().get_data(\"OBS:EPA_IoT\", stationIds=[\"11613429495\"], time_range=time) data 資料會以 List 的格式儲存在 data 中，並依照不同種類的數值一起存放。溫度、相對濕度、PM2.5 的資料會分別存放在對應名字下的 ‘values’ list 下，並標上每筆資料紀錄的時間，以 ISO8601 顯示。\n[{'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 30.7}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T12:54:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:53:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 11.9}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 12.15}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 12.2}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 12.22}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 12.31}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 12.19}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 12.26}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 12.17}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 12.04}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 11.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 11.67}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 11.56}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 11.56}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}}] 獲取特定區域之資料 在 pyCIOT 中也有根據地域獲取特定資料的方式。以特定地點的經度及緯度，以及以特定地點的經度及緯度為中心，並利用一個半徑距離構成搜尋範圍 (圓形)，以獲取特定空間內的測站 ID 及感測數值。\n特定區域的資料格式也是以字典 (Dict) 傳入，其中緯度、經度及半徑分別為 “latitude”、“longitude” 及 “distance”。特定區域及特定時間的篩選功能可以同時使用，搜尋特定區域時也可以將有要觀察的測站放進 “stationIds” 中，可以順便將區域外的測站去除。\n# 定義一個位置資訊，包含緯度、經度和搜尋的半徑 loc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # 搜尋的半徑，以公里為單位 } # 從指定的資料來源獲取與定義位置相關的空氣資訊 c = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) # 顯示取得搜尋結果的第一筆資料 c[0] { 'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': { 'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [ { 'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 35.84}] },{ 'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 59.5}] },{ 'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 11.09}] } ], 'location': { 'latitude': 24.998769, 'longitude': 121.512717, 'address': None } } 以上為在獲取 pyCIOT 測站資料時常用的，以時間和空間為篩選標準的方法之一，適用於所有包含 location及 timestamp型態的資料。為了示範，我們舉出一些簡單的例子，並使用這些 pyCIOT 函式庫實作。\n實作：所在地有比附近的空氣糟嗎？ 匯入資料：環保署智慧城鄉空品微型感測器（OBS:EPA_IoT） 測試地點：中和南勢角捷運站 1 號出口：(24.990550, 121.507532) 比較環境：新北市中和區 獲取檢測站資料 首先，需要獲得測試地點和比較環境的所有資料。我們可以利用「獲取特定區域之資料」的方法，將經度緯度設定在南勢角捷運站一號出口，將距離設定為三公里，即可簡單的將資料利用 Air().get_data()獲取：\n# 獲取檢測站的資料（溫度、濕度、 PM2.5） # 定義一個位置資訊，包含緯度、經度和搜尋的半徑 loc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # (km) # 搜尋的半徑，以公里為單位 } # 從 OBS:EPA_IoT 這個資料來源中獲取與定義位置相關的空氣資料 EPA_IoT_zhonghe_data_raw = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) # 顯示所獲取的檢測站的總數 print(\"len:\", len(EPA_IoT_zhonghe_data_raw)) # 顯示搜尋結果的第一筆資料 EPA_IoT_zhonghe_data_raw[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [{'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 94.84}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 3.81}]}, {'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 25.72}]}], 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}} 去除無效資料 在每個範圍內的測站中，不一定每個測站都還在順利運行。為了將這些測站去除，我們觀察無效測站的資料會有什麼特徵，發現三個資料（溫度、濕度、PM2.5 濃度）都會是 0。只要挑出並刪除這些資料便能夠進行下一步驟。\n# 資料清理 EPA_IoT_zhonghe_data = [] for datajson in EPA_IoT_zhonghe_data_raw: # 檢查是否存在 \"data\" 這個鍵，確認資料存在。 if \"data\" not in datajson: continue; # 將原始資料格式轉換成「Temperature」、「Relative_Humidity」和「PM2_5」的格式。 for rawdata_array in datajson['data']: if(rawdata_array['name'] == 'Temperature'): datajson['Temperature'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'Relative humidity'): datajson['Relative_Humidity'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'PM2.5'): datajson['PM2_5'] = rawdata_array['values'][0]['value'] # 移除 'data' 這個鍵，因為我們已經取出我們需要的資訊。 datajson.pop('data') # 確認資料中包含了我們所需的所有鍵，同時去除無資料（值全為 0）的檢測站 if \"Relative_Humidity\" not in datajson.keys(): continue if \"PM2_5\" not in datajson.keys(): continue if \"Temperature\" not in datajson.keys(): continue if(datajson['Relative_Humidity'] == 0 and datajson['PM2_5'] == 0 and datajson['Temperature'] == 0): continue EPA_IoT_zhonghe_data.append(datajson) # 顯示清理後的資料筆數 print(\"len:\", len(EPA_IoT_zhonghe_data)) # 顯示結果的第一筆資料 EPA_IoT_zhonghe_data[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24} 計算距離 假設每個測站的資料沒有誤差，那最接近目標地點的測站資料即為要比較的資料。為了找到最接近的測站，我們要算出將每個測站和目標地點的距離。\n我們可以利用點到點距離公式計算並排序找到最接近目標地點的測站，可以直接使用在 math 內的 pow() 函式，計算平方及平方根距離。但在這裡我們使用比較標準的 Haversine 公式計算地球上兩點間的球面距離，以下為在 WGS84 坐標系下的實作：\n# 為資料增加一個欄位，用來表示與南勢角站的距離 import math # 定義一個函式，計算兩點的緯度和經度之間的距離 def LLs2Dist(lat1, lon1, lat2, lon2): R = 6371 dLat = (lat2 - lat1) * math.pi / 180.0 dLon = (lon2 - lon1) * math.pi / 180.0 a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) * math.sin(dLon / 2) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) dist = R * c return dist # 計算每個資料點與南勢角站的距離，並將結果儲存在 'distance' 欄位中 for data in EPA_IoT_zhonghe_data: data['distance'] = LLs2Dist(data['location']['latitude'], data['location']['longitude'], 24.990550, 121.507532) # (24.990550, 121.507532) EPA_IoT_zhonghe_data[0] {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24, 'distance': 1.052754763080127} Pandas 函式庫 Pandas 是用於資料操縱和分析的函式庫，其中的 DataFrame 格式用來儲存雙維度或多欄位的資料格式，非常適合用來進行資料分析。我們將處理好的資料轉換成 DataFrame，並挑選出需要的欄位並根據先前計算的距離排序由小到大。\n# 將資料轉換為 Pandas.DataFrame 格式，以便於資料分析。 import pandas as pd # 使用 pandas 的 json_normalize 方法將 JSON 格式的資料轉換為 DataFrame。 df = pd.json_normalize(EPA_IoT_zhonghe_data) # 顯示轉換後的完整資料。 df # 從完整資料中選擇我們所需要的欄位。 EPA_IoT_zhonghe_data_raw = df[['distance', 'PM2_5', 'Temperature', 'Relative_Humidity', 'properties.stationID', 'location.latitude', 'location.longitude', 'properties.areaType']] # 根據 'distance' 和 'PM2_5' 欄位排序資料。 EPA_IoT_zhonghe_data_raw = EPA_IoT_zhonghe_data_raw.sort_values(by=['distance', 'PM2_5'], ascending=True) # 顯示處理後的資料。 EPA_IoT_zhonghe_data_raw 顯示結果 為了知道目標區域的空氣品質相較附近區域的好壞，可以大致上利用所有測站空氣品質的分佈得知。可以利用在 Python 中常利用的 numpy 資料科學處理常用函式庫等工具，或直接計算出平均及標準差，便可得到答案。\nimport numpy as np # 從 EPA_IoT_zhonghe_data_raw 取得最近測站的 PM2.5 值。 # [0,1] 這裡的兩個數字是指定的行和列的索引。0 表示第一行，1 表示第二列。 zhonghe_target = EPA_IoT_zhonghe_data_raw.iloc[0,1] # 計算 PM2.5 （[:,1]：第二列）的平均值和標準差 zhonghe_ave = np.mean(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) zhonghe_std = np.std(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) # 計算目標 PM2.5 值與平均值之間的差異，並以標準差為單位表示 result = (zhonghe_target-zhonghe_ave)/zhonghe_std # 顯示計算的結果 print('mean:', zhonghe_ave, 'std:', zhonghe_std) print('最近測站 PM2.5 濃度:', zhonghe_target) print('目標離平均', result, '個標準差\\n') # 根據計算結果顯示出空氣品質的評估 if(result\u003e0): print('Result: 現在家裡附近的空氣比附近糟') else: print('Result: 現在家裡附近的空氣比附近好') mean: 6.71 std: 3.18 最近測站 PM2.5 濃度: 7.38 目標離平均 0.21 個標準差 Result: 現在家裡附近的空氣比附近糟 參考資料 Python pyCIOT package (https://pypi.org/project/pyCIOT/) pandas - Python Data Analysis Library (https://pandas.pydata.org/) 10 minutes to pandas — pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) NumPy (https://numpy.org/) NumPy quickstart (https://numpy.org/doc/stable/user/quickstart.html) Haversine formula - Wikipedia (https://en.wikipedia.org/wiki/Haversine_formula) ",
    "description": "我們介紹如何在民生公共物聯網資料平台中獲取特定時間或時間段的資料，以及特定地理區域的資料，並透過簡單的案例演示其應用。",
    "tags": [
      "Python",
      "API",
      "空"
    ],
    "title": "3.2. 存取特定時空條件的資料",
    "uri": "/ch3/ch3.2/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引入 讀取資料 空品資料 水位資料 氣象資料 資料預處理 (Preprocess) 平穩性 (Stationary) 檢查 資料預測(Data forecast) ARIMA SARIMAX auto_arima Prophet LSTM Holt-Winter 綜合比較 參考資料 前一章節介紹了各種處理時序資料的方法，有視覺化呈現資料、時序資料分解……等，經過處理後的資料可以讓我們更進一步的運用，擁有過去的資料後，會想要預知未來，因此本章節將會判斷時序資料的特性以及使用多種預測模型找出資料的的模式，藉此預測未來。\n章節目標 時序資料特性的判斷：平穩性 學習各種預測模型並進行比較 使用時序資料進行訓練與預測 套件安裝與引入 在本章節中，我們將會使用到 pandas, matplotlib, numpy, statsmodels, warnings 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 並未預先安裝好的套件：kats 和 pmdarima，需使用下列的方式自行安裝：\n# 升級 pip，確保可以順利安裝後續的函式模組 !pip install --upgrade pip # 安裝特定版本的 kats、ax-platform 和 statsmodels 函式模組 # kats: Facebook 提供的時間序列工具箱 # ax-platform: 用於自適應實驗設計和優化的平台 # statsmodels: 提供統計模型和擬合的函式模組 !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # 安裝 pmdarima 函式模組，它提供自動選擇 ARIMA 模型的功能，適用於時間序列分析 !pip install pmdarima 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport warnings # 引入警告處理模組，用於控制警告訊息 import numpy as np # 引入數學運算和資料處理的基礎函式模組 import pandas as pd # 引入數學運算和資料處理的基礎函式模組 import pmdarima as pm # 引入時間序列預測的 pmdarima 函式模組，它提供自動選擇 ARIMA 模型的功能 import statsmodels.api as sm # 引入統計模型和擬合的 statsmodels 函式模組，並提供時間序列分析的方法 import matplotlib.pyplot as plt # 引入繪圖函式模組，用於資料視覺化 import os, zipfile # 引入操作系統和壓縮檔案的函式模組，用於檔案操作和解壓縮 from dateutil import parser as datetime_parser # 引入日期時間解析工具 from statsmodels.tsa.arima.model import ARIMA # 引入時間序列模型 from statsmodels.tsa.statespace.sarimax import SARIMAX # 引入時間序列模型 from statsmodels.tsa.stattools import adfuller, kpss # 引入時間序列穩定性檢測工具 from kats.consts import TimeSeriesData, TimeSeriesIterator # 引入 Kats（Facebook 提供的時間序列工具箱）的基本結構和工具 from kats.detectors.outlier import OutlierDetector # 引入 Kats 的異常值檢測工具 from kats.models.prophet import ProphetModel, ProphetParams # 引入 Kats 提供的 Prophet 模型，及其參數設定 from kats.models.lstm import LSTMModel, LSTMParams # 引入 Kats 提供的 LSTM 模型，及其參數設定 from kats.models.holtwinters import HoltWintersParams, HoltWintersModel # 引入 Kats 提供的 Holt-Winters 模型及其參數設定 讀取資料 本章節的探討主題為時序資料的資料預測，因此我們將分別以民生公共物聯網資料平台上的空品、水位和氣象資料進行資料讀取的演示，接著再使用空品資料進行更進一步的探究。其中，每一類別的資料處理都將使用其中一個測站長期以來觀測到的資料作為資料集，而在 dataframe 的時間欄位名稱則為設為 timestamp，由於時間欄位的數值具有唯一性，因此我們也將使用此欄位作為 dataframe 的索引 (index)。\n空品資料 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的歷史資料，並存入 Air 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Air 資料夾中。\n!mkdir Air CSV_Air # 創建資料夾來存放下載的資料和解壓縮後的 CSV 檔案 # 從指定網址下載 zip 檔案 !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" # 定義資料夾和副檔名的變數 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' # 解壓縮第一層的 zip 檔案 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() # 檢查是否還有子資料夾中包含的 zip 檔案並進行解壓縮 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() # 檢查更多層次的子資料夾中是否有 zip 檔，如果有則解壓縮 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # 將解壓縮後的檔案儲存到指定資料夾 zip_ref.close() elif item.endswith(extension_csv): # 如果找到 CSV 檔，則將它移到指定的資料夾 os.rename(path2, f'CSV_Air/{item}') 現在 CSV_Air 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 74DA38C7D2AC 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 air 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 定義目標資料夾和檔案副檔名 folder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' # 創建空的 DataFrame 來存放讀取的資料 air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 確保 pm25 欄位名稱是大寫的「PM25」 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 篩選出指定 device_id 的資料 filtered = df.query(f'device_id==@id') # 合併到主要的 DataFrame 中 air = pd.concat([air, filtered], ignore_index=True) # 清除空的時間戳記欄位資料 air.dropna(subset=['timestamp'], inplace=True) # 將時間戳記欄位轉換為 datetime 格式且去除時區資訊 for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive # 將時間戳記設為 DataFrame 的索引 air.set_index('timestamp', inplace=True) # 刪除原始的 Air 和 CSV_Air 資料夾 !rm -rf Air CSV_Air 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 移除「device_id」和「SiteName」這兩個欄位 air.drop(columns=['device_id', 'SiteName'], inplace=True) # 根據時間戳記排序資料 air.sort_values(by='timestamp', inplace=True) # 顯示資料集的簡要資訊 air.info() # 顯示資料集的簡要資訊 print(air.info()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 水位資料 和空品資料的範例一樣，由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「水利署地下水位站」的歷史資料，並存入 Water 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Water 資料夾中。\n# 建立資料夾存放下載的資料 !mkdir Water CSV_Water # 下載 2018 到 2021 年的水質資料 !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" # 定義要解壓縮的目錄及檔案附檔名 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' # 解壓縮主目錄下的 .zip 檔 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() # 解壓縮子目錄下的 .zip 檔 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() # 若子目錄中有更深層的資料夾，則將其中的 .zip 檔案也進行解壓縮 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # 決定解壓縮的目標路徑 zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') 現在 CSV_Water 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 338c9c1c-57d8-41d7-9af2-731fb86e632c 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 water 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 定義資料夾位置和 CSV 檔案的副檔名 folder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' # 初始化 DataFrame water = pd.DataFrame() # 讀取目錄下的所有 CSV 檔案 for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 若資料列名有「pm25」，則將其更名為「PM25」 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 只選取特定 id 的資料 filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) # 移除沒有 timestamp 的資料列 water.dropna(subset=['timestamp'], inplace=True) # 將 timestamp 的時區資訊移除 for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive # 將 timestamp 設定為資料框的索引 water.set_index('timestamp', inplace=True) # 移除不需要的資料夾 !rm -rf Water CSV_Water 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 移除不需要的資料欄 water.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) # 根據 timestamp 排序資料 water.sort_values(by='timestamp', inplace=True) # 顯示資料的基本資訊 water.info() # 顯示前五筆資料 print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 氣象資料 我們從民生公共物聯網資料平台的歷史資料庫下載「中央氣象局自動氣象站」的歷史資料，並存入 Weather 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Weather 資料夾中。\n# 建立 Weather 和 CSV_Weather 資料夾 !mkdir Weather CSV_Weather # 從指定網址下載資料並儲存至 Weather 資料夾 !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" # 定義要解壓縮的目錄及檔案附檔名 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' # 解壓縮主目錄下的 zip 檔案 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() # 對 Weather 目錄下的子目錄進行解壓縮 for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() # 對子目錄中的子目錄進行解壓縮，並將 csv 檔案移至 CSV_Weather 資料夾 for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # 解壓縮至指定目錄 zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') 現在 CSV_Weather 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 C0U750 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 weather 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 設定資料夾路徑和副檔名 folder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' # 指定要篩選的站點 ID # 建立一個空的 DataFrame，用於儲存結果 weather = pd.DataFrame() # 讀取 CSV_Weather 資料夾中的所有 csv 檔案 for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) # 若 csv 檔中有「pm25」欄位，將其重新命名為「PM25」 if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) # 依據站點 ID 篩選資料 filtered = df.query(f'station_id==@id') # 將篩選後的資料加入 weather DataFrame weather = pd.concat([weather, filtered], ignore_index=True) # 重新命名某些欄位以便統一命名規則 weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) # 移除缺失 timestamp 的資料 weather.dropna(subset=['timestamp'], inplace=True) # 將 timestamp 轉換成 datetime 格式且移除時區資訊 for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive # 設定 timestamp 為 DataFrame 的索引 weather.set_index('timestamp', inplace=True) # 刪除資料夾以釋放空間 !rm -rf Weather CSV_Weather 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\n# 移除不需要的欄位「station_id」 weather.drop(columns=['station_id'], inplace=True) # 根據「timestamp」欄位進行排序 weather.sort_values(by='timestamp', inplace=True) # 顯示 DataFrame 的資訊摘要，包括每列的非空值數、資料型態等 weather.info() # 顯示 DataFrame 的前五行以確認資料結構 print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN 以上我們已經成功示範空品資料 (air)、水位資料 (water) 和氣象資料 (weather) 的讀取範例，在接下來的探討中，我們將以空品資料示範初步的時間序列資料處理，相同的方法也可以輕易改成使用水位資料或氣象資料而得到類似的結果，大家可以自行嘗試看看。\n資料預處理 (Preprocess) 我們首先依照章節 4.1 所介紹的方法對資料進行重新採樣，將資料分別取每小時平均 (air_hour)、每天平均 (air_day) 和每月平均 (air_month)。\n# 針對「air」DataFrame，使用 resample 並根據每小時對資料取平均值 air_hour = air.resample('H').mean() # 針對「air」DataFrame，使用 resample 並根據每日對資料取平均值 air_day = air.resample('D').mean() # 針對「air」DataFrame，使用 resample 並根據每月對資料取平均值 air_month = air.resample('M').mean() 接著我們依照章節 4.1 所介紹的離群值偵測方法，將 air_hour 資料中的離群值移除，並將移除後的缺失資料，使用 Forward fill 方法填回。\n# 將 air_hour 轉換為 TimeSeriesData 格式 air_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') # 使用 OutlierDetector 進行離群值檢測 outlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() # 移除離群值，這裡選擇不插值 outliers_removed = outlierDetection.remover(interpolate=False) # 將檢測後的資料轉回 DataFrame 並重新命名欄位 air_hour_df = outliers_removed.to_dataframe() air_hour_df.rename(columns={'time': 'timestamp', 'y_0': 'PM25'}, inplace=True) # 將 timestamp 設定為索引 air_hour_df.set_index('timestamp', inplace=True) air_hour = air_hour_df # 重新取樣為每小時並取平均值 air_hour = air_hour.resample('H').mean() # 用 Forward 方法填回缺失值 air_hour.ffill(inplace=True) 平穩性 (Stationary) 檢查 在進行資料預測的探究前，我們先針對資料的平穩性 (stationary) 進行檢查。我們先挑選想要進行檢測的時間區段（例如 2020-06-10 ~ 2020-06-17），並將這個區段的資料存入 data 變數。\n# 選擇日期範圍從 2020 年 6 月 10 日到 2020 年 6 月 17 日的資料 data = air_hour.loc['2020-06-10':'2020-06-17'] 接著我們計算這些資料的平均數 (mean) 與變異數 (var)，並且進行繪圖。\n# 將 data.PM25 的資料轉換為 numpy array nmp = data.PM25.to_numpy() size = np.size(nmp) # 初始化存儲累積平均和變異數的 array nmp_mean = np.zeros(size) nmp_var = np.zeros(size) # 計算每一步的累積平均和變異數 for i in range(size): nmp_mean[i] = nmp[:i+1].mean() # 到目前為止的平均 nmp_var[i] = nmp[:i+1].var() # 到目前為止的變異數 # 準備畫圖的資料 y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) # 使用 matplotlib 繪製累積平均和變異數的圖表 plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() 從圖中可以發現平均數的變化不大，但是變異數的變化卻很大。我們稱這樣的資料具備較差的平穩性；相反地，若是具備平穩性的資料，其平均數與變異數的變化不會與時間的推移有關。\n換句話說，如果資料分布隨著時間有一定的趨勢變化，那它就沒有平穩性；如果資料的分佈不會因為時間推移，平均數與變異數也維持固定，那它就有平穩性 (stationary)。平穩性的資料有利於尋找適合的的模型 (model) 並預測未來的數值。\n若要檢查資料是否具有平穩性，至少有下列兩種常見的方法：\nAugmented Dickey Fuller (ADF) test：使用 unit root test，如果 p-value \u003c 0.05，則資料具有平穩性。 Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test：與 ADF test 相反，如果 p-value \u003c 0.05，則資料不具有平穩性 (non-stationary)。 # 執行 ADF 檢定 result = adfuller(data.PM25.values, autolag='AIC') # 顯示 ADF 檢定結果 print(f'ADF Statistic: {result[0]}') # ADF 統計值 print(f'p-value: {result[1]}') # p 值 # 輸出 ADF 檢定的臨界值 for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # 執行 KPSS 檢定 result = kpss(data.PM25.values, regression='c') # 顯示 KPSS 檢定結果 print('\\nKPSS Statistic: %f' % result[0]) # KPSS 統計值 print('p-value: %f' % result[1]) # p 值 # 顯示 KPSS 檢定的臨界值 for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -2.7026194088541704 p-value: 0.07358609270498144 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.620177 p-value: 0.020802 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 若以我們使用的範例資料為例，經過 ADF 檢測得到的 p-value 為 0.073，因此該資料並沒有平穩性。為了達到平穩性，我們接下來將資料進行差分，也就是將第 i 筆資料減第 i-1 筆資料，並使用得到的結果再次進行檢測。\n在 dataframe 的資料格式上我們可以直接使用 data.diff() 來將資料差分 ，並將經過差分後的資料命名為 data_diff。\n# 對 data 進行一階差分 data_diff = data.diff() # 顯示差分後的資料 data_diff PM25 timestamp\t2020-06-10 00:00:00\tNaN 2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 我們可以看到第一筆資料為 Nan，這是因為第一筆資料無法減去前一筆資料，所以我們要將第一筆資料捨棄。\n# 從 data_diff 中移除第一筆資料 data_diff = data_diff[1:] # 顯示修改後的 data_diff data_diff PM25 timestamp\t2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 2020-06-10 05:00:00\t-1.300000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 接著我們將資料繪圖來觀察經過差分後資料的平均數與變異數隨時間變化的關係。\n# 從一階差分資料中取出 PM2.5 的值 nmp = data_diff.PM25.to_numpy() # 計算總共有多少資料點 size = np.size(nmp) # 初始化兩個陣列，分別儲存時序資料的累計平均和累計變異數 nmp_mean = np.zeros(size) nmp_var = np.zeros(size) # 透過迴圈，對每個時間點計算到目前為止的累計平均和變異數 for i in range(size): nmp_mean[i] = nmp[:i+1].mean() # 計算到第i個時間點的平均 nmp_var[i] = nmp[:i+1].var() # 計算到第i個時間點的變異數 # 建立資料序列以便於繪圖 y1 = nmp_mean[:] # 累計平均的資料序列 y2 = nmp_var[:] # 累計變異數的資料序列 y3 = nmp # 原始資料序列 x = np.arange(size) # x軸，代表時間點 # 使用matplotlib進行資料繪圖 plt.plot(x, y1, label='mean') # 繪製累計平均 plt.plot(x, y2, label='var') # 繪製累計變異數 plt.legend() # 繪製累計變異數 plt.show() # 顯示圖形 從以上的結果我們發現平均數的變化依然不大，而變異數的變化則變小了。我們接著重複上述的平穩性檢測步驟：\n# 使用 ADF (Augmented Dickey-Fuller) 測試檢查資料的定態性 result = adfuller(data_diff.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') # 輸出ADF統計值 print(f'p-value: {result[1]}') # 輸出p值，通常小於0.05表示資料是定態的 # 輸出不同信賴度下的臨界值，與 ADF 統計值進行比較 for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # 使用 KPSS (Kwiatkowski-Phillips-Schmidt-Shin) 測試檢查資料的定態性 result = kpss(data_diff.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) # 輸出 KPSS 統計值 print('p-value: %f' % result[1]) # 輸出 p 值，通常大於 0.05 表示資料是定態的 # 輸出不同信賴度下的臨界值，與 KPSS 統計值進行比較 for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -13.350457196046884 p-value: 5.682260865619701e-25 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.114105 p-value: 0.100000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 經過檢測後得到 ADF test 的 p-value 為 5.68e-25，由此可知經過一次差分後的資料便具有平穩性，這個結果會在本章節後續的預測模型使用到。\n資料預測(Data forecast) 經過前一部分的預處理後，這邊我們將會示範使用不同的預測模型來對時序資料進行預測，我們將依次使用 ARIMA、SARIMAX、auto_arima、Prophet、LSTM 與 Holt-Winter 模型。\nARIMA ARIMA 模型其實是 ARMA 模型的擴展版本，因此我們先介紹先介紹 ARMA 模型，並將 ARMA 模型拆成兩部分，分別是:\n自迴歸模型 (AR, autogressive model)：使用一個參數 p，並以前 p 個歷史值做線性組合來預測當下的數值。 移動平均模型 (MA, moving average model)：使用一個參數 q，並以前 q 個使用 AR 模型的預測誤差進行線性組合，以預測當下的數值。 而 ARIMA 模型比 ARMA 模型還多使用一個參數 d，還記得前面的平穩性檢查嗎？如果資料不具有平穩性就要做差分，而參數 d 就代表需要做差分的次數。\n以下我們使用空品資料進行演練。首先，我們將資料製圖以選擇要使用的資料片段：\n# 使用 .loc 方法從資料集 air_hour 中選取 2020 年 6 月 1 日到 2020 年 6 月 30 日期間的 PM2.5 資料 # 之後對此段時間內的 PM2.5 數值進行繪圖 # figsize 參數確保圖形的大小為 12x8 英吋 air_hour.loc['2020-06-01':'2020-06-30']['PM25'].plot(figsize=(12, 8)) 我們選擇一段想要使用的資料，並將資料分為兩部分：\n訓練資料 (train data)：用來訓練模型，找出最適合模型的參數。 測試資料 (test data)：當得到訓練模型後，可用於評估該模型在資料預測上的準確度。 在我們接下來的範例中，我們設定測試資料的長度為 48 小時 (train_len=-48)，訓練資料則為扣除最後 48 小時的全部資料。\n# 從 air_hour 資料集中選擇 2020 年 6 月 17 日到 2020 年 6 月 21 日期間的資料 data_arima = air_hour.loc['2020-06-17':'2020-06-21'] # 定義訓練資料的長度。由於訓練資料末端是測試資料的開始，所以使用負數表示從資料末端算回去的資料點數 # 這裡的 -48 代表訓練資料的最後 48 個資料點將作為測試資料 train_len = -48 # 使用 iloc 函式根據上面定義的 train_len 從 data_arima 中取得訓練資料 train = data_arima.iloc[:train_len] # 將剩餘的資料作為測試資料 test = data_arima.iloc[train_len:] 我們首先判斷這段資料是具有平穩性，並以差分的次數決定 d 參數的數值\n# 進行 Dickey-Fuller 單根檢定以檢查訓練資料集的定態性 result = adfuller(train) # 輸出檢定統計值 print('The test stastics:', result[0]) # 輸出 p 值。p 值低於特定臨界值 (如 0.05) 時，我們將拒絕原假設，認為時間序列是定態的 print(\"The p-value:\", result[1]) The test stastics: -3.1129543556288826 The p-value: 0.025609243615341074 由於 p-value 已經比 0.05 小，因此我們不需要進行差分（亦即 d=0）即可繼續探討 ARIMA模型中的參數 p 和參數 q，而比較簡單的方法就是將可能的 p、q 組合分別帶入模型後，再從中判斷模型的好壞。\n我們可以使用 AIC 或 BIC 方法，來判斷模型跟訓練資料是否擬合，一般來說，其判斷出來的數值越小代表模型的效果越好。例如，我們先將 p 和 q 的範圍限制在 0~2 之間，這樣總共有 9 種可能的組合，再分別查看其 AIC 與 BIC 的數值，並以數值最小的 p 和 q 組合，作為這兩個參數的決定值。\n# 忽略所有警告，使輸出更加乾淨 warnings.filterwarnings('ignore') order_aic_bic =[] # 對 p 值（AR 項）進行 0 到 2 的迴圈 for p in range(3): # 對 q 值（MA 項）進行 0 到 2 的迴圈 for q in range(3): try: # 創建並擬合 SARIMA (p,0,q) 模型（其中 d（差分項）被固定為 0） model = sm.tsa.statespace.SARIMAX(train['PM25'], order=(p, 0, q)) results = model.fit() # 將 p, q 和計算出的 AIC、BIC 值存儲起來 order_aic_bic.append((p, q, results.aic, results.bic)) except: # 若模型無法擬合，輸出 p 和 q 值 print(p, q, None, None) # 將 p, q 值及其相應的 AIC、BIC 轉換成 DataFrame order_df = pd.DataFrame(order_aic_bic, columns=['p', 'q', 'aic','bic']) # 根據 AIC 和 BIC 對模型進行排序 # 根據 AIC 排序 print(\"Sorted by AIC \") # print(\"\\n\") print(order_df.sort_values('aic').reset_index(drop=True)) # 根據 BIC 排序 print(\"Sorted by BIC \") # print(\"\\n\") print(order_df.sort_values('bic').reset_index(drop=True)) Sorted by AIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 Sorted by BIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 我們可以發現當 (p,q) = (1,0) 時，AIC 和 BIC 的值最小，代表這是最好的模型組態，因此我們決定 p、d、q 這三個參數分別設為 1, 0, 0 後就可以正式開始訓練模型。\n# 建立 ARIMA 模型物件 # 此處使用的模型設定為 ARIMA(1,0,0)，這意味著AR(1)模型，沒有整合(I)和移動平均(MA)成分。 model = ARIMA(train, order=(1,0,0)) # 使用資料對模型進行擬合 results = model.fit() # 顯示模型的摘要資訊 print(results.summary()) # 繪製模型診斷圖，這可以幫助我們檢查模型擬合的品質 results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ============================================================================== Dep. Variable: PM25 No. Observations: 72 Model: ARIMA(1, 0, 0) Log Likelihood -168.853 Date: Fri, 26 Aug 2022 AIC 343.706 Time: 05:01:13 BIC 350.536 Sample: 06-17-2020 HQIC 346.425 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ const 6.3774 1.959 3.255 0.001 2.537 10.218 ar.L1 0.7792 0.047 16.584 0.000 0.687 0.871 sigma2 6.2934 0.746 8.438 0.000 4.832 7.755 =================================================================================== Ljung-Box (L1) (Q): 0.08 Jarque-Bera (JB): 76.49 Prob(Q): 0.77 Prob(JB): 0.00 Heteroskedasticity (H): 2.30 Skew: 1.44 Prob(H) (two-sided): 0.05 Kurtosis: 7.15 =================================================================================== 接著我們使用測試資料進行預測，並評估其預測的效果，從圖形化的資料呈現中，我們可以發現資料預測結果的曲線太過平滑，和實際上的數值差異很大。事實上，若觀察整體資料的變化趨勢，會發現資料本身存在有規律的起伏，而 ARIMA 只能預測出資料的趨勢，若要準確的預測資料的數值，其結果仍有極大的差距。\n# 使用預先訓練的 ARIMA 模型，對指定時間範圍內的資料進行預測 # start 和 end 參數確定了預測的時間範圍 data_arima['forecast'] = results.predict(start=24*5-48, end=24*5) # 使用 matplotlib 將真實的 PM2.5 值和預測值繪製在同一個圖上 # 這可以幫助我們直觀地看到模型的預測品質如何 data_arima[['PM25', 'forecast']].plot(figsize=(12, 8)) SARIMAX # 從「2020-06-17」到「2020-06-21」選取 PM2.5 的資料 data_sarimax = air_hour.loc['2020-06-17':'2020-06-21'] # 設定訓練資料的長度為從開始到最後的 48 小時之前 train_len = -48 # 使用 iloc[] 擷取訓練資料集 train = data_sarimax.iloc[:train_len] # 使用 iloc[] 擷取測試資料集 test = data_sarimax.iloc[train_len:] 我們接著介紹 SARIMAX 模型。SARIMAX 模型共有七個參數，分別是 p, d, q, P, D, Q, s；這些參數可以分為兩組：第一組為 order=(p, d, q) 這三個參數跟 ARIMA 模型的參數一樣；另一組是 seasonal_order=(P, D, Q, s)，也就是週期性的 AR 模型參數、週期性的差分次數、週期性的MA模型參數，最後再加上一個週期性長度的參數。\n參數 說明 p AR 模型參數 d 達到平穩性所需要的差分次數 q MA 模型參數 P 週期性的 AR 模型參數 D 週期上達到平穩性所需要的差分次數 Q 週期性的 MA 模型參數 s 週期長度 由於從先前的觀察可以發現，這些資料大致上約 24 個小時會有一個週期性的變化，因此我們讓 s=24，並用下列的指令進行模型建立。\n# 創建 SARIMAX 模型實例 model = SARIMAX(train, order=(1,0,0), seasonal_order=(0, 1, 0, 24)) # 擬合模型到訓練資料 results = model.fit() # 顯示模型摘要 print(results.summary()) # 繪製模型的診斷圖 results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ========================================================================================== Dep. Variable: PM25 No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:26 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== 接下來我們使用測試資料進行資料預測，並將預測結果用視覺化方式呈現，可以發現相較於 ARIMA 模型，SARIMA 模型的預測結果雖然仍有待加強，但已比 ARIMA 模型進步許多。\n# 總共有五天的資料，我們的目標是預測最後兩天的資料，這代表著48個小時的資料。 # 起始小時為 24*5-48（也就是第三天的結束），結束小時為 24*5（第五天的結束）。 data_sarimax['forecast'] = results.predict(start=24*5-48, end=24*5) data_sarimax[['PM25', 'forecast']].plot(figsize=(12, 8)) auto_arima 我們使用 pmdarima 這個 Python 套件，這個套件類似 R 語言中的 auto.arima 模型，可以自動尋找最合適的 ARIMA 模型參數，增加使用者在使用 ARIMA 模型時的方便性。目前 pmdarima 套件中的 pmdarima.ARIMA 物件，其實便同時包含了 ARMA, ARIMA 和 SARIMAX 這三種模型，而使用 pmdarima.auto_arima 方法時，只要提供參數 p, q, P, Q 的範圍，便能在指定的範圍內尋找出最適合的參數組合。\n我們接下來實作 pmdarima.auto_arima 的使用方法，先把資料集切分為訓練資料與預測資料：\n# 從 air_hour 中選取 2020-06-17 到 2020-06-21 的資料 data_autoarima = air_hour.loc['2020-06-17':'2020-06-21'] # 設定要取出的測試資料長度，此處設為48（代表兩天的小時數） train_len = -48 # 將資料分割為訓練資料和測試資料 # 用 iloc 取出前面的資料作為訓練資料 train = data_autoarima.iloc[:train_len] # 用 iloc 取出最後兩天的資料作為測試資料 test = data_autoarima.iloc[train_len:] 針對 p, q, P, Q 這四個參數，我們分別用 start 和 max 來指定對應的範圍，同時設定週期性參數 seasonal 為 True，並且設定週期變數 m 為 24 小時。接著我們便可以直接執行得到最佳的模型參數組合和模型擬合結果。\n# 使用 auto_arima 函式自動找到最適合的 ARIMA 模型參數 # train：輸入的訓練資料 # start_p, d, start_q：開始測試的 p、d 和 q 值 # max_p, max_d, max_q：p、d 和 q 的最大值 # start_P, D, start_Q：季節性參數的起始值 # max_P, max_D, max_Q：季節性參數的最大值 # m=24：季節性週期，此處設定為24，因為我們的資料是每小時的，所以一天有24個小時 # seasonal=True：考慮季節性 # error_action='warn'：如果出現錯誤，則發出警告 # trace=True：追踪步驟，顯示過程 # supress_warnings=True：壓制警告 # stepwise=True：以步驟方式搜索模型參數，這通常會使搜索更快 # random_state=20：確保結果的可重複性 # n_fits=20：進行20次不同參數的擬合 results = pm.auto_arima(train,start_p=0, d=0, start_q=0, max_p=5, max_d=5, max_q=5, start_P=0, D=1, start_Q=0, max_P=5, max_D=5, max_Q=5, m=24, seasonal=True, error_action='warn', trace = True, supress_warnings=True, stepwise = True, random_state=20, n_fits = 20) print(results.summary()) Performing stepwise search to minimize aic ARIMA(0,0,0)(0,1,0)[24] intercept : AIC=268.023, Time=0.04 sec ARIMA(1,0,0)(1,1,0)[24] intercept : AIC=247.639, Time=0.85 sec ARIMA(0,0,1)(0,1,1)[24] intercept : AIC=250.711, Time=0.79 sec ARIMA(0,0,0)(0,1,0)[24] : AIC=271.305, Time=0.04 sec ARIMA(1,0,0)(0,1,0)[24] intercept : AIC=247.106, Time=0.09 sec ARIMA(1,0,0)(0,1,1)[24] intercept : AIC=247.668, Time=0.45 sec ARIMA(1,0,0)(1,1,1)[24] intercept : AIC=inf, Time=2.63 sec ARIMA(2,0,0)(0,1,0)[24] intercept : AIC=249.013, Time=0.15 sec ARIMA(1,0,1)(0,1,0)[24] intercept : AIC=248.924, Time=0.21 sec ARIMA(0,0,1)(0,1,0)[24] intercept : AIC=250.901, Time=0.11 sec ARIMA(2,0,1)(0,1,0)[24] intercept : AIC=250.579, Time=0.30 sec ARIMA(1,0,0)(0,1,0)[24] : AIC=246.926, Time=0.06 sec ARIMA(1,0,0)(1,1,0)[24] : AIC=247.866, Time=0.28 sec ARIMA(1,0,0)(0,1,1)[24] : AIC=247.933, Time=0.31 sec ARIMA(1,0,0)(1,1,1)[24] : AIC=inf, Time=2.35 sec ARIMA(2,0,0)(0,1,0)[24] : AIC=248.910, Time=0.08 sec ARIMA(1,0,1)(0,1,0)[24] : AIC=248.893, Time=0.09 sec ARIMA(0,0,1)(0,1,0)[24] : AIC=252.779, Time=0.08 sec ARIMA(2,0,1)(0,1,0)[24] : AIC=250.561, Time=0.17 sec Best model: ARIMA(1,0,0)(0,1,0)[24] Total fit time: 9.122 seconds SARIMAX Results ========================================================================================== Dep. Variable: y No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:37 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== 最後我們使用尋找到的最佳模型進行資料預測，並且將預測結果與測試資料用疊圖的方式繪製在同一張圖上，由於本次所找到最佳模型即為剛剛介紹 SARIMAX 時的最佳模型參數組合，因此兩者的預測結果也大致相同。\n# 使用先前擬合的 ARIMA 模型進行預測 # n_periods=10：預測接下來的 10 個時段（在這個例子中，即預測接下來的10個小時） results.predict(n_periods=10) 2020-06-20 00:00:00 10.371336 2020-06-20 01:00:00 13.142043 2020-06-20 02:00:00 13.505843 2020-06-20 03:00:00 9.506395 2020-06-20 04:00:00 7.450378 2020-06-20 05:00:00 7.782850 2020-06-20 06:00:00 7.633757 2020-06-20 07:00:00 5.200781 2020-06-20 08:00:00 3.634188 2020-06-20 09:00:00 3.946824 Freq: H, dtype: float64 # 使用先前擬合的 ARIMA 模型進行預測 # n_periods=48：預測接下來的 48 個時段 # 將預測值新增到 data_autoarima DataFrame 的「forecast」列中 data_autoarima['forecast']= pd.DataFrame(results.predict(n_periods=48), index=test.index) # 在同一圖上繪製原始 PM2.5 資料和預測結果 data_autoarima[['PM25', 'forecast']].plot(figsize=(12, 8)) Prophet 我們接下來使用 kats 套件中提供的 Prophet 模型進行資料預測，這個模型是由 Facebook 的資料科學團隊提出，擅長針對週期性強的時間序列資料進行預測，並且可以容忍缺失資料 (missing data)、資料偏移 (shift) 以及偏離值 (outlier)。\n我們首先把資料集切分為訓練資料與預測資料，並用折線圖的方式呈現訓練資料的變化狀況。\n# 從 air_hour 中選擇特定的日期區間 data_prophet = air_hour.loc['2020-06-17':'2020-06-21'] # 定義訓練集和測試集的資料 # 由於我們要預測最後兩天，所以訓練集是從開始到倒數第二天，測試集則是最後兩天 train_len = -48 train = data_prophet.iloc[:train_len] test = data_prophet.iloc[train_len:] # 將訓練資料轉換為 Kats 的 TimeSeriesData 物件 # 這裡重置索引是因為 TimeSeriesData 需要時間作為一個欄位而不是索引 trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') # 使用 TimeSeriesData 的 plot 函式繪製 PM2.5 的資料趨勢 trainData.plot(cols=[\"PM25\"]) 我們接著使用 ProphetParams 設定 Prophet 模型的參數，並將訓練資料與參數用於初始化設定 ProphetModel，接著我們使用 fit 方法建立模型，並用 predict 方法進行資料預測，便能得到最後預測的結果。\n# 指定模型參數，這裡選擇了乘性季節性模式 params = ProphetParams(seasonality_mode=\"multiplicative\") # 使用訓練資料和上述參數，創建一個 Prophet 模型的實例 m = ProphetModel(trainData, params) # 擬合模型 m.fit() # 預測未來 48 小時的資料 fcst = m.predict(steps=48, freq=\"H\") # 將預測結果加入到原始的 data_prophet 資料框中 data_prophet['forecast'] = fcst[['time','fcst']].set_index('time') # 最後顯示預測結果 fcst time\tfcst\tfcst_lower fcst_upper 0\t2020-06-20 00:00:00\t14.705192\t12.268361\t17.042476 1\t2020-06-20 01:00:00\t15.089580\t12.625568\t17.573396 2\t2020-06-20 02:00:00\t14.921077\t12.459802\t17.411335 3\t2020-06-20 03:00:00\t13.846131\t11.444988\t16.200284 4\t2020-06-20 04:00:00\t12.278140\t9.863531\t14.858334 5\t2020-06-20 05:00:00\t10.934739\t8.372450\t13.501025 6\t2020-06-20 06:00:00\t10.126712\t7.654647\t12.658054 7\t2020-06-20 07:00:00\t9.535067\t7.034313\t11.762639 8\t2020-06-20 08:00:00\t8.661877\t6.255147\t11.132732 9\t2020-06-20 09:00:00\t7.424133\t5.055052\t9.770750 10\t2020-06-20 10:00:00\t6.229786\t3.640543\t8.625856 11\t2020-06-20 11:00:00\t5.464764\t3.039011\t7.939283 12\t2020-06-20 12:00:00\t4.998005\t2.692023\t7.550191 13\t2020-06-20 13:00:00\t4.334771\t1.961382\t6.506875 14\t2020-06-20 14:00:00\t3.349172\t1.059836\t5.768178 15\t2020-06-20 15:00:00\t2.819902\t0.399350\t5.226658 16\t2020-06-20 16:00:00\t4.060070\t1.556264\t6.322976 17\t2020-06-20 17:00:00\t7.792830\t5.331987\t10.237182 18\t2020-06-20 18:00:00\t13.257767\t10.873149\t15.542380 19\t2020-06-20 19:00:00\t18.466805\t15.895210\t20.874602 20\t2020-06-20 20:00:00\t21.535994\t19.150397\t23.960260 21\t2020-06-20 21:00:00\t22.005943\t19.509141\t24.691836 22\t2020-06-20 22:00:00\t21.014449\t18.610361\t23.661906 23\t2020-06-20 23:00:00\t20.191905\t17.600568\t22.868388 24\t2020-06-21 00:00:00\t20.286952\t17.734177\t22.905280 25\t2020-06-21 01:00:00\t20.728067\t18.235829\t23.235212 26\t2020-06-21 02:00:00\t20.411124\t17.755181\t22.777073 27\t2020-06-21 03:00:00\t18.863739\t16.261775\t21.315573 28\t2020-06-21 04:00:00\t16.661351\t13.905466\t19.374726 29\t2020-06-21 05:00:00\t14.781150\t12.401465\t17.499478 30\t2020-06-21 06:00:00\t13.637436\t11.206142\t16.239831 31\t2020-06-21 07:00:00\t12.793609\t9.940829\t15.319559 32\t2020-06-21 08:00:00\t11.580455\t9.059603\t14.261605 33\t2020-06-21 09:00:00\t9.891025\t7.230943\t12.471543 34\t2020-06-21 10:00:00\t8.271552\t5.840853\t10.677227 35\t2020-06-21 11:00:00\t7.231671\t4.829449\t9.733231 36\t2020-06-21 12:00:00\t6.592515\t4.108251\t9.107216 37\t2020-06-21 13:00:00\t5.699548\t3.288052\t8.019402 38\t2020-06-21 14:00:00\t4.389985\t1.848621\t6.825121 39\t2020-06-21 15:00:00\t3.685033\t1.196467\t6.150064 40\t2020-06-21 16:00:00\t5.289956\t2.907623\t8.012851 41\t2020-06-21 17:00:00\t10.124029\t7.397842\t12.676256 42\t2020-06-21 18:00:00\t17.174959\t14.670539\t19.856592 43\t2020-06-21 19:00:00\t23.856724\t21.102924\t26.712359 44\t2020-06-21 20:00:00\t27.746195\t24.636118\t30.673178 45\t2020-06-21 21:00:00\t28.276321\t25.175013\t31.543197 46\t2020-06-21 22:00:00\t26.932054\t23.690073\t29.882014 47\t2020-06-21 23:00:00\t25.811943\t22.960132\t28.912079 # 顯示包括了原始的 PM2.5 值以及由 Prophet 模型所預測的值 data_prophet timestamp\tPM25\tforecast 2020-06-17 00:00:00\t6.300000\tNaN 2020-06-17 01:00:00\t11.444444\tNaN 2020-06-17 02:00:00\t6.777778\tNaN 2020-06-17 03:00:00\t4.875000\tNaN 2020-06-17 04:00:00\t5.444444\tNaN ...\t...\t... 2020-06-21 19:00:00\t18.777778\t23.856724 2020-06-21 20:00:00\t21.400000\t27.746195 2020-06-21 21:00:00\t11.222222\t28.276321 2020-06-21 22:00:00\t9.800000\t26.932054 2020-06-21 23:00:00\t8.100000\t25.811943 我們接著使用 ProphetModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\n# 將原始的時間序列資料和模型的預測結果繪製在同一個圖上 m.plot() 為了更容易觀察預測結果的正確性，我們使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，藍色曲線與同時間的黑色曲線在變化趨勢與數值區間皆十分相似，整體來說預測結果已可算是差強人意。\n# 創建一個 12x7 英吋的圖和軸物件 fig, ax = plt.subplots(figsize=(12, 7)) # 在同一個軸上繪製訓練資料，並標記為「train」，顏色設為黑色 train.plot(ax=ax, label='train', color='black') # 在同一個軸上繪製測試資料，顏色設為黑色 test.plot(ax=ax, color='black') # 在同一個軸上繪製預測值，顏色設為藍色 fcst.plot(x='time', y='fcst', ax=ax, color='blue') # 繪製預測值的信賴區間，使用藍色區域表示 ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) # 移除圖例，讓圖表看起來更乾淨 ax.get_legend().remove() LSTM 接下來，我們介紹使用 LSTM (long short-term memory) 模型進行資料預測。LSTM 模型是一種適合使用在連續資料的預測模型，因為它會對不同時間的資料產生不同的長短期記憶，並藉此預測出最後的結果。目前在 kats 套件中就有提供 LSTM 模型，因此我們可以直接用類似使用 Prophet 模型的語法進行操作。\n我們首先把資料集切分為訓練資料與預測資料，並用繪圖的方式查看訓練資料的變化狀況。\n# 選取 2020 年 6 月 17 日至 6 月 21 日的小時資料 data_lstm = air_hour.loc['2020-06-17':'2020-06-21'] # 設定訓練資料的長度為最後兩天前的所有資料 train_len = -48 # 切割資料為訓練集和測試集 train = data_lstm.iloc[:train_len] test = data_lstm.iloc[train_len:] # 使用 Kats 的 TimeSeriesData 物件轉換訓練資料 trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') # 使用 matplotlib 繪製訓練資料的 PM2.5 資料圖 trainData.plot(cols=[\"PM25\"]) 接著我們依序選擇 LSTM 模型的各項參數，分別是訓練次數 (num_epochs)、一次讀入的資料時間長度 (time_window)、還有跟長短期記憶比較相關的神經網路層數 (hidden_size)，然後便可以直接進行模型訓練與資料預測。\n# 設定 LSTM 模型的參數 params = LSTMParams( hidden_size=10, # 隱藏層的大小 time_window=24, # 時間窗格的大小 num_epochs=30 # 訓練時的迭代次數 ) # 使用上述參數建立一個 LSTM 模型實例 m = LSTMModel(trainData, params) # 對模型進行訓練 m.fit() # 使用模型進行預測，預測接下來的 48 小時 fcst = m.predict(steps=48, freq=\"H\") # 將預測結果添加到 data_lstm 資料框中 data_lstm['forecast'] = fcst[['time', 'fcst']].set_index('time') # 顯示預測結果 fcst time\tfcst\tfcst_lower\tfcst_upper 0\t2020-06-20 00:00:00\t11.905971\t11.310672\t12.501269 1\t2020-06-20 01:00:00\t10.804338\t10.264121\t11.344554 2\t2020-06-20 02:00:00\t9.740741\t9.253704\t10.227778 3\t2020-06-20 03:00:00\t8.696406\t8.261586\t9.131226 4\t2020-06-20 04:00:00\t7.656923\t7.274077\t8.039769 5\t2020-06-20 05:00:00\t6.608442\t6.278019\t6.938864 6\t2020-06-20 06:00:00\t5.543790\t5.266600\t5.820979 7\t2020-06-20 07:00:00\t4.469023\t4.245572\t4.692474 8\t2020-06-20 08:00:00\t3.408312\t3.237897\t3.578728 9\t2020-06-20 09:00:00\t2.411980\t2.291381\t2.532578 10\t2020-06-20 10:00:00\t1.564808\t1.486567\t1.643048 11\t2020-06-20 11:00:00\t0.982147\t0.933040\t1.031255 12\t2020-06-20 12:00:00\t0.792612\t0.752981\t0.832242 13\t2020-06-20 13:00:00\t1.105420\t1.050149\t1.160691 14\t2020-06-20 14:00:00\t1.979013\t1.880062\t2.077964 15\t2020-06-20 15:00:00\t3.408440\t3.238018\t3.578862 16\t2020-06-20 16:00:00\t5.337892\t5.070997\t5.604786 17\t2020-06-20 17:00:00\t7.659332\t7.276365\t8.042299 18\t2020-06-20 18:00:00\t10.104147\t9.598940\t10.609355 19\t2020-06-20 19:00:00\t12.047168\t11.444809\t12.649526 20\t2020-06-20 20:00:00\t12.880240\t12.236228\t13.524252 21\t2020-06-20 21:00:00\t12.748750\t12.111312\t13.386187 22\t2020-06-20 22:00:00\t12.128366\t11.521947\t12.734784 23\t2020-06-20 23:00:00\t11.311866\t10.746273\t11.877459 24\t2020-06-21 00:00:00\t10.419082\t9.898128\t10.940036 25\t2020-06-21 01:00:00\t9.494399\t9.019679\t9.969119 26\t2020-06-21 02:00:00\t8.551890\t8.124296\t8.979485 27\t2020-06-21 03:00:00\t7.592260\t7.212647\t7.971873 28\t2020-06-21 04:00:00\t6.613075\t6.282421\t6.943729 29\t2020-06-21 05:00:00\t5.614669\t5.333936\t5.895402 30\t2020-06-21 06:00:00\t4.605963\t4.375664\t4.836261 31\t2020-06-21 07:00:00\t3.611552\t3.430974\t3.792129 32\t2020-06-21 08:00:00\t2.679572\t2.545593\t2.813550 33\t2020-06-21 09:00:00\t1.887442\t1.793070\t1.981814 34\t2020-06-21 10:00:00\t1.340268\t1.273255\t1.407282 35\t2020-06-21 11:00:00\t1.156494\t1.098669\t1.214318 36\t2020-06-21 12:00:00\t1.440240\t1.368228\t1.512252 37\t2020-06-21 13:00:00\t2.251431\t2.138859\t2.364002 38\t2020-06-21 14:00:00\t3.592712\t3.413076\t3.772347 39\t2020-06-21 15:00:00\t5.415959\t5.145161\t5.686757 40\t2020-06-21 16:00:00\t7.613187\t7.232528\t7.993847 41\t2020-06-21 17:00:00\t9.918564\t9.422636\t10.414493 42\t2020-06-21 18:00:00\t11.755348\t11.167580\t12.343115 43\t2020-06-21 19:00:00\t12.576593\t11.947764\t13.205423 44\t2020-06-21 20:00:00\t12.489052\t11.864599\t13.113504 45\t2020-06-21 21:00:00\t11.915885\t11.320090\t12.511679 46\t2020-06-21 22:00:00\t11.133274\t10.576610\t11.689938 47\t2020-06-21 23:00:00\t10.264495\t9.751270\t10.777719 我們一樣使用 LSTMModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\nm.plot() 為了觀察預測結果的正確性，我們也使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，從圖中可以觀察到藍色曲線與同時間的黑色曲線在變化趨勢上大致一致，但整體來說資料預測的結果 (藍線) 則比實際測試資料 (黑線) 的數值略低一些。\n# 創建一個 12x7 大小的圖形和對應的座標軸 fig, ax = plt.subplots(figsize=(12, 7)) # 在圖形上畫出訓練資料，並標記為'train'，使用黑色表示 train.plot(ax=ax, label='train', color='black') # 在圖形上畫出測試資料，使用黑色表示 test.plot(ax=ax, color='black') # 在圖形上畫出預測資料，使用藍色表示 fcst.plot(x='time', y='fcst', ax=ax, color='blue') # 填充預測結果的上下界範圍，使用透明度為 0.1 的區域表示不確定性範圍 ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) # 移除圖例 ax.get_legend().remove() Holt-Winter 我們也使用 kats 套件提供的 Holt-Winter 模型，這是一種利用移動平均的概念，分配歷史資料的權重，以進行資料預測的方法。我們一樣先把資料集切分為訓練資料與預測資料，並用繪圖的方式查看訓練資料的變化狀況。\n# 選擇「2020-06-17」到「2020-06-21」期間的資料 data_hw = air_hour.loc['2020-06-17':'2020-06-21'] # 設定測試資料的長度為48筆 train_len = -48 # 切分訓練資料和測試資料 train = data_hw.iloc[:train_len] test = data_hw.iloc[train_len:] # 將訓練資料轉換成 Kats 的 TimeSeriesData 格式 trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') # 使用 matplotlib 繪製訓練資料的 PM25 時間序列圖 trainData.plot(cols=[\"PM25\"]) 接著我們需要設定 Holt-Winter 模型的參數，分別是設定使用加法或乘法來分解時序資料 (以下範例使用乘法 mul)，以及週期性的長度 (以下範例設為 24 小時)，然後便可以進行模型訓練與資料預測。\n# 忽略所有警告，使輸出更加乾淨 warnings.simplefilter(action='ignore') # 設定 Holt-Winters 模型的參數： # - 趨勢 (trend) 使用乘法方法 # - 季節性 (seasonal) 也使用乘法方法 # - 季節性週期 (seasonal_periods) 設定為 24，代表一天的小時數。 params = HoltWintersParams( trend=\"mul\", seasonal=\"mul\", seasonal_periods=24, ) # 建立 Holt-Winters 時序模型的實例。 # 輸入包括先前定義的資料集和參數。 m = HoltWintersModel( data=trainData, params=params) # 使用訓練資料集擬合模型 m.fit() # 預測未來48小時的資料 fcst = m.predict(steps=48, freq='H') # 將預測結果添加到原始的資料框中 data_hw['forecast'] = fcst[['time', 'fcst']].set_index('time') # 顯示預測結果 fcst time\tfcst 72\t2020-06-20 00:00:00\t14.140232 73\t2020-06-20 01:00:00\t14.571588 74\t2020-06-20 02:00:00\t12.797056 75\t2020-06-20 03:00:00\t10.061594 76\t2020-06-20 04:00:00\t9.927476 77\t2020-06-20 05:00:00\t8.732691 78\t2020-06-20 06:00:00\t10.257460 79\t2020-06-20 07:00:00\t8.169070 80\t2020-06-20 08:00:00\t6.005400 81\t2020-06-20 09:00:00\t5.038056 82\t2020-06-20 10:00:00\t6.391835 83\t2020-06-20 11:00:00\t5.435677 84\t2020-06-20 12:00:00\t3.536135 85\t2020-06-20 13:00:00\t2.725477 86\t2020-06-20 14:00:00\t2.588198 87\t2020-06-20 15:00:00\t2.967987 88\t2020-06-20 16:00:00\t3.329448 89\t2020-06-20 17:00:00\t4.409821 90\t2020-06-20 18:00:00\t10.295263 91\t2020-06-20 19:00:00\t10.587033 92\t2020-06-20 20:00:00\t14.061718 93\t2020-06-20 21:00:00\t18.597275 94\t2020-06-20 22:00:00\t12.040684 95\t2020-06-20 23:00:00\t12.124081 96\t2020-06-21 00:00:00\t13.522973 97\t2020-06-21 01:00:00\t13.935499 98\t2020-06-21 02:00:00\t12.238431 99\t2020-06-21 03:00:00\t9.622379 100\t2020-06-21 04:00:00\t9.494116 101\t2020-06-21 05:00:00\t8.351486 102\t2020-06-21 06:00:00\t9.809694 103\t2020-06-21 07:00:00\t7.812468 104\t2020-06-21 08:00:00\t5.743248 105\t2020-06-21 09:00:00\t4.818132 106\t2020-06-21 10:00:00\t6.112815 107\t2020-06-21 11:00:00\t5.198396 108\t2020-06-21 12:00:00\t3.381773 109\t2020-06-21 13:00:00\t2.606503 110\t2020-06-21 14:00:00\t2.475216 111\t2020-06-21 15:00:00\t2.838426 112\t2020-06-21 16:00:00\t3.184109 113\t2020-06-21 17:00:00\t4.217320 114\t2020-06-21 18:00:00\t9.845847 115\t2020-06-21 19:00:00\t10.124881 116\t2020-06-21 20:00:00\t13.447887 117\t2020-06-21 21:00:00\t17.785455 118\t2020-06-21 22:00:00\t11.515076 119\t2020-06-21 23:00:00\t11.594832 我們一樣使用 HoltWintersModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\n# 使用模型的 plot() 方法來繪製原始資料、預測值和預測的信賴區間 m.plot() 為了觀察預測結果的正確性，我們也使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，從圖中可以觀察到藍色曲線與同時間的黑色曲線在變化趨勢與數值區間皆大致一致，但整體來說資料預測的結果 (藍線) 對於上升坡段的反應略慢於測試資料 (黑線)。\n# 創建一個 12x7 的圖形和座標軸 fig, ax = plt.subplots(figsize=(12, 7)) # 繪製訓練資料，以黑色線表示 train.plot(ax=ax, label='train', color='black') # 繪製測試資料，也是以黑色線表示 test.plot(ax=ax, color='black') # 繪製預測結果，使用藍色線 fcst.plot(x='time', y='fcst', ax=ax, color='blue') # 填充預測的信賴區間，使用淡藍色 # ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) # 移除圖例 ax.get_legend().remove() 綜合比較 最後，為了方便觀察與比較起見，我們將剛剛介紹的六種預測模型的預測結果，同時繪製在下方的圖中（註：要先跑過上面所有預測模型的程式碼才看得到六張圖），可以清楚地觀察與比較六種模型在不同時間區間與曲線變化特性下的預測準確度，方便使用者決定最終的模型選擇，以及未來的可能應用。\n# 創建一個 3x2 的子圖，整體圖的大小為 12x8 fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8)) # 在第 1 行第 1 列的子圖中，繪製 ARIMA 的真實資料和預測資料 data_arima[['PM25', 'forecast']].plot(ax=axes[0, 0], title='ARIMA') # 在第 2 行第 1 列的子圖中，繪製 SARIMAX 的真實資料和預測資料 data_sarimax[['PM25', 'forecast']].plot(ax=axes[1, 0], title='SARIMAX') # 在第 3 行第 1 列的子圖中，繪製 auto_arima 的真實資料和預測資料 data_autoarima[['PM25', 'forecast']].plot(ax=axes[2, 0], title='auto_arima') # 在第 1 行第 2 列的子圖中，繪製 Prophet 的真實資料和預測資料 data_prophet[['PM25', 'forecast']].plot(ax=axes[0, 1], title='Prophet') # 在第 2 行第 2 列的子圖中，繪製 LSTM 的真實資料和預測資料 data_lstm[['PM25', 'forecast']].plot(ax=axes[1, 1], title='LSTM') # 在第 3 行第 2 列的子圖中，繪製 Holt-Winter 的真實資料和預測資料 data_hw[['PM25', 'forecast']].plot(ax=axes[2, 1], title='Holt-Winter') # 調整子圖的間距，使子圖不會重疊 fig.tight_layout(pad=1, w_pad=2, h_pad=5) 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Rob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice, 3rd edition (https://otexts.com/fpp3/) Stationarity, NIST Engineering Statistics Handbook (https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm) Unit root test - Wikipedia (https://en.wikipedia.org/wiki/Unit_root_test) Akaike information criterion (AIC) - Wikipedia (https://en.wikipedia.org/wiki/Akaike_information_criterion) Bayesian information criterion (BIC) - Wikipedia (https://en.wikipedia.org/wiki/Bayesian_information_criterion) ARIMA models (https://otexts.com/fpp2/arima.html) SARIMAX: Introduction (https://www.statsmodels.org/stable/examples/notebooks/generated/statespace_sarimax_stata.html) Prophet: Forecasting at scale (https://facebook.github.io/prophet/) Long short-term memory (LSTM) – Wikipedia (https://en.wikipedia.org/wiki/Long_short-term_memory) Time Series Forecasting with ARIMA Models In Python [Part 1] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48?gi=264dc7630363) Time Series Forecasting with ARIMA Models In Python [Part 2] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-2-91a30d10efb0) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057) Kats - Time Series Forecasting By Facebook | by Himanshu Sharma | MLearning.ai | Medium (https://medium.com/mlearning-ai/kats-time-series-forecasting-by-facebook-a2741794d814) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件 (例如 scikit-learn、Kats 等)，用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "4.2. 時間序列資料預測",
    "uri": "/ch4/ch4.2/"
  },
  {
    "content": "\nTable Of Contents 勢力分佈圖 (Voronoi diagram) 最小範圍多邊形/凸包 (Convex hull) 空間群聚 (Clustering) 密度 (Kernel density) 空間內插 (Spatial interpolation) 反距離加權法 (Inverse Distance Weighting) 克力金法 (Kriging) 最近鄰居插值 (Nearest neighbor Interpolation) 空間內插後的資料處理 擷取等值線資料 (contour) 擷取橫切面的資料 (Profile) 參考資源 廣布在生活環境中的微型測站，協助我們掌握細緻的環境變化，並可據以決策跟行動。所以，清楚地掌握測站間的分布和數據特性，也是我們在分析測站數據時的重要基礎。這些測站除了本身的位置可能會形成某種幾何結構或空間群聚。同時，我們也可以依照測站位置與數值的差異，去推估沒有測站的區域的數值，從而獲得一個更為全面的數值分佈狀況，並從中探索感測數值與環境因子間的相關性。在這一個段落中，我們會利用水利署在不同縣市的淹水感測器與地下水位站資料，來進行一些簡單的空間分析。\n勢力分佈圖 (Voronoi diagram) 首先，我們可能需要釐清個別測站的服務/防守範圍，並以此範圍中的測站數據來代表該區的現況。這個時候，我們可以利用沃羅諾伊圖（voronoi diagram）的方法去找尋這個範圍。沃羅諾伊圖的原理是在兩個相鄰測站間建立一條垂直平分線段，並藉由整合這些線段以構成一個多邊形；每個多邊形範圍的中心點就是測站，而該測站的數值則約可代表這個範圍內的數值。在這個範例中，我們嘗試利用嘉義縣、嘉義市的淹水感測器資料，去練習建立沃羅諾伊圖，這樣我們就可以初略知道這些淹水感測器的勢力分佈範圍。\n# 引入所需的模組 import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json # 安裝 Geopython 相關函式模組 !apt install gdal-bin python-gdal python3-gdal # 安裝 Geopandas 需要的 python3-rtree !apt install python3-rtree # 安裝 Geopandas !pip install geopandas # 安裝 Pykrige !pip install pykrige # 安裝 Elevation !pip install elevation # 安裝 Affine 和 Rasterio !pip install affine rasterio # 安裝 Geopandas 需要的 Descartes !pip install descartes # 引入 Geopandas import geopandas as gpd # 安裝和引入 pyCIOT !pip install pyCIOT import pyCIOT.data as CIoT # 使用 wget 指令下載一個名為 \"shp.zip\" 的壓縮檔，檔案來源網址是內政部開放資料平台 !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" # 使用 unzip 指令解壓縮 \"shp.zip\"，並將解壓縮的內容存到 \"shp\" 資料夾 !unzip shp.zip -d shp # 以水利署淹水感測器資料為例，資料集 gpd 為感測器數值與位置資料、basemap為台灣縣市地理邊界 shp file # 以pyCIOT取得資料 # 使用 CIoT 的 Water 模組，從 WRA 和 WRA2 來源收集水位資料 wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") flood_list = wa + wa2 # 合併兩個資料來源 # 讀取「嘉義縣」和「嘉義市」的地圖資料 county = gpd.read_file('/content/shp/COUNTY_MOI_1090820.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\", \"嘉義市\"])] # 創建一個 DataFrame 來儲存水位觀測站的資料 flood_df = pd.DataFrame([], columns=['name', 'Observations', 'lon', 'lat']) # 迴圈歷遍每一個水位觀測站 for i in flood_list: if len(i['data']) \u003e 0: # 如果有資料 df = pd.DataFrame([[i['properties']['stationName'], i['data'][0]['values'][0]['value'], i['location']['longitude'], i['location']['latitude']]], columns=['name', 'Observations', 'lon', 'lat']) else: # 若沒有資料，填入 -999 df = pd.DataFrame([[i['properties']['stationName'], -999, -999, -999]], columns=['name', 'Observations', 'lon', 'lat']) flood_df = pd.concat([flood_df, df]) # 移除重複的觀測站並重新排序 result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station = result_df.sort_values(by=['lon', 'lat']) station = station[station.lon != -999] station.reset_index(inplace=True, drop=True) # 轉換 DataFrame 為 GeoDataFrame gdf_flood = gpd.GeoDataFrame(station, geometry=gpd.points_from_xy(station.lon, station.lat), crs=\"EPSG:4326\") # 設定地圖投影並取得交集資料 basemap = basemap.set_crs(4326, allow_override=True) intersected_data = gpd.overlay(gdf_flood, basemap, how='intersection') # 引入 scipy 的 Voronoi 和 voronoi_plot_2d 函式 from scipy.spatial import Voronoi, voronoi_plot_2d # 創建一個 6x10 英寸的圖形和座標軸 fig, ax = plt.subplots(figsize=(6, 10)) # 提取交集資料的經緯度作為 Voronoi 圖的輸入點 inputp = intersected_data[['lon', 'lat']] # 繪製基本地圖，面顏色為空，邊界顏色為紫色 basemap.plot(ax=ax, facecolor='none', edgecolor='purple') # 建立 Voronoi 圖物件 vor = Voronoi(inputp) # 繪製 Voronoi 圖，不顯示頂點 voronoi_plot_2d(vor, ax=ax, show_vertices=False) # 顯示圖形 plt.show() 此外，我們也可以利用德勞內三角分割（Delaunay triangulation）去描述測站的服務/防守範圍；它的原理是以單一測站為中心，並尋找最鄰近的兩個點，以連接成一個三角形範圍。若我們將三角形的範圍視為一個均質的平面，而這個範圍內的感測數值則可用三個節點測站值的平均來替代。\n整體而言，這兩個演算法都可以協助我們以圖形的方式，理解感測器在空間上的分佈，及其所建立的空間結構。\n# 引入 scipy 的 Delaunay 和 delaunay_plot_2d 函式 from scipy.spatial import Delaunay, delaunay_plot_2d import numpy as np # 引入 numpy 函式模組 # 創建一個 6x10 英寸的圖形和座標軸 fig, ax = plt.subplots(figsize=(6, 10)) # 把輸入點（經緯度）轉換成 numpy 陣列，因為 Delaunay 需要陣列格式的輸入 inputp = np.array(inputp) # 建立 Delaunay 三角劃分物件 tri = Delaunay(inputp) # 繪製基本地圖，面顏色為空，邊界顏色為紫色 basemap.plot(ax=ax, facecolor='none', edgecolor='purple') # 繪製 Delaunay 三角劃分圖 delaunay_plot_2d(tri, ax=ax) # 顯示圖形 plt.show() 最小範圍多邊形/凸包 (Convex hull) 最小範圍多邊形的演算法，是從一群測站中，選出位於最邊緣的若干個測站構成一個能含納所有的點位、且邊長最小的多邊形，這樣我們就可以在一堆測站中，找到一個群聚的範圍，並用這個範圍來發展一些計算。最小範圍多邊形的演算法，主要是依照測站的x座標排序測站位置，而當 X 座標相同則以 Y 座標大小排序，從而找到最外圍的端點並連接成為多邊形 (當然類似的概念還有許多方法)，透過最小範圍多邊形的演算，我們可以評估測站的有效監測範圍。所以，我們也可以利用嘉義縣、嘉義市的淹水感測器分佈，已瞭解這些淹水感測器的覆蓋範圍。\n# 引入 scipy 的 ConvexHull 和 convex_hull_plot_2d 函式 from scipy.spatial import ConvexHull, convex_hull_plot_2d # 創建一個 6x10 英寸的圖形和座標軸 fig, ax = plt.subplots(figsize=(6, 10)) # 使用 ConvexHull 函式來計算凸包 hull = ConvexHull(inputp) # 繪製基本地圖，面顏色為空，邊界顏色為紫色 basemap.plot(ax=ax, facecolor='none', edgecolor='purple') # 繪製凸包圖 convex_hull_plot_2d(hull, ax=ax) # 調整布局以適應所有子圖元素 plt.tight_layout() 空間群聚 (Clustering) 正如前文所說，越鄰近的測站，其周邊環境中的干擾因子可能也越相似，所以我們可以利用 Kmeans 這種分類演算法，將測站進行分群，以進一步探勘其感測數據與環境因子間的關係。Kmeans主要是根據我們預先設定的分群的數量 n，並隨機尋找 n 個點做為中心，去尋找周邊的鄰居，經由量測樣本點與中心點的直線距離，去把樣本點分群並計算各群的平均值，最後，重複前述的程序直到所有樣本點與中心點的距離平均值最短，即可完成分群。\n此外，測站不單只有空間位置，也會有其量測數值，如果我們同時參考其空間位置與量測數值，將量測數值較為相似且地理位置相近的測站聚集起來，這些聚集的測站所遭受的環境干擾因子亦較為接近，而透過測站的空間位置，亦正可以反映出不同環境干擾因子在地理空間上的影像範圍。因此，空間群聚也是地理空間資料分析的重要一環。在這個案例中，我們也嘗試以雲林地區的地下水位站為案例，去描述這些測站的空間群聚狀況。\n# 獲取地下水位站資料 # 初始化計數器和 DataFrame count = 733 num = 0 water_level = pd.DataFrame([]) # 使用 while 迴圈來處理多個 API 請求 while(num\u003c=count): # 拼接 API 網址 url_level = \"https://sta.ci.taiwan.gov.tw/STA_WaterResource_v2/v1.0/Datastreams?$skip=\"+str(num)+\"\u0026$filter=%28%28Thing%2Fproperties%2Fauthority_type+eq+%27%E6%B0%B4%E5%88%A9%E7%BD%B2%27%29+and+substringof%28%27Datastream_Category_type%3D%E5%9C%B0%E4%B8%8B%E6%B0%B4%E4%BD%8D%E7%AB%99%27%2CDatastreams%2Fdescription%29%29\u0026$expand=Thing,Thing%28%24expand%3DLocations%29,Observations%28%24top%3D1%3B%24orderby%3DphenomenonTime+desc%3B%24top%3D1%29\u0026$count=true\" # 建立安全的 HTTPS 連線 ssl._create_default_https_context = ssl._create_unverified_context # 發送 API 請求並接收回應 r_l = urllib.request.urlopen(url_level) string_l = r_l.read().decode('utf-8') # 載入JSON數據 jf_level = json.loads(string_l) # 整理數據 station = pd.DataFrame(jf_level['value']).filter(items=['Thing','observedArea','Observations']) station['lat']=station['observedArea'] for i in range(len(station)): station['Thing'][i] = station['Thing'][i]['properties']['stationName'] # 處理經緯度和觀測數據 if pd.isnull(station['observedArea'][i]): station['lat'][i]=-1 station['observedArea'][i]=-1 else: station['lat'][i]=station['lat'][i]['coordinates'][1] station['observedArea'][i]=station['observedArea'][i]['coordinates'][0] if len(station['Observations'][i])!=0: station['Observations'][i] = station['Observations'][i][0]['result'] else: station['Observations'][i] = -1 # 重命名欄位 station = station.rename(columns={\"Thing\": \"name\", 'observedArea': 'lon'}) # 合併資料 if num ==0 : water_level = station else: water_level = pd.concat([water_level, station]) # 更新計數器 num+=100 # 去除重複站點，並進行排序 result_df = water_level.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) # 過濾掉不合理的經緯度 station = station[station.lon!=-1] # 轉換為地理資料框架 gdf_level = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") # 與雲林縣範圍 intersect # 選取「雲林縣」的底圖 basemap = county.loc[county['COUNTYNAME'].isin([\"雲林縣\"])] # 設定底圖的坐標系統 basemap = basemap.set_crs(4326, allow_override=True) # 將水位觀測站的地理數據與「雲林縣」的底圖進行交集運算 intersected_data = gpd.overlay(gdf_level, basemap, how='intersection') # 引入 KMeans 分群演算法的函式模組 from sklearn.cluster import KMeans # 引入 scipy 的 ConvexHull 函式模組，用於繪製凸包圖 from scipy.spatial import ConvexHull # 引入 folium 函式模組，用於繪製地圖 import folium # 從交集數據中選取需要的欄位：名稱、經度、緯度和觀測值 clusterp = intersected_data[[\"name\", \"lon\", 'lat', 'Observations']] # 1. 使用 KMeans 進行分群 # 1.1 資料前處理，只取經度和緯度作為特徵 X = clusterp.iloc[:, 1:3].values # 用 Elbow Method 來找出最佳的群數 wcss = [] # 存放每個 k 值對應的誤差平方和 for i in range(1, 11): # 試驗 1 到 10 的 k 值 kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42) kmeans.fit(X) wcss.append(kmeans.inertia_) # 將誤差平方和加到列表中 # 畫出 Elbow Method 的圖，觀察轉折點以決定群數 plt.plot(range(1, 11), wcss) plt.title('The Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show() # 1.2 使用 KMeans 演算法，根據選定的群數（這裡是 3）來訓練模型 kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42) # 進行模型訓練，並得到每個資料點的分群結果 y_kmeans = kmeans.fit_predict(X) # 1.3 將分群結果存回原始 DataFrame # 因為 K-Means 的群編號從 0 開始，所以這裡加 1，讓群編號從 1 開始 clusterp['cluster'] = y_kmeans + 1 # 2.將資料畫在地圖上 m = folium.Map(location=[clusterp['lat'].mean(), clusterp['lon'].mean()], tiles='CartoDB positron', zoom_start=7) # 創建底圖 # 為每個分群建立一個特殊的圖層 layer1 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup1\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer1) layer2 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup2\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer2) layer3 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup3\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer3) layer4 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup4\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer4) # 加入圖層 # 自訂 CSS 樣式 m.get_root().html.add_child(folium.Element(my_symbol_css_class)) # 遍歷資料框並根據分群結果放置標記 for index, row in clusterp.iterrows(): # 遍歷每一行資料 # 決定要使用哪一個圖層和 icon if row['cluster'] == 1: color='black' fa_symbol = 'fa-g1' lay = layer1 elif row['cluster'] == 2: color='purple' fa_symbol = 'fa-g2' lay = layer2 elif row['cluster'] == 3: color='orange' fa_symbol = 'fa-g3' lay = layer3 elif row['cluster'] == 4: color='blue' fa_symbol = 'fa-g4' lay = layer4 # 在地圖上創建標記 folium.Marker( location=[row['lat'], row['lon']], title=row['name']+ ' group:{}'.format(str(row[\"cluster\"])), popup=row['name']+ ' group:{},value:{}'.format(str(row[\"cluster\"]),str(row['Observations'])), icon=folium.Icon(color=color, icon=fa_symbol, prefix='fa')).add_to(lay) # 將標記添加到對應的圖層 # 準備將資料畫上地圖 layer_list = [layer1,layer2,layer3,layer4] color_list = ['black','purple','orange','blue'] # 遍歷每一個獨立的群組 for g in clusterp['cluster'].unique(): # 利用 ConvexHull 找到每個群的邊界 latlon_cut =clusterp[clusterp['cluster']==g].iloc[:, 1:3] # 選取該群組的經緯度 hull = ConvexHull(latlon_cut.values) # 使用 ConvexHull 計算邊界 Lat = latlon_cut.values[hull.vertices,0] Long = latlon_cut.values[hull.vertices,1] # 獲取邊界的經緯度 # 將經緯度資訊轉成 list，以在 folium 中使用 cluster = pd.DataFrame({'lat':Lat,'lon':Long }) area = list(zip(cluster['lat'],cluster['lon'])) # 畫出該群組的邊界 list_index = g-1 lay_cluster = layer_list[list_index] folium.Polygon(locations=area, color=color_list[list_index], weight=2, fill=True, fill_opacity=0.1, opacity=0.8).add_to(lay_cluster) # 加入圖層控制器 folium.LayerControl(collapsed=False,position= 'bottomright').add_to(m) # 輸出地圖並存為 HTML print(m) # 顯示地圖 m.save('River_clustering.html') # 將地圖儲存為 HTML 檔案 密度 (Kernel density) 密度是我們常用以描述事件聚集強度的觀念，而傳統一般的密度都是以：個數/面積的公式去計算，但是，這樣的計算方法卻容易受到面積的影響，同樣的測站數/案例數在面積不同的鄉鎮，就會出現不同的密度，反而讓我們無法正確地探勘事件的聚集強度。\n因此，為了避免面積造成計算的差異，所以我們可以用核密度 (kernel density) 的方式去描述事件聚集的強度；核密度的概念就是：以樣本點為中心，並利用固定半徑的移動視窗 (moving windows) 去框選其他樣本點，最後以所有樣本點的數值加總來取代樣本點的舊有數值。這樣的方式可以標準化密度公式中的「面積」，並取得一個全面性的密度分佈圖，以協助我們瞭解整體事件的分佈強度 (圖1)。\n在這個案例中，我們也來看看是否能用核密度的方法去描述地下水位站的分佈強弱。\n# 篩選出 \"嘉義縣\" 和 \"嘉義市\" 的地圖資料 basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\", \"嘉義市\"])] # 將坐標系統設為 WGS 84 (EPSG:4326) basemap = basemap.set_crs(4326, allow_override=True) # 使用 'intersection' 方法來找出 gdf_level 和 basemap 的交集 gdf = gpd.overlay(gdf_level, basemap, how='intersection') # 接下來，我們會選擇多邊形的幾何字段， # 以過濾出那些未與基礎地圖重疊的點。 # 利用plotly套件 畫出河川水位核密度地圖，gdf為河川水位資料點位與數值 # 載入 plotly.express 套件，用於繪製核密度地圖 import plotly.express as px # 使用 density_mapbox 函式來繪製核密度地圖 # 參數設定： # - lat, lon, z 來自 gdf DataFrame，分別代表緯度、經度和觀測值 # - radius 是用來計算密度的半徑，設為 25 # - center 指定地圖中心點的緯度和經度 # - zoom 設為 8，是地圖的初始縮放等級 # - mapbox_style 是地圖的風格 fig = px.density_mapbox(gdf, lat='lat', lon='lon', z='Observations', radius=25, center=dict(lat=23.5, lon=120.5), zoom=8, mapbox_style=\"stamen-terrain\") # 顯示地圖 fig.show() 空間內插 (Spatial interpolation) 微型感測站的設置就像是我們在空間上進行數值採樣，透過這些採樣的結果，我們可以利用一些統計方法還原母體的全貌。因為我們無法在每一吋土地上佈滿測站，所以一定會面臨有些地區有資料，有些地區卻沒有資料的狀況，而空間內插 (Spatial interpolation) 就是以統計的方法，協助我們推估沒有資料的區域，進而了解母體全貌的方法。\n要利用空間內插的方法，首先，我們必須要先瞭解：確定性模型（Deterministic model）與機率性模型 (Stochastic model) 這兩個概念。所謂確定性模型就是我們再掌握某空間現象的分佈規則下，舊可以利用某個相關性參數，去推估未知區域的數值，例如台灣的門牌號碼是將單數與雙數非別排列，所以假若某間房子的前一個門牌為6號，而後一個門牌為10號，那我們可以推估中間這個房子的門牌為8號。而機率性則是假設真實環境非常複雜，然而我們只能透過機率以及變異數的變化去建立適當的推估模型，並接受其差值以及不確定性 (uncertainty)。\n所以，接下來我們就利用雲林的地下水位站來練習幾種常見的空間內插方法。\n反距離加權法 (Inverse Distance Weighting) 在反距離加權法的模式中，我們會利用已知樣本點間的數值差與間距建立推估模型。一般而言，若兩點的差值為10單位、而間距為100公尺，則理論上每10公尺的差值應該為1，但是考量到差值的分佈不應該是線性關係，所以反距離加權法利用地理學第一定律：越鄰近的事物越相近，去以距離作為評估兩點之間數值差異的依據，其策略就是把差值X距離次方的倒數，以獲得該位置的推估值。所以，距離越大權數越小，反之距離越近，權數愈大。\n# 引入 numpy 套件用於數值運算 import numpy as np # 引入 matplotlib.pyplot 用於數據繪製 import matplotlib.pyplot as plt # 引入 scipy.interpolate 的 Rbf 函式用於進行徑向基函式插值 from scipy.interpolate import Rbf # 定義一個函式計算兩組點之間的距離矩陣 def distance_matrix(x0, y0, x1, y1): # 垂直堆疊 x 和 y 坐標，並轉置 obs = np.vstack((x0, y0)).T interp = np.vstack((x1, y1)).T # 建立距離矩陣，根據 x 和 y 的差值 d0 = np.subtract.outer(obs[:,0], interp[:,0]) d1 = np.subtract.outer(obs[:,1], interp[:,1]) # 輸出矩陣的資料型別（用於除錯） print(d0.dtype, d1.dtype) # 計算歐式距離並返回 return np.hypot(d0, d1) # 定義一個簡單的 IDW（Inverse Distance Weighting）函式 def simple_idw(x, y, z, xi, yi, pows): # 計算距離矩陣 dist = distance_matrix(x, y, xi, yi) # 計算 IDW 權重，即 1/距離 weights = 1.0 / dist # 正規化權重，使其總和為 1 weights /= weights.sum(axis=0) # 計算加權的 z 值 zi = np.dot(weights.T, z) return zi # 初始化繪圖環境，設定圖片大小與長寬比 fig, ax = plt.subplots(figsize=(6, 4)) ax.set_aspect('equal') # 設定權重參數與網格數量 pows = 2 nx, ny = 100, 100 # 設定網格範圍 xmin, xmax = 119.8, 121.2 ymin, ymax = 23, 24 # 提取 gdf 中的經緯度和觀測數據 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] x = interpolatep['lon'].astype(\"float64\") y = interpolatep['lat'].astype(\"float64\") z = interpolatep['Observations'].astype(\"float64\") # 產生插值點的網格 xi = np.linspace(xmin, xmax, nx) yi = np.linspace(ymin, ymax, ny) xi, yi = np.meshgrid(xi, yi) xi, yi = xi.flatten(), yi.flatten() # 使用 IDW 插值算法計算插值結果 grid = simple_idw(x, y, z, xi, yi, pows) grid = grid.reshape((ny, nx)) # 繪製 IDW 插值結果 plt.imshow(grid, extent=(xmin, xmax, ymin, ymax)) # 繪製底圖與數據點 basemap.plot(ax=ax, facecolor='none', edgecolor='lightgray') ax.scatter(x, y, marker=\".\", color='orange', s=z, label=\"input point\") # 顯示顏色條與座標軸範圍 plt.colorbar() plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) # 設定標題並顯示圖片 plt.title('IDW') plt.show() 克力金法 (Kriging) 克利金法的原理則是利用已知點的位置與數值，去建立一個半變異圖 (semi-variogram)，並依據這個圖進行樣本點的分組，以得到數個區域性變量 (regionalized variable)，並以其為基礎進行數值推估。克利金法與前述反距離加權法相似，都是利用已知點的數值與距離去推估鄰近地區的未知點數值，比較大的差異是克力金法會將樣本的依照距離作分組，從而依照距離調整其不同的推估公式。\n# 引入 NumPy 套件 import numpy as np # 設定網格解析度，以公尺為單位 resolution = 0.1 # 根據解析度建立 x 和 y 的網格範圍 # x 軸範圍從 119.8 到 121.2 # y 軸範圍從 23 到 24 gridx = np.arange(119.8, 121.2, resolution) gridy = np.arange(23, 24, resolution) # 定義函式 raster to polygon import itertools from shapely.geometry import Polygon # 定義 raster to polygon 函式 def pixel2poly(x, y, z, resolution): \"\"\" x: x座標的網格點 y: y座標的網格點 z: 每個 (x, y) 座標點對應的數值矩陣 resolution: 每個網格單元的空間解析度 \"\"\" polygons = [] # 存放多邊形資訊 values = [] # 存放 z 值資訊 half_res = resolution / 2 # 網格單元一半的解析度 # 利用 itertools.product 遍歷每個網格點 for i, j in itertools.product(range(len(x)), range(len(y))): # 計算多邊形邊界 minx, maxx = x[i] - half_res, x[i] + half_res miny, maxy = y[j] - half_res, y[j] + half_res # 創建多邊形並存入列表 polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)])) # 根據 z 是否為單一數值或矩陣，來存入對應的 z 值 if isinstance(z, (int, float)): values.append(z) else: values.append(z[j, i]) return polygons, values # 引入 pykrige 套件中的 OrdinaryKriging 類別進行克里金內插 from pykrige.ok import OrdinaryKriging # 初始化 OrdinaryKriging 類別，設定變異函式模型為 \"spherical\"，並啟用偽逆矩陣計算 krig = OrdinaryKriging(x=gdf[\"lon\"], y=gdf[\"lat\"], z=gdf['Observations'], variogram_model=\"spherical\", pseudo_inv=True) # 執行克里金內插，對 gridx 和 gridy 進行計算，返回數值 z 和半方差 ss z, ss = krig.execute(\"grid\", gridx, gridy) # 使用 matplotlib 繪製內插後的數值 z plt.imshow(z); # 利用plotly 將網格與地圖疊合呈現 # 引入 plotly.express 進行視覺化 import plotly.express as px # 使用先前定義的 pixel2poly 函式將網格轉換為多邊形和相應的數值 polygons, values = pixel2poly(gridx, gridy, z, resolution) # 將多邊形和數值組合成 GeoDataFrame，並設定座標系統 water_model = (gpd.GeoDataFrame({\"water_modelled\": values}, geometry=polygons, crs=\"EPSG:4326\") .to_crs(\"EPSG:4326\") ) # 利用 plotly 建立地圖，並將多邊形與地圖疊合 fig = px.choropleth_mapbox(water_model, geojson=water_model.geometry, locations=water_model.index, color=\"water_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5, center={\"lat\": 24, \"lon\": 121}, zoom=6, mapbox_style=\"carto-positron\") # 更新 layout 和 trace 設定 fig.update_layout(margin=dict(l=0, r=0, t=30, b=10)) fig.update_traces(marker_line_width=0) 最近鄰居插值 (Nearest neighbor Interpolation) 最近鄰居法的方法其實很簡單，若我們想得知空間上某一個位置的數值，只需要找到最鄰近且有數值的測站，就可以當作是這個位置的數值。這個方法基本上也是依循越鄰近越相似的原理去設計，且常被應用在影像處理及放大的案例上。\n# 引入所需的套件 from scipy.interpolate import NearestNDInterpolator import matplotlib.pyplot as plt # 初始化 matplotlib 的 figure 和 axis fig, ax = plt.subplots(figsize=(6, 4)) # 從 GeoDataFrame 中提取需要的經緯度和觀察數據 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") # 建立網格範圍 X = np.linspace(min(xd), max(xd)) Y = np.linspace(min(yd), max(yd)) X, Y = np.meshgrid(X, Y) # 轉為 2D 網格 # 使用 NearestNDInterpolator 方法進行內插 interp = NearestNDInterpolator(list(zip(xd, yd)), zd) Z = interp(X, Y) # 繪製內插後的數據 im = ax.pcolormesh(X, Y, Z, shading='auto') # 繪製基本地圖 basemap.plot(ax=ax, facecolor='none', edgecolor='gray') # 繪製原始的數據點 sns.scatterplot(x='lon', y='lat', data=interpolatep, label=\"input point\") # 加入圖例和顏色條 plt.legend() plt.colorbar(im) # 設定 x 和 y 軸的範圍 plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) # 顯示圖形 plt.show() 空間內插後的資料處理 擷取等值線資料 (contour) 一般來說，我們將測站依照其位置與數值進行空間內插後，就會得到一個全面性的網格資料，然而，我們可以如何解析這些網格資料呢？首先，最容易的方式就是依照網格的數值與位置，去將數值相近的點連成一條線，其概念類似於在起伏不定的地形上，劃設等高線，而我們這樣的話法可以將其視為等值線。\n# 引入 GDAL 模組，用於地理空間數據操作 from osgeo import gdal # 引入 NumPy 模組，用於數組運算 import numpy as np # 引入 Matplotlib 模組，用於繪圖 import matplotlib # 引入 pyplot，用於 2D 圖形繪製 import matplotlib.pyplot as plt # 引入 elevation 模組，用於處理高程數據 import elevation # 初始化 Matplotlib 的 figure 和 axis fig, ax = plt.subplots(figsize=(6, 10)) # 定義網格範圍 X = np.linspace(xmin, xmax) Y = np.linspace(ymin, ymax) # 使用 OrdinaryKriging 進行克里金內插 krig = OrdinaryKriging(x=interpolatep['lon'], y=interpolatep['lat'], z=interpolatep['Observations'], variogram_model=\"spherical\") z, ss = krig.execute(\"grid\", X, Y) # 利用 contourf 繪製等高線圖 im = ax.contourf(z, cmap=\"viridis\", levels=list(range(-30, 30, 10)), extent=(xmin, xmax, ymin, ymax)) # 繪製基本地圖 basemap.plot(ax=ax, facecolor='none', edgecolor='black') # 加入標題 plt.title(\"Elevation Contours Taiwan\") # 顯示圖形 plt.show() 擷取橫切面的資料 (Profile) 劃設等值線可以讓我們獲知數值的分佈梯度與範圍，而另一個協助我們瞭解數值分佈的方式則是剖面線，其原理就是在兩點之間劃設一條直線，並依照直線的位置去擷取相對應的推估數值。這樣的方法可以協助我們知道兩點之間的數值變化起伏，在某些空氣品質的研究中，科學家就會利用剖面線的方式評估道路兩側的PM2.5變化。\n# 引入 Affine 和 rasterio 模組，用於處理地理轉換和寫入 GeoTIFF 檔 from affine import Affine import rasterio # 定義將內核密度估計輸出為 raster 格式的函式 def export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename): '''Export and save a kernel density raster.''' # 計算 X 和 Y 的解析度 xres = (max_x - min_x) / len(XX) yres = (max_y - min_y) / len(YY) # 創建轉換矩陣，用於定義地理空間的定位 transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres) # 使用 rasterio 模組開啟一個新的 GeoTIFF 檔案來儲存輸出 with rasterio.open( filename, mode=\"w\", driver=\"GTiff\", height=Z.shape[0], width=Z.shape[1], count=1, dtype=Z.dtype, crs=proj, transform=transform, ) as new_dataset: # 將計算好的內核密度估計寫入新的 GeoTIFF 檔案 new_dataset.write(Z, 1) # 引入 Kriging 插值需要的 pykrige 模組 from pykrige.ok import OrdinaryKriging # 引入 affine 模組，用於處理坐標轉換 from affine import Affine # 引入 rasterio 模組，用於讀寫 raster 格式的地理資料 import rasterio # 引入 math 模組，用於數學運算，如平方根等 import math # 引入 pandas 模組，用於數據分析 import pandas as pd # 引入 geopandas 模組，用於處理地理空間資料 import geopandas as gpd # 引入 numpy 模組，用於數值計算 import numpy as np # 設定起點和終點的座標，以及網格解析度 start_cor = [119.9, 23.2] end_cor = [120.1, 23.9] npoints = 100 X = np.linspace(xmin, xmax, npoints) Y = np.linspace(ymin, ymax, npoints) # 從 GeoDataFrame 中取出觀測資料 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'].astype(\"float64\") yd = interpolatep['lat'].astype(\"float64\") zd = interpolatep['Observations'].astype(\"float64\") # 執行 Ordinary Kriging 插值 krig = OrdinaryKriging(x=xd, y=yd, z=zd, variogram_model=\"spherical\") zr, ss = krig.execute(\"grid\", X, Y) # 使用之前定義的函式輸出 Kriging 插值結果為 raster 格式 export_kde_raster(Z=zr, XX=X, YY=Y, min_x=xmin, max_x=xmax, min_y=ymin, max_y=ymax, proj=4326, filename=\"kriging_result.tif\") # 開啟輸出的 Kriging raster kriging = rasterio.open(\"kriging_result.tif\", mode='r') # 計算起點和終點之間的距離 dist = math.sqrt((end_cor[0] - start_cor[0]) ** 2 + (end_cor[1] - start_cor[1]) ** 2) * 111 # 建立線性內插的網格點 npoints = 500 lat = np.linspace(start_cor[1], end_cor[1], npoints) lon = np.linspace(start_cor[0], end_cor[0], npoints) distarray = np.linspace(0, dist, npoints) # 創建 DataFrame 來儲存資料 df = pd.DataFrame({'Latitude': lat, 'Longtitude': lon, 'h_distance': distarray}) df['Observations'] = 0 gdf_pcs = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.Longtitude, df.Latitude)) gdf_pcs.crs = {'init': 'epsg:4326'} # 從 Kriging raster 中讀取對應點的數值 for index, row in gdf_pcs.iterrows(): rows, cols = kriging.index(row['geometry'].x, row['geometry'].y) kri_data = kriging.read(1) df['Observations'].loc[index] = kri_data[rows, cols] # 繪製水平距離和觀測數值的關係圖 profile = df[['h_distance', 'Observations']] profile.plot(x='h_distance', y='Observations') # 關閉 raster 檔 kriging.close() 參考資源 Geopandas初探, Chimin. https://ithelp.ithome.com.tw/articles/10202336 scipy.spatial 空間處理模組說明 (https://docs.scipy.org/doc/scipy/reference/spatial.html) scipy.interpolate 空間內插模組說明 (https://docs.scipy.org/doc/scipy/reference/interpolate.html) pykrige 克力金內插模組說明 (https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/api.html#krigging-algorithms) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "5.2. 地理空間分析",
    "uri": "/ch5/ch5.2/"
  },
  {
    "content": "\nTable Of Contents 異常檢測框架 異常事件種類 異常事件可能原因 實際案例演練 套件安裝與引用 讀取資料與環境設定 尋找鄰近的感測器 每五分鐘進行感測資料的時間切片 以時間切片為單位計算鄰居感測器的平均感測值 判斷異常事件的門檻值 非正常使用機器偵測模組 (MD) 實作 即時污染偵測模組 (RED) 實作 感測器可靠度評估模組 (DR) 實作 參考資料 異常檢測框架 目前已有多個大規模微型空品監測系統成功部署於不同的國家與城市之中，然而這些微型感測器的主要挑戰之一為如何確保數據品質，並且能即時偵測出可能的異常現象。在台灣的中央研究院資訊科學研究所網路實驗室研究團隊，於2018年提出了一種可用於實際環境中的異常檢測框架，稱之為 Anomaly Detection Framework (ADF)。\n此異常檢測框架由四個模組所組成：\n時間片斷異常偵測 (Time-Sliced Anomaly Detection, TSAD)：可即時偵測感測器於空間或時間上的異常數據，並將結果輸出給其他模組進行進一步分析。 即時污染偵測模組 (Real-time Emission Detection, RED)：可透過 TSAD 的偵測結果，即時檢測潛在的區域性污染事件。 感測器可靠度評估模組 (Device Ranking, DR)：可累積 TSAD 的偵測結果，並據以評估每個微型感測器設備的可靠度 非正常使用機器偵測模組 (Malfunction Detection, MD)：可累積 TSAD 的偵測結果，透過數據分析判別可能為非正常使用的微型感測器，例如安裝在室內的機器、安置在持續性污染源旁邊的機器等。 異常事件種類 在 ADF 框架中，TSAD 模組在微型感測器每次收到新的感測資料後，便會進行時間類或空間類的異常事件判斷，我們以微型空品感測器為例，進行說明：\n時間類異常事件：我們假設空氣的擴散是均勻緩慢的，因此同一台微型空品感測器在短時間內的數值變化應極為平緩，如果有某台微型空品感測器的感測數值在短時間內出現劇烈的變化，代表在時間維度上可能出現異常事件。 空間類異常事件：我們可以假設戶外的空氣在地理空間上是會均勻擴散的，因此微型空品感測器的感測數值，理應與周圍鄰近的感測器相似，如果有某台微型空品感測器的感測數值，與同時間鄰近區域的微型空品感測器的感測數值出現極大的差異，代表該感測器所處的空間可能出現異常事件。 異常事件可能原因 以上所述的異常事件有許多可能的原因，常見的原因有：\n安裝環境異常：感測器被安裝於特定環境，因此無法呈現整體環境現象，例如安裝於廟宇旁、燒烤店內或其他室內不通風的地點。 機器故障或安裝錯誤：例如感測器安裝時將取風口的方向弄錯，或者感測器的風扇積垢導致運轉不順暢。 出現臨時污染源：例如感測器旁邊剛好有人在抽菸、發生火災或排放污染物質。 實際案例演練 在這篇文章中，我們將以民生公共物聯網中的空品資料為例，使用部分佈建於高雄市的校園微型空品感測器來進行分析，並且介紹如何使用 ADF 檢測框架來找出其中可能為室內機器或位於污染源附近的機器，藉此過濾出可信度相對較低的機器們，進而提高整體空品感測結果的可信度。\n套件安裝與引用 在這個案例中，我們將會使用到 pandas, numpy, plotly 和 geopy 等套件，這些套件在我們使用的開發平台 Google Colab 上已有預先提供，因此我們不需要另行安裝，可以直接用下列的方法引用，以備之後資料處理與分析使用。\n# 引入 pandas 與 numpy 模組，用於資料分析和處理。 import pandas as pd import numpy as np # 引入 plotly.express 模組，用於資料視覺化。 import plotly.express as px # 引入 geopy.distance 中的 geodesic 模組，用於計算兩地點之間的距離。 from geopy.distance import geodesic 讀取資料與環境設定 在這個案例中，我們將使用部分民生公共物聯網佈建於高雄的校園微型空品感測器來進行分析，我們所設定的時間和空間範圍如下：\n地理區域：緯度: 22.631231 - 22.584989, 經度: 120.263422 - 120.346764 時間區間：2022.10.15 - 2022.10.28 註：校園微型空品感測器的原始資料請至 民生公共物聯網-資料服務平台下載，為了方便讀者可以重現這個範例的內容和結果，我們先把所有使用到的資料整理成 allLoc.csv 檔案，做為接下來資料分析的依據。\n我們首先載入資料檔案，並預覽資料的內容：\n# 使用 pandas 的 read_csv 方法從指定的 URL 讀取 CSV 檔案，並將其存入 DF 這個 DataFrame 中。 DF = pd.read_csv(\"https://LearnCIOT.github.io/data/allLoc.csv\") # 呼叫 DataFrame 的 head 方法，預設顯示前五筆資料以檢查其內容。 DF.head() 接著我們擷取資料檔中每個感測器的 GPS 地理位置座標，由於這些感測器的 GPS 座標都不會改變，我們使用比較特別的方式，將資料檔中每個感測器的經度和緯度資料各自取平均值，做為該感測器的地理位置座標。\n# 從 DF 中選取 \"device_id\", \"lon\", \"lat\" 這三列，然後按照 \"device_id\" 進行分組，計算平均值，再重設索引。 dfId = DF[[\"device_id\",\"lon\",\"lat\"]].groupby(\"device_id\").mean().reset_index() # 輸出 dfId 的前五筆資料以檢查其內容。 print(dfId.head()) device_id lon lat 0 74DA38F207DE 120.340 22.603 1 74DA38F20A10 120.311 22.631 2 74DA38F20B20 120.304 22.626 3 74DA38F20B80 120.350 22.599 4 74DA38F20BB6 120.324 22.600 為了大致了解資料檔內感測器的地理位置分布，我們將感測器的位置繪製在地圖上。\n# 使用 plotly.express 的 scatter_mapbox 函數，根據 dfId 的經緯度數據繪製散點地圖。 # 設定連續色階為 IceFire，初始放大級別為 9，並使用 \"carto-positron\" 的地圖風格。 fig_map = px.scatter_mapbox(dfId, lat=\"lat\", lon=\"lon\", color_continuous_scale=px.colors.cyclical.IceFire, zoom=9, mapbox_style=\"carto-positron\") # 顯示地圖視覺化結果。 fig_map.show() 尋找鄰近的感測器 由於我們所使用的校園微型空氣品質感測器都是固定安裝在校園內，其 GPS 地理位置座標並不會改變，為了節省之後資料分析的運算時間，我們先統一將每個感測器的「鄰居」清單計算出來。在我們的案例中，我們定義如果兩個微型感測器的相互距離小於等於 3 公里，那麼這兩個感測器變互為鄰居關係。\n我們首先撰寫一個小函式 countDis，可以針對輸入的兩個 GPS 地理座標位置，計算兩者之間的實體公里距離。\n# 定義一個 countDis 函式，用於計算兩個裝置間的地理距離。 # 輸入為兩個裝置的資訊（含經緯度），輸出為兩裝置間的距離（公里）。 def countDis(deviceA, deviceB): return geodesic((deviceA[\"lat\"], deviceB['lon']), (deviceB[\"lat\"], deviceB['lon'])).km 接著我們將原有資料的感測器列表從 DataFrame 資料型態，轉化成 Dictionary 資料型態，並且計算所有任意兩個感測器間的距離，只要兩者間的距離小於 3km，便將彼此存入對方的鄰居感測器列表 dicNeighbor 中。\n# 設定兩個鄰近裝置間的最大距離為 3 公里。 DISTENCE = 3 # 將 DataFrame 轉換成字典格式。 # 格式為：{iD: {'lon': 經度值, 'lat': 緯度值}, ...}。 dictId = dfId.set_index(\"device_id\").to_dict(\"index\") # 取得感測器 device_id 的列表。 listId = dfId[\"device_id\"].to_list() # 初始化 dicNeighbor 字典。 # 格式為：{iD: []}。 dicNeighbor = {iD: [] for iD in listId} # 使用 countDis 函數計算每兩個感測器之間的距離。 # 如果兩感測器間的距離小於 3 公里，則認定它們為彼此的鄰居。 # 時間複雜度：N! for x in range(len(listId)): for y in range(x+1, len(listId)): if ( countDis( dictId[listId[x]], dictId[listId[y]]) \u003c DISTENCE ): dicNeighbor[listId[x]].append( listId[y] ) dicNeighbor[listId[y]].append( listId[x] ) 每五分鐘進行感測資料的時間切片 由於原始資料中每個感測器的感測資料在時間上並未同步，在 ADF 框架中提出將感測資料以每單位時間為間隔，獲取整體感測結果的時間切片 (time slice)。我們首先將原始資料中的 date 和 time 兩個欄位進行整併，並以 Python 語言中的 datetime 時間資料型態，儲存形成 datetime 新欄位，接著再將原有的 date、time、RH、temperature、lat、lon 等不需要的欄位刪除。\n# 將 'date' 和 'time' 這兩列結合，創建一個新的列 'datetime'。 DF[\"datetime\"] = DF[\"date\"] + \" \" + DF[\"time\"] # 移除一些不必要的列。 DF.drop(columns=[\"date\",\"time\", \"RH\",\"temperature\",\"lat\",\"lon\"], inplace=True) # 將新的 'datetime' 列轉換為 datetime 資料型態。 DF['datetime'] = pd.to_datetime(DF.datetime) 由於我們所處理的校園微型空氣品質感測器的資料頻率約為 5 分鐘，因此我們將時間切片的單位時間 FREQ 也設為 5 分鐘，並計算每五分鐘內，每個感測器所回傳的感測值的平均值。為了確保資料正確性，我們也多做了一道檢查，將 PM2.5 感測值為負數的資料予以刪除。\n# 設定分組的頻率為 '5min'。 FREQ = '5min' # 根據 'device_id' 和頻率為 '5min' 的 'datetime' 進行分組，然後計算每組的平均值。 dfMean = DF.groupby(['device_id', pd.Grouper(key = 'datetime', freq = FREQ)]).agg('mean') # 移除無效紀錄，即 PM2.5 小於 0 的資料。 dfMean = dfMean[ dfMean['PM2.5'] \u003e= 0 ] # 輸出 dfMean 的前五筆資料以檢查。 print(dfMean.head()) PM2.5 device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 2022-10-15 00:05:00 37.0 2022-10-15 00:10:00 37.0 2022-10-15 00:20:00 36.0 2022-10-15 00:25:00 36.0 以時間切片為單位計算鄰居感測器的平均感測值 為了計算在特定時間切片上，特定感測器的鄰居感測器的平均感測值，我們撰寫 cal_mean 函式，可以根據輸入的特定感測器代碼 iD 與時間戳記 dt，回傳該感測器的鄰居感測器數量與平均感測值。\n# 定義一個函數 cal_mean，用於計算某個裝置在某個時間點的鄰居裝置的 PM2.5 平均值。 def cal_mean(iD, dt): # 從 dfMean 中選取符合指定時間 dt 和指定裝置 iD 的鄰居裝置的 PM2.5 資料。 neighborPM25 = dfMean[ (dfMean.index.get_level_values('datetime') == dt) \u0026 (dfMean.index.get_level_values(0).isin(dicNeighbor[iD])) ][\"PM2.5\"] # 計算鄰居裝置的 PM2.5 平均值。 avg = neighborPM25.mean() # 計算有多少鄰居裝置的 PM2.5 資料被考慮。 neighbor_num = neighborPM25.count() # 回傳平均值和鄰居裝置的數量。 return avg, neighbor_num 接著我們針對 dfMean 中每台感測器的每個時間戳記，計算其鄰居感測器的數量與平均感測值，並分別存入 avg 與 neighbor_num 兩個新欄位中。比較特別的是，這邊我們使用 zip 和 apply 的語法，可以將 DataFrame 的數值帶入函式當中進行運算，其中：\n我們使用 zip 的語法，用於打包 apply_func 所回傳的兩個參數值。 我們使用 apply 的語法，以配合 DataFrame 的規則，接收 apply_func 的回傳值。 # 定義一個函數 apply_func，將 cal_mean 函式適用於 dfMean 的每一列。 # 使用 x.name 取得當前列的索引值 (裝置ID 和時間)，並傳遞給 cal_mean 函式。 def apply_func(x): return cal_mean(x.name[0], x.name[1]) # 使用 apply 函式和 apply_func，計算 dfMean 中每一列的 PM2.5 平均值和鄰居裝置的數量。 # 結果會被分解並分別存儲在 'avg' 和 'neighbor_num' 這兩列。 dfMean['avg'], dfMean['neighbor_num'] = zip(*dfMean.apply(apply_func, axis=1)) # 輸出 dfMean 的前五筆資料以檢查。 print(dfMean.head()) PM2.5 avg neighbor_num device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 2022-10-15 00:05:00 37.0 19.888889 9 2022-10-15 00:10:00 37.0 16.500000 12 2022-10-15 00:20:00 36.0 16.750000 8 2022-10-15 00:25:00 36.0 17.000000 11 判斷異常事件的門檻值 我們透過觀察發現，所謂的「異常事件」指的是感測器的感測值，與我們心中認定的合理值有過大的差距，這個合理值有可能是鄰近的感測器感測值（空間類異常），也有可能是同一個感測器的前一筆感測值（時間類異常），或者是根據其他資訊來源的合理推估值。同時，我們也發現所謂的「過大的差距」，其實是一個很模糊的說法，其具體數值隨感測值的大小，亦有極大的不同。\n因此，我們首先根據現有 dfMean[’avg'] 的分布狀況，切分為 9 個區間，並且針對每一個區間的 PM2.5 感測值，計算其標準差，並據以做為判斷感測值是否異常的門檻值標準，如下表所示。舉例來說，當原始數值為 10 (ug/m3) 時，倘若周圍感測器平均高於 10+6.6 ，或低於 10-6.6 ，我們便會認定該感測器的該筆感測值為異常事件。\n原始數值 (ug/m3) 門檻值標準 0-11 6.6 12-23 6.6 24-35 9.35 36-41 13.5 42-47 17.0 48-58 23.0 59-64 27.5 65-70 33.5 71+ 91.5 根據這個對照表，我們撰寫下列的函式 THRESHOLD，根據輸入的感測數值，回傳對應的門檻值，並將此門檻值存入 dfMEAN 的新欄位 PM_thr 中。\n# 定義一個函數 THRESHOLD，根據 PM2.5 的值傳回相應的閾值。 def THRESHOLD(value): if value\u003c12: return 6.6 elif value\u003c24: return 6.6 elif value\u003c36: return 9.35 elif value\u003c42: return 13.5 elif value\u003c48: return 17.0 elif value\u003c54: return 23.0 elif value\u003c59: return 27.5 elif value\u003c65: return 33.5 elif value\u003c71: return 40.5 else: return 91.5 # 使用 apply 方法和 THRESHOLD 函式，計算 dfMean 中每一列 PM2.5 的閾值。 # 結果存入新的 'PM_thr' 列中。 dfMean['PM_thr'] = dfMean['PM2.5'].apply(THRESHOLD) 由於原始資料的最後一筆紀錄的時間是 2022-10-28 23:45，因此我們將資料判斷的時間設為 2022-10-29 日，並將原始資料中每一筆紀錄與資料判斷時間的差距，存入 dfMEAN 的新欄位 days 中。接著我們先預覽一下目前 dfMEAN 資料表的狀況。\n# 設定目標日期為 \"2022-10-29\"。 TARGET_DATE = \"2022-10-29\" # 使用 assign 方法，計算 dfMean 中每一列時間與目標日期的天數差距。 # 結果存入在新的 'days' 列中。 dfMean = dfMean.assign(days = lambda x: ( (pd.to_datetime(TARGET_DATE + \" 23:59:59\") - x.index.get_level_values('datetime')).days ) ) # 輸出 dfMean 的前五筆資料以檢查。 print(dfMean.head()) PM2.5 avg neighbor_num PM_thr days device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 13.5 14 2022-10-15 00:05:00 37.0 19.888889 9 13.5 14 2022-10-15 00:10:00 37.0 16.500000 12 13.5 14 2022-10-15 00:20:00 36.0 16.750000 8 13.5 14 2022-10-15 00:25:00 36.0 17.000000 11 13.5 14 非正常使用機器偵測模組 (MD) 實作 在接下來的範例中，我們實作 ADF 中的非正常使用機器偵測模組 (Malfunction Detection, MD)，其核心觀念為，倘若我們將某一個微型感測器的 PM2.5 數值與其周圍 3 公里內的其他感測器進行比較，如果其感測數值低於鄰近感測器的平均值 (avg) 減掉可接受的門檻值 (PM_thr)，則將該機器視為裝設在室內的機器（標記為indoor）；如果其感測數值高於鄰近感測器的平均值 (avg) 加上可接受的門檻值 (PM_thr)，則將該機器視為裝設在污染源旁的機器（標記為emission）。為了避免因為參考的鄰近感測器數量不夠導致誤判，我們只採計鄰近區域存有多於 2 個其他感測器的案例。\n# 設定最小鄰居數量為 2。 MINIMUM_NEIGHBORS = 2 # 計算 \"indoor\" 標籤。 # 如果當前裝置的 PM2.5 平均值與該裝置的 PM2.5 值的差大於閾值，且該裝置的鄰居數量大於或等於最小鄰居數量，則為 True，否則為 False。 dfMean[\"indoor\"] = ((dfMean['avg'] - dfMean['PM2.5']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) # 計算 \"emission\" 標籤。 # 如果該裝置的 PM2.5 值與 PM2.5 平均值的差大於閾值，且該裝置的鄰居數量大於或等於最小鄰居數量，則為 True，否則為 False。 dfMean[\"emission\"] = ((dfMean['PM2.5'] - dfMean['avg']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) # 顯示 dfMean 的內容。 dfMean 由於每日的空氣品質狀況不同，對於 indoor 和 emission 的判斷結果也會產生不同程度的影響，為了獲得更有說服力的判斷結果，我們考慮過去 1 天、7 天、 14 天等三個時間長度，分別計算在不同時間長度下，同一個感測器被判斷為 indoor 或 emission 的比例。同時，為了避免外在環境導致的誤判影響最後的判斷結果，我們強制修改計算出來的結果，將比例小於 1/3 的數值，強制改為 0。\n# 初始化兩個字典，用於儲存每個裝置的 \"indoor\" 和 \"emission\" 的紀錄。 dictIndoor = {iD: [] for iD in listId} dictEmission = {iD: [] for iD in listId} # 迴圈遍歷每一個裝置 iD 進行計算。 for iD in listId: # 選取與當前裝置 iD 相符的資料。 dfId = dfMean.loc[iD] # 對於 [1, 7, 14] 這三個天數，計算其 \"indoor\" 和 \"emission\" 的比例。 for day in [1, 7, 14]: # 計算 \"indoor\" 的比例，並四捨五入到小數點後三位。 indoor = (dfId[ dfId['days'] \u003c= day]['indoor'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) # 若 \"indoor\" 的比例大於 0.333，則記錄該裝置的 iD 值，否則記錄 0。 dictIndoor[iD].append( indoor if indoor \u003e 0.333 else 0 ) # 計算 \"emission\" 的比例，並四捨五入到小數點後三位。 emission = (dfId[ dfId['days'] \u003c= day]['emission'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) # 若 \"emission\" 的比例大於 0.333，則記錄該裝置的 iD 值，否則記錄 0。 dictEmission[iD].append( emission if emission \u003e 0.333 else 0 ) 我們接著分別輸出 dictIndoor 和 dictEmission 的內容，觀察判斷出來的結果。\n# 輸出 dictIndoor 的內容。 dictIndoor {'74DA38F207DE': [0, 0, 0], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0.995, 0.86, 0.816], '74DA38F20B80': [0.995, 0.872, 0.867], '74DA38F20BB6': [0, 0, 0], '74DA38F20C16': [0.989, 0.535, 0], '74DA38F20D7C': [0, 0, 0], '74DA38F20D8A': [0, 0, 0], '74DA38F20DCE': [0, 0, 0], '74DA38F20DD0': [0.984, 0.871, 0.865], '74DA38F20DD8': [0, 0.368, 0.369], '74DA38F20DDC': [0, 0, 0], '74DA38F20DE0': [0, 0, 0], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0, 0, 0], '74DA38F20E42': [0.99, 0.866, 0.87], '74DA38F20E44': [0, 0, 0], '74DA38F20F0C': [0.979, 0.872, 0.876], '74DA38F20F2C': [0, 0, 0], '74DA38F210FE': [0.99, 0.847, 0.864]} # 輸出 dictEmission 的內容。 dictEmission {'74DA38F207DE': [0.92, 0.737, 0.735], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0, 0, 0], '74DA38F20B80': [0, 0, 0], '74DA38F20BB6': [0.492, 0.339, 0], '74DA38F20C16': [0, 0, 0], '74DA38F20D7C': [0.553, 0.342, 0.337], '74DA38F20D8A': [0.672, 0.457, 0.388], '74DA38F20DCE': [0.786, 0.556, 0.516], '74DA38F20DD0': [0, 0, 0], '74DA38F20DD8': [0, 0, 0], '74DA38F20DDC': [0.345, 0, 0], '74DA38F20DE0': [0.601, 0.503, 0.492], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0.938, 0.75, 0.75], '74DA38F20E42': [0, 0, 0], '74DA38F20E44': [0.938, 0.69, 0.6], '74DA38F20F0C': [0, 0, 0], '74DA38F20F2C': [0.744, 0.575, 0.544], '74DA38F210FE': [0, 0, 0]} 由上述的結果可以發現，對於 indoor 和 emission 的判斷結果，隨著參考的過往時間長短，存在不小的差異，由於不同過往時間的長度代表不同的參考意義，因此我們使用權重的方式，給予 1 天參考時間的權重為 A ，給予 7 天參考時間的權重為 B ，最後給予 14 天參考時間的權重為 1-A-B 。\n同時，我們考量在一天參考時間內的工作時間約為 24 小時中的 8 小時，七天參考時間內的工作時間約為 168 小時中的 40 小時，14 天參考時間內的工作時間約為 336 小時中的 80 小時，因此若一個感測器被判斷為 indoor 或 emission 類型，其在 MD 中所佔的加權比重應大於等於 MD_thresh，且\nMD_thresh = (8.0/24.0)*A+(40.0/168.0)B+(80.0/336.0)(1-A-B)\n在我們的範例中，我們假定 A=0.2，B=0.3，因此我們可以透過下列的程式獲得加權判斷後的 indoor 和 emission 類別感測器清單。\n# 定義常數 A 為 1 天參考時間的權重， B 表示 7 天參考時間的權重。 A=0.2 B=0.3 # 計算 MD_thresh 的值，此值將用作後續的閾值。 MD_thresh=(8.0/24.0)*A+(40.0/168.0)*B+(80.0/336.0)*(1-A-B) # 初始化兩個列表，用於儲存超過閾值的 \"indoor\" 和 \"emission\" 的裝置ID及其比例。 listIndoorDevice = [] listEmissionDevice = [] # 迴圈遍歷每一個裝置 iD 進行計算。 for iD in listId: # 計算 \"indoor\" 的加權比重。 # 如果加權比重超過 MD_thresh，則將其加入 listIndoorDevice。 rate1 = A*dictIndoor[iD][0] + B*dictIndoor[iD][1] + (1-A-B)*dictIndoor[iD][2] if rate1 \u003e MD_thresh: listIndoorDevice.append( (iD, rate1) ) # 計算 \"emission\" 的加權比重。 # 如果加權比率超過 MD_thresh，則將其加入 listEmissionDevice。 rate2 = A*dictEmission[iD][0] + B*dictEmission[iD][1] + (1-A-B)*dictEmission[iD][2] if rate2 \u003e MD_thresh: listEmissionDevice.append( (iD, rate2) ) 我們接著分別輸出 listIndoorDevice 和 listEmissionDevice 的內容，觀察透過加權判斷出來的結果。\n# 輸出 listIndoorDevice 的內容。 listIndoorDevice [('74DA38F20B20', 0.865), ('74DA38F20B80', 0.8941), ('74DA38F20C16', 0.3583), ('74DA38F20DD0', 0.8906), ('74DA38F20DD8', 0.2949), ('74DA38F20E42', 0.8928), ('74DA38F20F0C', 0.8954), ('74DA38F210FE', 0.8841)] # 輸出 listEmissionDevice 的內容。 listEmissionDevice [('74DA38F207DE', 0.7726), ('74DA38F20D7C', 0.38170000000000004), ('74DA38F20D8A', 0.4655), ('74DA38F20DCE', 0.5820000000000001), ('74DA38F20DE0', 0.5171), ('74DA38F20E0E', 0.7876), ('74DA38F20E44', 0.6945999999999999), ('74DA38F20F2C', 0.5933)] 即時污染偵測模組 (RED) 實作 針對即時污染偵測 (Real-time Emission Detection, RED)，我們假設某個微型感測器在連續的取樣中，如果最新的感測數值比前一次的感測數值多出 1/5 以上，代表其周遭環境出現劇烈的變化，極有可能是因為周遭出現空汙的排放現象。由於當感測數值較小時，1/5 的變化量其實在 PM2.5 的真實濃度變化上仍十分輕微，因此我們也排除感測數值小於 20 的狀況，以避免因為感測器本身的誤差導致即時污染偵測的誤判。\n# 初始化 'red' 列，預設為 False。 dfMean['red'] = False # 迴圈遍歷每一個裝置 iD 進行計算。 for iD in listId: # 選取與當前裝置 iD 相符的資料。 dfId = dfMean.loc[iD] p_index = '' # 前一筆資料的索引。 p_row = [] # 前一筆資料的內容。 # 遍歷當前裝置 iD 的每一筆資料。 for index, row in dfId.iterrows(): red = False # 初始化當前資料的 'red' 值為 False。 # 如果有前一筆資料，計算當前資料和前一筆資料的 PM2.5 差值。 if p_index: diff = row['PM2.5'] - p_row['PM2.5'] # 若前一筆資料的 PM2.5 值大於 20 且差值大於前一筆資料的 PM2.5 值的 1/5，將 'red' 設為 True。 if p_row['PM2.5']\u003e20 and diff\u003ep_row['PM2.5']/5: red = True # 更新 dfMean 中對應的 'red' 值。 dfMean.loc[pd.IndexSlice[iD, index.strftime('%Y-%m-%d %H:%M:%S')], pd.IndexSlice['red']] = red p_index = index # 更新前一筆資料的索引。 p_row = row # 更新前一筆資料的內容。 # 顯示 dfMean 的內容。 dfMean 感測器可靠度評估模組 (DR) 實作 最後，我們總結 RED 與 MD 兩個模組的判斷結果，進行微型感測器的可靠度評估 (Device Ranking, DR)，其主要概念為：\n如果感測器的感測資料常常被判定為時間類異常或空間類異常，代表該感測器的硬體，或者其所屬的環境可能有潛在的問題，需要進一步釐清。 如果感測器的感測資料較少被判定為異常，代表該感測器的感測數值與周遭感測器的數值具備很高的一致性，因此可信度高。 我們以日為單位，根據每個感測器每日的所有感測資料，統計曾經被判斷為時間類異常 (red=True) 或空間類異常 (indoor=True 或 emission=True) 的總次數，並計算其占一日所有資料筆數中的比例，以此做為感測器的資訊是否可靠的依據。\n# 初始化一個空的 DataFrame 來存放每個裝置的排名資訊。 device_rank = pd.DataFrame() # 迴圈遍歷每一個裝置 iD 進行計算。 for iD in listId: # 選取與當前裝置 iD 相符的資料。 dfId = dfMean.loc[iD] # 初始化異常計數和總數的字典。 abnormal = {} num = {} # 遍歷當前裝置 iD 的每一筆資料。 for index, row in dfId.iterrows(): # 初始化日期的計數值 d = index.strftime('%Y-%m-%d') if d not in abnormal: abnormal[d] = 0 if d not in num: num[d] = 0 num[d] = num[d] + 1 # 檢查每一筆資料是否符合異常條件。 if row['indoor'] or row['emission'] or row['red']: abnormal[d] = abnormal[d] + 1 # 根據異常計數和總數，計算每個裝置的排名。 for d in num: # 將排名資訊加入 device_rank。 device_rank = device_rank.append(pd.DataFrame({'device_id': [iD], 'date': [d], 'rank': [1 - abnormal[d]/num[d]]})) # 設定 device_rank 的索引為 'device_id' 和 'date'。 device_rank.set_index(['device_id','date']) 參考資料 Ling-Jyh Chen, Yao-Hua Ho, Hsin-Hung Hsieh, Shih-Ting Huang, Hu-Cheng Lee, and Sachit Mahajan. ADF: an Anomaly Detection Framework for Large-scale PM2.5 Sensing Systems. IEEE Internet of Things Journal, volume 5, issue 2, pp. 559-570, April, 2018. (https://dx.doi.org/10.1109/JIOT.2017.2766085) ",
    "description": "我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "6.2. 異常資料偵測",
    "uri": "/ch6/ch6.2/"
  },
  {
    "content": " Table Of Contents 章節目標 資料來源 Tableau 基本操作說明 資料導入 工作表 (Worksheet) 簡介 Tableau 應用範例ㄧ：空品資料時空分佈圖 繪製空間分布圖 (地理資訊地圖) 繪製時間分布圖(折線圖) Tableau 應用範例二：災情通報資料儀表板 (Dashboard) 災情資料格式轉換 儀錶板大小設定 將工作表加入儀錶板 加入互動式按鈕 讓同一個互動按鈕作用於多個工作表 加入其他資訊 文本 (Story) 總結 參考資料 在資料分析中，將資料視覺化往往可以協助我們對資料有更進一步的理解。而在將資料圖像化時，不同的資料類型在進行資料圖像化時所使用的圖表類型也有所不同，例如在呈現有經緯度座標的資料時，多半會使用地圖類的圖表，而在呈現時間序列資料時，則可以使用折線圖、直方圖等。然而，許多的資料常常具備不只一種特性，如民生公共物聯網的感測器資料就同時具備了經緯座標及時間序列，因此在呈現時，往往需要用到不只一種圖表才能呈現資料的意涵。而本章節要來介紹一款十分方便的軟體 Tableau，來幫助我們呈現上述複雜的資料型態。\nTableau 是一個提供使用者容易上手的視覺化分析平台，除了可以用其來快速製作各式圖表外，其 Dashboard 更是可以讓使用者更加方便地呈現不同的圖表資料；此外，Dashboard 還可以讓讀者與圖表進行互動，除了可以幫助讀者更加輕鬆快速地理解資料外，同時也讓資料分析變得更加地容易。Tableau 原本是一款付費的商業軟體，不過它也有提供免費版的 Tableau Public，來供大家使用。然而要注意的是，因為是 “Public” 所以利用它所製作出來的成果均會被公開。\n註釋 本文所操作之 Tableau Public 版本為 Tableau Desktop Public Edition (2022.2.2 (20222.22.0916.1526) 64 位)，惟本文所使用之功能皆為該軟體之基本功能，若使用其他版本軟體，應仍可正常操作。\n章節目標 知道如何將資料導入 Tableau 進行圖像化呈現 利用工作表 (Worksheet) 來繪製出分析圖表 利用儀錶板 (Dashboard) 及文本 (Story) 來設計出互動式圖表/簡報 資料來源 民生公共物聯網歷史資料 - 空品資料 - 中研院_校園空品微型感測 (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器) 民生公共物聯網歷史資料 - 災害示警與災情通報 - 消防署_災情通報 (https://history.colife.org.tw/#/?cd=%2F災害示警與災情通報%2F消防署_災情通報) Tableau 基本操作說明 資料導入 Tableau 可以讀取文字檔 (csv) 及空間資訊檔 (shp、geojson)，要匯入資料時點選 資料 \u003e 新增資料來源，來選擇要匯入的資料檔案。\n根據輸入的檔案類型不同，下面提供不同的操作範例：\n空間檔案 (shp、geojson) 匯入範例\n首先點擊左下角的資料來源，可以看到現在匯入的資料欄位等。 ![Tableau screenshot](figures/7-2-3-2.png) 接著點擊欄位名稱的上方符號可以更改該欄位資料的屬性。\n最後將經緯度座標賦予地理角色，以便後續的繪圖工作\n文字檔案 (CSV) 匯入範例\n將 PM 2.5 的紀錄值及測站位置匯入 Tableau 後，先建立起資料表之間的關係：\n接下來建立 PM 2.5 測站與測站座標之間的連結，點擊兩下測站資料進入聯結畫布，將另一個資料表拖到聯結畫布上。\n最後點選中間的連結圖示，設定連結形式以及運算邏輯。\n當設定完成後，可以看到右下角的 Station id 是從 Location 的資料表來的，這樣一來就把 PM 2.5資料與測站位置資料連結在一起了。\n工作表 (Worksheet) 簡介 工作表是 Tableau 可以用來將資料視覺化的地方，除了可以將資料繪製成圓餅圖、長條圖、折線圖等傳統圖形外，也可以用來繪製地理資訊的地圖。接下來將來展示如何繪製不同的圖表。\n在處理完資料後，點選左下的新增工作表來進行繪圖，進入工作表後可以看到下圖介面。\n在工作表的右上方有一個顯示的按鈕，點擊下去後便會看到不同類型的圖表 (系統會自動判斷可以繪製的圖表，無法繪製的會以反白的方式來呈現) ，接著可以點擊想要的圖表來更改圖表類型。\nTableau 應用範例ㄧ：空品資料時空分佈圖 繪製空間分布圖 (地理資訊地圖) 在開始繪製之前，我們需要先將經緯度從度量改成維度，有關度量及維度的詳細解說可見參考資料，詳細操作過程如以下動畫：\n接著我們將經緯度拉到欄及列的位置，並將PM 2.5值拉到標記位置並將其標記改為色彩，即可產生全台PM 2.5的分布圖，如以下動畫。\n我們可以點選色彩來改變圓點的顏色，而在上方的篩選條件則可以篩選特定條件的PM 2.5值，並且點選 PM 2.5 \u003e度量，則可以選擇 PM 2.5 的平均值、總和、中位數等。下面將示範如何顯示新北市的 PM 2.5值。\n將Site Name拉到篩選條件 點選萬用字元，選擇包含後輸入新北市 點選確定 繪製時間分布圖(折線圖) 我們將 Timestamp 拉到欄，並將 Site Name 及 PM 2.5 值拉到列。接著點選 Timestamp 便可以設定時間的間隔，例如下圖中我們將時間間隔設定為 1 小時。注意：跟繪製空間分布圖時一樣，這裡也可以用篩選功能對要展示的測站進行篩選。而下方的動畫中，我們示範了如何顯示位於高雄的PM2.5一整天的監測數值。\nTableau 應用範例二：災情通報資料儀表板 (Dashboard) 儀錶板可以將不同的工作表 (圖表) 組合在一起，以此來呈現更加豐富的資訊，並讓使用者可以動態的檢視資料，在接下來的範例中，我們將介紹如何建立簡單的儀表板。由於在前面的文章中我們已經用過空品資料做為範例，因此我們在這邊將改使用暴雨災情資料來作為後續示範用途。\n災情資料格式轉換 我們使用 2018/08/23 所發生的 823 水災為例，由於原始檔案格式為 xml 檔，我們首先使用下方網址將原始檔案轉為csv檔：https://www.convertcsv.com/xml-to-csv.htm (如下動畫)\n儀錶板大小設定 在 Tableau 選單中點選儀錶板後，在左方的工具列表中可以設定儀表板的大小。\n將工作表加入儀錶板 接著在左方的工具列表中可以看到之前創建的工作表，將其拖拉至儀表板的空白處，即可在儀表板中加入工作表。\nTableau 在接收到工作表的資料後，便會自動讀取內容的資訊，並自動產生對應的初始圖表。\n由於剛拖入的工作表是固定大小且無法隨意改變的，這時在工作表的右上方有一個往下的箭頭，點下後選擇「漂浮」後，就可以改變圖形的大小。\n加入互動式按鈕 接著我們展示如何提供互動的介面，讓使用者可以自行挑選災情資料中，想要觀察的時間點。\n我們首先點選工作表右上方往下的箭頭，點下後選擇「篩選條件」，並選擇「日 (Case Time)」。\n點選後便會看到工作表彈出一個可以勾選日期的選擇欄位，這時使用者只要勾選想要觀察的日期，即可看到該日的災情情形。\n若我們想要將日期的選擇方式，也可以點選選擇欄位右上方的往下鍵號，便可以改變選擇欄位的樣式。\n例如我們可以將選擇的方式，由原先的清單方式改成滑桿方式如下。\n讓同一個互動按鈕作用於多個工作表 上述的互動按鈕只能用於一個工作表，若想讓儀錶板上的多個工作表同時共用同一個互動按鈕，可參考下列作法：\n照上面的方法先建立一個互動按鈕欄位；\n點選互動按鈕欄位右上方的往下鍵號，選擇套用於工作表並點擊選取的工作表；\n選擇要套用的工作表，即可完成設定。\n加入其他資訊 在儀表板上若需要加入其他的資訊，可以在工具列的左下方找到一個物件欄位，裡面有加入文字、圖片等物件，只要將所需的物件拖拉到儀錶板上方即可在儀表板中加入文字、圖片等物件。\n文本 (Story) 文本是可以將多個儀表板或工作表組合，以此來做成類似投影片的顯示效果。將文本加入儀表板或工作表的方法，跟從儀錶板中加入工作表一樣，將右方已創建的工作表或儀表板拖入文本的空白處即可完成。\n若有新增文本頁面的需求，可以點選上方的新增文本點，即可新增新的空白頁面，或是複製現有頁面成為新的頁面。\n最後，為了讓文本資訊更貼切其表達的內容，我們點擊文本頁面上方的方框兩次，即可修改頁面的標題。\n總結 在本章節簡單的介紹了 Tableau 的一些操作方法，以及如何利用 Tableau 來設計出與讀者互動的簡報/圖表，然而 Tableau 的功能遠遠不止這些，而網路上也有許多利用 Tableau 所創作出來的互動式簡報範例，因此鼓勵大家多多去其他網站上學習。\n參考資料 Tableau Public (https://public.tableau.com/) 使用 Tableau 設計自選式視覺化圖表—初階講義 (https://www.beclass.com/share/202007/7145995555358rap_2.pdf) 維度和度量（藍色和綠色）(https://help.tableau.com/current/pro/desktop/zh-tw/datafields_typesandroles.htm) VISUALIZING HEALTH DATA 看見健康數據 (https://visualizinghealthdata.idv.tw/?route=article/faq\u0026faq_id=138) Get Started with Tableau (https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-home.htm) Tableau Tutorial — Learn Data Visualization Using Tableau (https://medium.com/edureka/tableau-tutorial-71ef4c122e55) YouTube: Tableau教程 (https://www.youtube.com/watch?v=1q8rlXS4gEc\u0026list=PLAKvINjDAWNImaqKWJIErQtaNi19q84yF) YouTube: 用 Tableau 10.3 做大數據分析初階 (https://www.youtube.com/watch?v=oS5cM3v9ILc) YouTube: 用Tableau 做大數據分析進階 (https://www.youtube.com/watch?v=HuBQ-4khWm8) YouTube: Tableau For Data Science 2022 (https://www.youtube.com/watch?v=Wh4sCCZjOwo) YouTube: Tableau Online Training (https://www.youtube.com/watch?v=ttCDqyfrcEc) ",
    "description": "我們介紹使用 Tableau 工具呈現民生公共物聯網的開放資料，並使用空品資料和災害通報資料進行兩個範例演示。我們介紹如何使用工作表、儀表板和文本來建立互動式的資料視覺化系統，方便使用者近一步深入探索。我們同時提供豐富的參考資料供使用者學習參考。",
    "tags": [
      "空",
      "災"
    ],
    "title": "7.2. Tableau 應用",
    "uri": "/ch7/ch7.2/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引用 讀取資料與資料預處理 (Preprocessing) 資料分群 / 集群分析 (Clustering) 快速傅立葉轉換 (Fast Fourier Transform) 小波轉換 (Wavelet Transform) 參考資料 集群分析 (Cluster Analysis) 是資料科學中常見的資料處理方法，其主要目的是被用來找出資料中相似的群聚，透過集群分析後，將性質相近的資料群聚在一起，以方便使用者可以針對特徵相似的資料進行更深入的分析與處理。在民生公共物聯網的開放資料中，由於每一種感測器的資料都是時序性的資料，為了將為數眾多的感測器進行適當的分群，以利更深入的資料分析，我們在這個單元將介紹時序資料在分群時常使用的特徵擷取方法，以及在集群分析時常使用到的分群方法。\n章節目標 學習使用快速傅立葉轉換 (FFT) 與小波轉換 (Wavelet) 擷取時序資料的特徵 採用非監督式學習的方法，針對時序資料進行分群 套件安裝與引用 在本章節中，我們將會使用到 pandas, numpy, matplotlib, pywt 等 Colab 開發平台已預先安裝好的套件，以及另外一個 Colab 並未預先安裝的套件 tslearn ，需使用下列的方式自行安裝：\n# 更新 pip 至最新版本 !pip install --upgrade pip # 安裝 tslearn 套件，它是一個針對時間序列分析的套件 !pip install tslearn 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\n# 引入 numpy 和 pandas，這兩個套件是進行資料分析時非常基本且常用的。 import numpy as np import pandas as pd # pywt 是進行小波轉換的套件，通常用於訊號處理。 import pywt # os 和 zipfile 用於處理文件和壓縮檔。 import os, zipfile # 引入 matplotlib.pyplot 來繪製圖表。 import matplotlib.pyplot as plt # datetime 和 timedelta 是兩個處理日期和時間的模組。 from datetime import datetime, timedelta # fft 和 ifft 是進行快速傅立葉轉換的函式。 from numpy.fft import fft, ifft # cwt 是連續小波轉換的函式。 from pywt import cwt # TimeSeriesKMeans 是 tslearn 套件裡的一個用於時間序列資料集的 k-均值聚類方法。 from tslearn.clustering import TimeSeriesKMeans 讀取資料與資料預處理 (Preprocessing) 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的 2021 年歷史資料，並存入 Air 資料夾中。\n# 建立名為 Air 和 CSV_Air 的資料夾 !mkdir Air CSV_Air # 下載 2021.zip 壓縮檔到 Air 資料夾 !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" # 將 Air/2021.zip 這個壓縮檔解壓縮到 Air 資料夾 !unzip Air/2021.zip -d Air 同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先將其解壓縮，產生每日資料的壓縮檔案。由於我們在接下來的範例中只會使用到 2021/08 的資料，因此我們將 202108 子資料夾中的檔案進行解壓縮，讀取當中 csv 檔的資料，再存入 air_month 這個 dataframe 中。\n# 設定想要操作的資料夾路徑和兩種檔案的副檔名 folder = 'Air/2021/202108' extension_zip = 'zip' extension_csv = 'csv' # 遍歷該資料夾下的所有檔案 for item in os.listdir(folder): # 如果檔案是 .zip 壓縮檔 if item.endswith(extension_zip): file_name = f'{folder}/{item}' # 獲取該壓縮檔的完整路徑 zip_ref = zipfile.ZipFile(file_name) # 開啟該壓縮檔 zip_ref.extractall(folder) # 解壓縮到指定資料夾 zip_ref.close() # 關閉該壓縮檔 # 創建一個空的 DataFrame 用來存放合併後的資料 air_month = pd.DataFrame() # 再次遍歷該資料夾下的所有檔案 for item in os.listdir(folder): # 如果檔案是 .csv 檔 if item.endswith(extension_csv): file_name = f'{folder}/{item}' # 獲取該 .csv 檔的完整路徑 # 讀取該 .csv 檔，並將「timestamp」欄位解析為 datetime 格式 df = pd.read_csv(file_name, parse_dates=['timestamp']) air_month = air_month.append(df) # 將讀取到的資料加到 air_month DataFrame 中 # 將「timestamp」欄位設為索引 air_month.set_index('timestamp', inplace=True) # 根據「timestamp」欄位對資料進行排序 air_month.sort_values(by='timestamp', inplace=True) 目前 air_month 的格式還不符合我們的需求，需要先將其整理成以站點資料為欄，以時間資料為列的資料格式。我們先找出資料中共有多少站點，並將這些站點資訊存在一個序列中。\n# 從 air_month 資料框中取出「device_id」欄位的所有資料，並儲存為 numpy 陣列 id_list = air_month['device_id'].to_numpy() # 使用 numpy 的 unique 函數找出 id_list 中的唯一裝置ID id_uniques = np.unique(id_list) # 這時，id_uniques 陣列會包含所有唯一的裝置ID id_uniques array(['08BEAC07D3E2', '08BEAC09FF12', '08BEAC09FF22', ..., '74DA38F7C648', '74DA38F7C64A', '74DA38F7C64C'], dtype=object) 接著我們將每個站點的資料存成一個欄，並放入 「air」 這個 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\n# 創建一個空的 DataFrame air = pd.DataFrame() # 遍歷每一個裝置ID for i in range(len(id_uniques)): # print('device_id==\"' + id_uniques[i] + '\"') # 從 air_month 中過濾出當前裝置的資料 query = air_month.query('device_id==\"' + id_uniques[i] + '\"') # 根據時間戳對資料進行排序 query.sort_values(by='timestamp', inplace=True) # 按小時重新取樣，計算平均值 query_mean = query.resample('H').mean() # 將 PM25 的列名重新命名為當前裝置 ID query_mean.rename(columns={'PM25': id_uniques[i]}, inplace=True) # 將重新取樣的資料添加到 air DataFrame 中 air = pd.concat([air, query_mean], axis=1) # 刪除 Air 目錄 !rm -rf Air 我們可以用下列的語法快速查看 air 的內容。\n# 從資料框 air 取得資訊，如資料的形狀、列名稱、非空資料的數量等。 air.info() # 顯示資料框 air 的前五行，幫助使用者初步了解資料的樣子。 print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 745 entries, 2021-08-01 00:00:00 to 2021-09-01 00:00:00 Freq: H Columns: 1142 entries, 08BEAC07D3E2 to 74DA38F7C64C dtypes: float64(1142) memory usage: 6.5 MB 08BEAC07D3E2 08BEAC09FF12 08BEAC09FF22 08BEAC09FF2A \\ timestamp 2021-08-01 00:00:00 2.272727 1.250000 16.818182 12.100000 2021-08-01 01:00:00 1.909091 1.285714 13.181818 14.545455 2021-08-01 02:00:00 2.000000 1.125000 12.727273 16.600000 2021-08-01 03:00:00 2.083333 3.000000 11.800000 12.090909 2021-08-01 04:00:00 2.000000 2.600000 10.090909 8.545455 08BEAC09FF34 08BEAC09FF38 08BEAC09FF42 08BEAC09FF44 \\ timestamp 2021-08-01 00:00:00 2.727273 0.000000 1.181818 NaN 2021-08-01 01:00:00 2.000000 0.545455 0.909091 NaN 2021-08-01 02:00:00 4.090909 1.583333 0.636364 NaN 2021-08-01 03:00:00 0.545455 1.454545 1.181818 NaN 2021-08-01 04:00:00 1.363636 1.363636 2.454545 NaN 08BEAC09FF46 08BEAC09FF48 ... 74DA38F7C62A \\ timestamp ... 2021-08-01 00:00:00 2.636364 2.545455 ... 6.777778 2021-08-01 01:00:00 1.636364 2.272727 ... 7.800000 2021-08-01 02:00:00 1.400000 3.100000 ... 7.300000 2021-08-01 03:00:00 2.181818 4.000000 ... 12.000000 2021-08-01 04:00:00 1.909091 2.100000 ... 9.000000 74DA38F7C630 74DA38F7C632 74DA38F7C634 74DA38F7C63C \\ timestamp 2021-08-01 00:00:00 9.800000 13.200000 5.0 6.200000 2021-08-01 01:00:00 13.000000 15.700000 5.2 6.800000 2021-08-01 02:00:00 12.800000 19.300000 5.0 7.300000 2021-08-01 03:00:00 8.444444 15.200000 5.1 6.777778 2021-08-01 04:00:00 6.500000 10.222222 4.9 6.100000 74DA38F7C63E 74DA38F7C640 74DA38F7C648 74DA38F7C64A \\ timestamp 2021-08-01 00:00:00 NaN 7.500000 5.000000 9.000000 2021-08-01 01:00:00 NaN 10.200000 4.900000 6.600000 2021-08-01 02:00:00 NaN 10.500000 3.666667 7.600000 2021-08-01 03:00:00 NaN 8.500000 8.400000 8.000000 2021-08-01 04:00:00 NaN 5.571429 6.200000 5.666667 74DA38F7C64C timestamp 2021-08-01 00:00:00 7.600000 2021-08-01 01:00:00 7.700000 2021-08-01 02:00:00 7.888889 2021-08-01 03:00:00 6.400000 2021-08-01 04:00:00 5.000000 接下來，我們首先將資料中有資料缺漏的部分 (數值為 Nan) 予以刪除，並且先將資料繪製成圖形，觀察資料的分布狀況。\n# 將資料框 air 的最後一行刪除 air = air[:-1] # 使用 dropna 方法移除在任何欄位中有缺失值的列 # axis=1 代表操作在欄位上；how='any' 代表只要欄位中有任何缺失值就刪除整個欄位 air_clean = air.dropna(1, how='any') # 繪製資料框 air_clean 中的所有資料，不顯示圖例 air_clean.plot(figsize=(20, 15), legend=None) 由於原始資料中感測器的瞬間數值容易因為環境變化而有瞬間劇烈的變動，因此我們使用移動平均的方式，將每十次的感測值進行移動平均，讓處理過的資料可以較為平順並反映感測器周遭的整體趨勢，以方便進入接下來的集群分析。\n# 使用移動平均法對 air_clean 進行平滑處理。window 設定為 10 代表考慮前 10 個資料點的平均值；min_periods=1 表示資料點數量至少有 1 個時才計算平均。 air_clean = air_clean.rolling(window=10, min_periods=1).mean() # 繪製平滑後的 air_clean 資料，設定圖片大小並不顯示圖例。 air_clean.plot(figsize=(20, 15), legend=None) 資料分群 / 集群分析 (Clustering) 由於目前 dataframe 內的資料儲存是將每一個測站的資料分別放在獨立的一欄 (column) 中，而將每一列 (row) 存放特定時間點所有站點的感測器數值，此種資料格式較適合作以時間為主軸的時序資料處理與分析，但是對於我們接下來要進行的資料分群，反而需要改成以測站為主軸進行資料處理，因此我們在開始之前，需要先將現有資料進行轉製 (transpose)，也就是將資料的欄與列交換，然後才能進入資料分群。\n資料分群已是一項十分普遍的資料科學方法，在許多常見的資料分群方法中，我們選用 tslearn 套件所提供的 KMeans 分群方法 (TimeSeriesKMeans)，在機器學習的領域中，把這類的方法歸類為一種「非監督式學習」的方法，因為在分群的過程中並沒有一定的標準將資料歸為某一群，而是只利用資料彼此間的相似度決定分群，因此後續若要找出離群值或進行預測也更為方便。\nKMeans 分群法的內部運作大致分為下列幾個步驟：\n先決定 k 的值，亦即最後要分成幾個群； 隨機 k 筆資料，當為起始的 k 個群的中心點 (又稱「群心」)； 根據距離公式計算每筆資料到每個群心的距離，並選擇最近的群心，將該筆資料歸屬到該群； 針對每個群，重新計算其新的群心，並重複上述步驟，直到 k 的群的群心不再有變動為止。 我們先設定 k=10，並使用 TimeSeriesKMeans，將資料分為 10 群 (0~9) 如下：\n# 將 air_clean 資料框進行轉置，使得時間序列資料位於列上，各感測器的資料為行。 air_transpose = air_clean.transpose() # 建立時間序列分群模型。指定分群數量為10，使用動態時間規整 (DTW) 作為計算距離的指標，最多迭代 5 次進行分群。 model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) # 使用轉置後的資料訓練分群模型。 pre = model.fit(air_transpose) # 顯示模型對各感測器資料的分群結果。 pre.labels_ array([3, 3, 6, 3, 3, 3, 3, 6, 9, 6, 3, 6, 3, 3, 3, 3, 3, 1, 3, 3, 6, 3, 3, 3, 3, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 9, 3, 3, 3, 3, 3, 6, 3, 2, 6, 3, 2, 3, 3, 3, 6, 9, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 6, 3, 3, 3, 3, 3, 6, 3, 6, 3, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 6, 9, 6, 7, 3, 2, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 6, 6, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 6, 3, 3, 3, 0, 6, 0, 6, 3, 0, 3, 3, 3, 6, 6, 6, 6, 3, 3, 6, 3, 3, 9, 6, 3, 6, 3, 6, 6, 3, 2, 3, 3, 3, 6, 9, 6, 6, 3, 3, 6, 3, 0, 3, 3, 6, 6, 6, 3, 0, 0, 6, 3, 6, 6, 6, 3, 6, 6, 3, 0, 3, 3, 0, 3, 3, 6, 3, 6, 6, 6, 3, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 9, 9, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 9, 0, 9, 0, 0, 0, 9, 9, 0, 9, 0, 0, 0, 5, 0, 0, 0, 0, 9, 9, 0, 1, 0, 9, 2, 9, 9, 0, 0, 5, 0, 9, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 2, 3, 3, 0, 3, 0, 0, 0, 6, 3, 0, 8, 9, 3, 9, 9, 0, 5, 0, 3, 0, 9, 9, 5, 5, 5, 3, 5, 9, 3, 0, 0, 3, 0, 0, 0, 3, 5, 5, 3, 0, 5, 6, 5, 5, 5, 0, 9, 0, 6, 6, 6, 3, 0, 3, 5, 5, 9, 6, 3, 0, 0, 0, 0, 3, 5, 0, 5, 5, 0, 3, 3, 9, 5, 5, 9, 5, 9, 9, 9, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5]) 由於分群的結果有可能因為少數感測器擁有特殊的資料特徵而自成一個群集，而這些感測器其實可以被視為偏離測站，並沒有進一步分析的價值，因此我們下一步便要先統計一下每一個分離出來的群集，其內部感測器的個數是否只有一個，如果是的話，便將該群集捨棄掉。例如在這個範例中，我們便會捨棄掉其中只有一個感測器的群集如下：\n# 建立一個輔助用的資料框，將每一個 metric (即原始資料的欄位名稱) 對應到它所屬的分群編號。 df_cluster = pd.DataFrame(list(zip(air_clean.columns, pre.labels_)), columns=['metric', 'cluster']) # 建立一些輔助用的字典和清單。 # 將每一個分群裡的 metrics 彙整到一個清單中。 cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() # 計算每一個分群中有多少 metrics。 cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() # 找出那些只有一個 metric 的分群。 clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] # 找出那些有超過一個 metric 的分群。 clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() # 顯示那些只有一個 metric 的分群。 clusters_dropped [7, 4, 8] 最後，我們將剩下的群集分別用繪圖的方式展示，可以發現每一個群集內的資料皆呈現極高的相似度，而不同群集的資料，彼此則呈現明顯可分辨的差異。為了量化群集內資料的相似度，我們定義一個新的變數 qualiuty，且這個變數的值等於內部每筆資料與其他資料的相關性 (correlation) 的平均值，且 quality 的值越低代表這個群集內部的資料越相似。\n# 根據最終的分群數量，設定子圖的數量並初始化圖形。 fig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) # 遍歷所有的分群。 for idx, cluster_number in enumerate(clusters_final): # 計算該分群中的時間序列之間的相關性。 x_corr = air_clean[cluster_metrics_dict[cluster_number]].corr().abs().values # 計算相關性矩陣中上三角形部分的平均值，排除對角線。 x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) # 設定子圖的標題，包含分群編號、平均相關性以及該分群中的時間序列數量。 plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' # 繪製該分群中的所有時間序列。 air_clean[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) # 移除圖例。 axes[idx].get_legend().remove() # 調整子圖的間距，確保不會重疊。 fig.tight_layout() # 顯示圖形。 plt.show() 到目前為止，我們已經展示如何使用空品感測的資料，針對資料內的特徵資訊進行資料分群，然而由於原始資料中夾雜許多的雜訊干擾，因此在分群上的表現仍有所不足。為了減少原始資料中的雜訊干擾，我們以下介紹兩種常用的進階方法：傅立葉轉換 (Fourier transform)、小波轉換 (wavelet transform)，分別提取時序資料的進階特徵後，再用來進行資料分群，可以更有效提升感測器分群的成效。\n快速傅立葉轉換 (Fast Fourier Transform) 傅立葉轉換 (Fourier Transform) 是一種常用的信號分析方法，能將原始資料由時域 (time domain) 轉換至頻域 (frequency domain) ，以方便更進一步的特徵擷取與資料分析，常見於工程與數學領域，以及聲音與時序資料的分析。由於標準的傅立葉轉換牽涉到複雜的數學運算，在實作上較為複雜費時，因此後來發展出將原始資料離散化的快速傅立葉轉換方法，可以大幅減少傅立葉轉換的運算複雜度，特別適用於大量數據資料的處理；因此在我們接下來的主題中，將使用快速傅立葉轉換的方法，來協助資料分群所需要的資料特徵的擷取。\n我們首先針對單一測站 (‘08BEAC07D3E2’) 透過繪圖的方式，觀察其在時域下的變化圖。\n# 根據指定的測站ID，繪製該測站的資料。 air_clean['08BEAC07D3E2'].plot() 接著我們使用 numpy 套件中的快速傅立葉轉換工具 fft，將測站的資料輸入進行轉換，並用繪圖的方式觀察轉換到頻域後的資料分佈狀況。\n# 使用快速傅立葉轉換 (FFT) 計算資料的頻譜 X = fft(air_clean['08BEAC07D3E2']) N = len(X) # 取得資料的長度 n = np.arange(N) # 產生 0 到 N-1 的序列 # 獲得取樣率，這裡是每小時一次，所以取樣率為 1/3600 sr = 1 / (60*60) T = N/sr # 總取樣時間 freq = n/T # 計算每個頻率成分 # 取得單側的頻譜，因為 FFT 結果是對稱的 n_oneside = N//2 f_oneside = freq[:n_oneside] # 繪製頻譜 plt.figure(figsize = (12, 6)) plt.plot(f_oneside, np.abs(X[:n_oneside]), 'b') # 使用絕對值表示振幅 plt.xlabel('Freq (Hz)') # x軸標籤 plt.ylabel('FFT Amplitude |X(freq)|') # y 軸標籤 plt.show() # 顯示圖形 然後我們將相同的轉換步驟，擴及到所有的感測器，並且將所有感測器完成快速傅立葉轉換後的結果，儲存在 air_fft 變數中。\n# 創建一個空的資料框來存放每個測站的 FFT 結果 air_fft = pd.DataFrame() # 獲取所有測站的名稱 col_names = list(air_clean.columns) # 對每個測站的資料執行 FFT for name in col_names: X = fft(air_clean[name]) # 計算 FFT N = len(X) # 獲取資料的長度 n = np.arange(N) # 產生 0 到 N-1 的序列 # 獲取取樣率，這裡是每小時一次，所以取樣率為 1/3600 sr = 1 / (60*60) T = N/sr # 計算總取樣時間 freq = n/T # 計算每個頻率成分 # 取得單側的頻譜，因為 FFT 結果是對稱的 n_oneside = N//2 # 將計算的 FFT 振幅存到資料框中 f_oneside = freq[:n_oneside] air_fft[name] = np.abs(X[:n_oneside]) # 顯示 FFT 資料框的前五行 print(air_fft.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 12019.941563 11255.762431 13241.408211 12111.740447 11798.584351 1 3275.369377 2640.372699 1501.672853 3096.493565 3103.006928 2 1109.613670 1201.362571 257.659947 1085.384353 1128.373457 3 2415.899146 1631.128345 888.838822 2281.597031 2301.936400 4 1130.973327 446.032078 411.940722 1206.512460 1042.054041 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 12441.845754 11874.390693 12259.096742 3553.255916 13531.649701 ... 1 3262.357287 2999.042917 1459.720167 1109.764942 646.846038 ... 2 1075.877632 1005.445596 478.569869 368.572815 1163.425916 ... 3 2448.646527 2318.870954 956.029693 272.486798 553.409732 ... 4 1087.461354 1172.755489 437.920193 471.824176 703.557830 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 11502.841221 10589.689400 11220.068048 10120.220198 11117.146124 1 2064.762890 1407.105290 1938.647888 1126.088084 2422.787262 2 2163.528535 1669.014077 2054.586664 1759.326882 1782.523300 3 1564.407983 1157.759192 1253.849261 1244.799149 1519.477057 4 1484.232397 1177.909914 1318.704021 1106.349846 1373.167639 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 11243.094213 26.858333 9408.414826 11228.246949 8931.871618 1 2097.343959 15.020106 1667.485473 1687.251179 1395.239491 2 1806.524987 10.659603 1585.987276 1851.628868 1527.925427 3 1521.392873 6.021244 1217.547879 1240.173667 1022.239794 4 1393.469185 3.361938 1161.975844 1350.756178 1051.434944 [5 rows x 398 columns] 我們接著使用和前面相同的方法，使用 TimeSeriesKMeans 針對已轉換到頻域的感測器資料分成 10 個群集，並將分群後只有單一感測器的群集予以刪除，在這個範例中，最後會剩下 9 個群集，我們把每個感測器所屬的群集代碼列印出來。\n# 將 FFT 的結果進行轉置，因為後面的分群模型需要的輸入格式是每列是一段時間序列，每行是不同的時間點。 fft_transpose = air_fft.transpose() # 將資料行列交換以符合分群模型輸入的需求 # 使用時間序列 K 均值進行分群，這裡選擇了 10 個群，使用動態時間歪曲 (DTW) 作為距離度量。 model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) pre = model.fit(fft_transpose) # 建立一個輔助資料框，將每個測站 (即每個資料行) 映射到其對應的群標籤。 df_cluster = pd.DataFrame(list(zip(air_fft.columns, pre.labels_)), columns=['metric', 'cluster']) # 創建輔助的字典和列表來了解每個群中有多少測站，以及哪些測站屬於哪個群。 cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() # 找出只有一個測站的群 (這些群對我們可能沒有太大的意義)。 clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] # 找出有多於一個測站的群。 clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() # 顯示每個測站及其所屬的群標籤。 print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 0 3 08BEAC09FF42 7 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 0 8 08BEAC09FF8C 2 9 08BEAC09FF9C 8 最後，我們把九個群集的感測器資料分別繪製出來，可以發現每個群集內的感測器在頻域上的資料皆有極度的相似性，同時不同群集的感測器在頻域上的資料彼此相異程度，亦明顯比同群集內的感測器資料差異度為大。\n# 根據分群的結果建立多個子圖，每個子圖對應一個群。 fig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # 遍歷每一個群，並在對應的子圖上畫出該群內所有測站的FFT結果。 for idx, cluster_number in enumerate(clusters_final): # 計算該群內所有測站之間的相關係數，並取絕對值。 x_corr = air_fft[cluster_metrics_dict[cluster_number]].corr().abs().values # 計算上三角矩陣中所有相關係數的平均值，得到該群的平均相關度。 x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) # 設置子圖的標題，顯示群號、群的平均相關度和群內測站的數量。 plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' # 畫出該群內所有測站的 FFT 結果。 air_fft[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) # 移除圖例，因為對於這樣的圖而言，圖例沒有太大的意義。 axes[idx].get_legend().remove() # 調整子圖之間的間距，使得整體圖形看起來更加整齊。 fig.tight_layout() # 顯示整體圖形。 plt.show() 小波轉換 (Wavelet Transform) 除了傅立葉轉換外，小波轉換也是另一種將原始資料從時域轉換到頻域的常見手法，相較於傅立葉轉換，小波轉換可以提供更多頻域資料的觀察角度，因此近期無論在視訊、音訊、時序等相關的資料分析，已被廣泛地使用，並得到很好的效果。\n我們使用 pywt 套件進行小波轉換的資料處理，由於小波轉換會使用到母小波來擷取時序資料中的特徵，因此我們先用下列的語法查看可使用的母小波名稱。\n# 從 pywt 模組中取得所有連續小波的名稱列表。 wavlist = pywt.wavelist(kind=\"continuous\") # 顯示這些連續小波的名稱。 print(wavlist) ['cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'cmor', 'fbsp', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'shan'] 不同於傅立葉轉換只能看到頻率上的變化，小波轉換可以將有限長的母小波進行縮放並在一段資料上擷取特徵，而在挑選母小波時可以先將其作圖觀察後再做決定，若以 morl 母小波為例，我們可以使用下列的語法進行作圖。\n# 使用 Morlet 小波建立一個連續小波物件。 wav = pywt.ContinuousWavelet(\"morl\") # 定義小波的尺度。 scale = 1 # 計算指定小波的連續小波變換整合。 int_psi, x = pywt.integrate_wavelet(wav, precision=10) # 正規化整合小波以使其最大絕對值為 1。 int_psi /= np.abs(int_psi).max() # 反轉整合小波，得到小波濾波器。 wav_filter = int_psi[::-1] # 定義時間軸。 nt = len(wav_filter) t = np.linspace(-nt // 2, nt // 2, nt) # 繪製小波濾波器的實部。 plt.plot(t, wav_filter.real) plt.ylim([-1, 1]) plt.xlabel(\"time (samples)\") 我們首先進行小波轉換的基本參數設定，並且使用 morl 母小波，針對所選定的感測器，將小波的縮放範圍設為 1~31 倍，進行小波轉換並作圖。\n# 參數設定 F = 1 # 每小時取樣次數 hours = 744 # 31 天的小時數 nos = np.int(F*hours) # 31 天內的總取樣次數 # 從資料框中選擇某一測站的資料 x = air_clean['08BEAC09FF2A'] scales = np.arange(1, 31, 1) # 使用 Morlet 小波進行連續小波變換 coef, freqs = cwt(x, scales, 'morl') # 繪製 scalogram plt.figure(figsize=(15, 10)) plt.imshow(abs(coef), extent=[0, 744, 30, 1], interpolation='bilinear', cmap='viridis', aspect='auto', vmax=abs(coef).max(), vmin=abs(coef).min()) # 顯示小波變換的絕對值 plt.gca().invert_yaxis() # Y 軸反向，使尺度從大到小排列 plt.yticks(np.arange(1, 31, 1)) plt.xticks(np.arange(0, nos/F, nos/(20*F))) # 設定 X 軸的刻度 plt.ylabel(\"scales\") # 設定 Y 軸標籤 plt.xlabel(\"hour\") # 設定 X 軸標籤 plt.colorbar() # 顯示色條 plt.show() # 顯示圖形 整體而言，我們可以發現在圖中 scales 較大時的顏色較偏向黃綠色，代表擷取出來的特徵與母小波較為相近，反之則較偏向藍色，這些比對結果的特徵數值將會被用於接下來的資料分群。\n不過，由於經過小波轉換後，每個測站的資料已經轉為二維的特徵數值，在開始進行資料分群前，我們需要先將原本二維資料的每一個欄位串接起來，變成一維的資料格式，並存入 air_cwt 變數中。\n# 初始化空的資料框用於存儲所有測站的小波連續變換結果 air_cwt = pd.DataFrame() # 定義要使用的小波尺度範圍，此處選擇了 28, 30 這兩個尺度 scales = np.arange(28, 31, 2) # 從 air_clean 中獲取所有測站的名稱 col_names = list(air_clean.columns) # 對每一個測站的資料進行小波連續變換 for name in col_names: coef, freqs = cwt(air_clean[name], scales, 'morl') # 使用 Morlet 小波進行連續小波變換 air_cwt[name] = np.abs(coef.reshape(coef.shape[0]*coef.shape[1])) # 重塑 coef 陣列並取其絕對值 # 顯示資料框 air_cwt 的前五行 print(air_cwt.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 0.778745 6.336664 2.137342 1.849035 0.778745 1 0.929951 8.348031 2.977576 1.829540 0.958915 2 1.190476 11.476048 3.223773 1.832544 1.292037 3 1.227021 12.890554 4.488244 1.634717 1.338655 4 1.252126 14.103178 5.715656 1.430744 1.376562 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 0.555941 0.334225 8.110581 2.287378 0.409913 ... 1 0.596532 0.201897 8.774271 1.706411 0.311242 ... 2 0.771641 0.294242 8.327808 1.018296 2.754100 ... 3 0.713931 0.065011 8.669036 0.249029 3.278048 ... 4 0.670639 0.193083 8.795376 0.624988 3.628597 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 1.875873 1.090635 0.284901 1.111999 1.049112 1 0.643478 1.446061 0.420701 0.974641 0.478931 2 1.101801 2.404254 1.389941 1.190141 0.511454 3 2.370436 2.510728 1.927692 0.703424 1.054164 4 3.563873 2.387982 2.463458 0.098485 1.448078 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 1.107551 0.000918 0.080841 0.859855 0.415574 1 0.245802 0.005389 1.053260 1.762642 0.190458 2 1.162331 0.009754 2.667145 3.210326 0.516525 3 1.993138 0.014496 3.669712 3.889207 0.666492 4 2.672666 0.020433 4.482730 4.311888 0.677992 [5 rows x 398 columns] 由於經過二維資料轉換成一維資料的程序，每筆資料的特徵數值數量因而增加，將造成後續資料運算的複雜度大幅提升，因此我們先只取前 100 個特徵數值做為每個感測器的代表性特徵，並以此套用 KMeans 方法進行資料分群。\n由於小波轉換可以得到更細微的資料特徵，因此我們在資料分群的過程中，預設分為 20 個群集（讀者可自行測試不同的群集數目設定，觀察結果會產生哪些變化）；此外，我們對於分群結果也一樣先檢查是否存在只有單一感測器的小群集，並將其剔除，以避免少數特殊狀況的感測器，影響整體資料分群的結果。\n# 選擇 air_cwt 資料框的前 101 列作為一個子集 air_cwt_less = air_cwt.iloc[:, 0:101] # 由於 TimeSeriesKMeans 模型需要資料的每一列為一個時間序列，因此我們將資料框轉置 cwt_transpose = air_cwt_less.transpose() # 初始化 TimeSeriesKMeans 模型，設定分群數量為 20，使用 DTW 作為相似度指標，並設定其他參數 model = TimeSeriesKMeans(n_clusters=20, metric=\"dtw\", max_iter=10, verbose=1, n_jobs=-1) # 擬合模型到資料 pre = model.fit(cwt_transpose) # 創建一個幫助資料框，將資料的名稱映射到其相應的分群標籤 df_cluster = pd.DataFrame(list(zip(air_cwt.columns, pre.labels_)), columns=['metric', 'cluster']) # 創建一些幫助字典和列表 cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] # 被捨棄的分群 clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] # 最終使用的分群 clusters_final.sort() # 對分群編號進行排序 # 顯示 df_cluster 的前 10 行以供查看 print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 6 3 08BEAC09FF42 10 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 6 8 08BEAC09FF8C 15 9 08BEAC09FF9C 19 在我們的範例中，經過上述的程序後，最後留下 14 個群集，我們將每個群集內的感測器原始資料一一繪出，我們可以發現相同群集內的感測器資料一致性更加明顯，同時不同群集間的差異也更加細緻明確，比之前使用原始資料或傅立葉轉換時的成效更為明顯。\n# 根據我們的分群數量，建立一個多行、單列的子圖結構 fig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # 對於每個分群，我們會做以下操作： for idx, cluster_number in enumerate(clusters_final): # 計算這個分群中的時間序列的相關性 x_corr = air_cwt[cluster_metrics_dict[cluster_number]].corr().abs().values # 計算平均相關性 x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) # 設定子圖的標題，包括分群號、平均相關性和該分群中的時間序列數量 plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' # 在子圖上繪製該分群中的所有時間序列 air_cwt[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) # 移除子圖的圖例，以保持視覺清晰 axes[idx].get_legend().remove() # 調整子圖的間距，使其適應畫布 fig.tight_layout() # 顯示圖表 plt.show() 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Time Series Clustering — Deriving Trends and Archetypes from Sequential Data | by Denyse | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) How to Apply K-means Clustering to Time Series Data | by Alexandra Amidon | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) Understanding K-means Clustering: Hands-On with SciKit-Learn | by Carla Martins | May, 2022 | Towards AI (https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81) Fast Fourier Transform. How to implement the Fast Fourier… | by Cory Maklin | Towards Data Science (https://towardsdatascience.com/fast-fourier-transform-937926e591cb) PyWavelets/pywt: PyWavelets - Wavelet Transforms in Python (https://github.com/PyWavelets/pywt) ",
    "description": "我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，並且探討不同的資料集與不同時間解析度的資料分群，在真實場域所代表的意義，以及可能衍生的應用。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "4.3. 時間序列屬性分群",
    "uri": "/ch4/ch4.3/"
  },
  {
    "content": "\nTable Of Contents 套件安裝與引用 讀取資料與環境設定 資料前處理 校正模型訓練與驗證 輸出本日最佳校正模型 校正成果簡介 參考資料 在這篇文章中，我們將以民生公共物聯網中的空品資料為例，介紹如何讓兩個不同等級的空品感測資料，可以透過資料科學的方法，進行系統性的動態校正，以達到系統資料融合的目的，讓不同佈建專案的成果，可以合力打造更為全面性的空品感測結果。我們使用下列兩種空品感測系統：\n環保署空品感測器：在傳統的空品監測方式中，以極為專業、大型、昂貴的監測站為主，該專業的監測站由於部署及維護成本較高，通常會是由當地的環境保護機構（EPA）來負責運營。也因此不會在每個社區都有部署。依據台灣環保署網站公告，截至目前 2022 年 7 月為止，台灣的中央監測站數量為 81 座。 微型空品感測器：與傳統的大型專業測站相比，微型空品感測器利用低成本的感測器，透過網路的資料串連，以物聯網的方式建構更密集的空品感測網，這項技術不僅架設的成本低，連帶的提供了更靈活的安裝條件，擴大了可覆蓋的範圍。同時，這項技術具有易於安裝和維護的特點，滿足了大規模即時空氣品質監測系統的條件，而且能夠做到每分鐘上傳一次數據的資料頻率，也使用戶對於突然的污染事件得以立即的反應，進一步的降低傷害。 當然我們不能期待成本較低的感測器會擁有專業儀器的高準確度，如何提高其精準度成為成了另一項需要被解決的問題。因此，在以下的內容中，我們將演示如何利用資料科學的方法，調校微型空品感測器的空品感測結果，讓其感測資料的準確度能達到與環保署空品感測器相比擬的方法，以促進系統整合與更近一步的資料應用。\n套件安裝與引用 在本章節中，我們將會使用到 pandas, numpy, datetime, sklearn, scipy, 及 joblib 等套件，這些套件在我們使用的開發平台 Colab 上已有預先提供，因此我們不需要另行安裝，可以直接用下列的方法引用，以備之後資料處理與分析使用。\n# 引入資料處理模組 import pandas as pd # 用於資料處理和分析 import numpy as np # 用於數值計算 # 從 datetime 模組中引入日期和時間相關的功能 from datetime import datetime, timedelta # 引入 sklearn 模組中的機器學習工具 from sklearn import linear_model, svm, tree # 線性模型、隨機森林回歸模型、決策樹模型 # 引入模型評估和交叉驗證的工具 from sklearn import metrics as sk_metrics from sklearn.ensemble import RandomForestRegressor # 隨機森林回歸模型 # 模型選擇的工具，如訓練測試資料分割和交叉驗證 from sklearn.model_selection import train_test_split, cross_validate # 引入特徵選擇工具 from sklearn.feature_selection import f_regression # 引入 scipy.stats 用於統計計算 import scipy.stats as stats # 引入 scipy.stats 用於統計計算 import joblib 讀取資料與環境設定 在這個範例中，我們將使用環保署空品測站萬華站 (wanhua)，以及中研院校園空品微型感測器中，兩台與環保署萬華測站放置在同一地點的空氣盒子 (機器代號分別為 08BEAC028A52 和 08BEAC028690) 作為案例，並且使用線性迴歸 (Linear Regression)、隨機森林迴歸 (Random Foreset Regression) 以及支持向量迴歸 (Support Vector Regression, SVR) 三個訓練模型，同時考量感測資料中的 PM2.5 濃度、溫度、相對濕度和時間戳記（小時值）共計四個欄位的資料，搭配三個不同時間長度的歷史資料（3 天、5 天、8 天），進行一系列的探究。\n為了方便起見，我們先依據這些想定的內容，進行下列的程式初始設定。\n# 設定站點名稱為「wanhua」 SITE= \"wanhua\" # 環保署萬華測站在輸入資料中的代碼 EPA= \"EPA-Wanhua\" # 兩台與環保署萬華測站放置在相同位置的空氣盒子機器代號 AIRBOXS= ['08BEAC028A52', '08BEAC028690'] # 不同歷史資料的長度設定 DAYS= [8, 5, 3] # 本範例所使用的迴歸方法 METHODS= ['LinearRegression', 'RandomForestRegressor', 'SVR'] # 本範例所使用的迴歸方法的簡寫 METHOD_SW= { 'LinearRegression':'LinearR', 'RandomForestRegressor':'RFR', 'SVR':'SVR' } # 本範例所使用的迴歸方法的函數名稱 METHOD_FUNTION= {'LinearRegression':linear_model.LinearRegression(), 'RandomForestRegressor': RandomForestRegressor(n_estimators= 300, random_state= 36), 'SVR': svm.SVR(C=20) } # 本範例中所使用的各項欄位資料對照 FIELD_SW= {'s_d0':'PM25', 'pm25':'PM25', 'PM2_5':\"PM25\", 'pm2_5':\"PM25\", 's_h0':\"HUM\", 's_t0':'TEM'} # 本範例中所探究的各種不同欄位組合方式 FEATURES_METHOD= {'PHTR':[\"PM25\", \"HR\", \"TEM\", \"HUM\"], 'PH':['PM25','HR'], 'PT':['PM25','TEM'], 'PR':['PM25', 'HUM'], 'P':['PM25'], 'PHR':[\"PM25\", \"HR\", \"HUM\"], 'PTR':[\"PM25\", \"TEM\", \"HUM\"], 'PHT':[\"PM25\", \"HR\", \"TEM\"] } 接下來我們考慮需要下載多少資料供系統校正與資料融合使用。如下圖所示，假設我們所要取得的是第 N 天的校正模型，那麽第 N - 1 天的資料便會做為測試資料使用，以評估未來校正模型的準確度；因此，倘若我們所設定的訓練資料為 X 天，那就代表第 N - 2 到 N - (2+X) 天的歷史資料將被作為訓練資料使用。在我們的想定中，我們將 N 設定為目前時間（今天），而最大的可能 X 值為 8，因此我們將需要預先準備共計十天的歷史資料以供接下來的操作使用。\n為了方面日後的使用，我們先用下面的程式碼，明定校正模型的日期 (第 N 天: TODAY)、測試資料的日期 (第 N-1 天: TESTDATE)、以及訓練資料的結束日期 (第 N-2 天: ENDDATE)。\n# 取得當前的日期和時間 TODAY = datetime.today() # 設定測試日期為昨天 TESTDATE = (TODAY - timedelta(days=1)).date() # 設定結束日期為前天 ENDDATE = (TODAY - timedelta(days=2)).date() 我們使用中研院校園空品微型感測器所提供的資料下載網址，根據指定的日期 與機器代碼 ，可以使用 CSV 檔案格式下載對應日期與機器的感測資料。注意，由於下載平台的限制，下載時所指定的日期 只能是下載當天算起，過去 30 天內的日期。\nhttps://pm25.lass-net.org/data/history-date.php?device_id=\u003cID\u003e\u0026date=\u003cYYY-MM-DD\u003e\u0026format=CSV 例如，假設我們要下載環保署萬華測站在 2022 年 9 月 21 日的感測資料，我們可以用下列的方式進行下載：\nhttps://pm25.lass-net.org/data/history-date.php?device_id=EPA-Wanhua\u0026date=2022-09-21\u0026format=CSV 利用這個資料下載的方式，我們撰寫一個 Python 的小函式 getDF，可以針對輸入的機器代碼，下載過去 10 天的感測資料，並將資料彙整成單一一個 DataFrame 物件後回傳。\n# 定義一個函式來取得指定裝置 id 的資料 def getDF(id): temp_list = [] # 創建一個空的串列，稍後用來儲存各天的資料 # 迭代 10 次，分別取得前 10 天的資料 for i in range(1,11): # 計算要取得資料的日期 date = (TODAY - timedelta(days=i)).strftime(\"%Y-%m-%d\") # 建立資料的 URL URL = \"https://pm25.lass-net.org/data/history-date.php?device_id=\" + id + \"\u0026date=\" + date + \"\u0026format=CSV\" # 從 URL 取得資料，並將其加入到串列中 temp_DF = pd.read_csv( URL, index_col=0 ) temp_list.append( temp_DF ) # 輸出當前取得資料的裝置 id、日期和資料的形狀 print(\"ID: {id}, Date: {date}, Shape: {shape}\".format(id=id, date=date, shape=temp_DF.shape)) # 合併所有的資料成一個資料框 All_DF = pd.concat( temp_list ) return All_DF # 回傳整合後的資料框 接著，我們可以下載安裝在環保署萬華測站的第一台空氣盒子感測資料，並存放在 AirBox1_DF 物件中：\n# 使用前面定義的 getDF 函式，取得第一台空氣盒子的資料 AirBox1_DF = getDF(AIRBOXS[0]) # 顯示取得資料的前五行 AirBox1_DF.head() ID: 08BEAC028A52, Date: 2022-09-29, Shape: (208, 19) ID: 08BEAC028A52, Date: 2022-09-28, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-27, Shape: (225, 19) ID: 08BEAC028A52, Date: 2022-09-26, Shape: (230, 19) ID: 08BEAC028A52, Date: 2022-09-25, Shape: (231, 19) ID: 08BEAC028A52, Date: 2022-09-24, Shape: (232, 19) ID: 08BEAC028A52, Date: 2022-09-23, Shape: (223, 19) ID: 08BEAC028A52, Date: 2022-09-22, Shape: (220, 19) ID: 08BEAC028A52, Date: 2022-09-21, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-20, Shape: (215, 19) 利用同樣的方法，我們依次下載在環保署萬華測站的第二台空氣盒子感測資料，以及環保署萬華測站的感測資料，並分別存放在 AirBox2_DF 和 EPA_DF 物件中。\n# 使用前面定義的 getDF 函式，取得第二台空氣盒子的資料 AirBox2_DF = getDF(AIRBOXS[1]) # 使用前面定義的 getDF 函式，取得環保署萬華測站的資料 EPA_DF = getDF(EPA) 資料前處理 由於我們目前所下載的資料中，含有許多不需要用到的欄位，為了避免佔用太多的記憶體空間，我們先精簡所使用的資料，只留下需要的內容。\n# 定義我們需要的欄位名稱列表 Col_need = [\"timestamp\", \"s_d0\", \"s_t0\", \"s_h0\"] # 從 AirBox1_DF 挑選我們需要的欄位 AirBox1_DF_need = AirBox1_DF[Col_need] print(AirBox1_DF_need.head()) # 從 AirBox2_DF 挑選我們需要的欄位 AirBox2_DF_need = AirBox2_DF[Col_need] print(AirBox2_DF_need.head()) # 重新定義需要從 EPA_DF 取得的欄位名稱 Col_need = [\"time\", \"date\", \"pm2_5\"] # 從 EPA_DF 挑選我們需要的欄位 EPA_DF_need = EPA_DF[Col_need] print(EPA_DF_need.head()) # 釋放不再使用的資料框記憶體空間 del AirBox1_DF del AirBox2_DF del EPA_DF timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:03:28Z 9.0 29.75 71.0 1 2022-09-30T00:33:46Z 11.0 31.36 67.0 2 2022-09-30T00:39:51Z 10.0 31.50 67.0 3 2022-09-30T00:45:58Z 12.0 31.50 66.0 4 2022-09-30T00:52:05Z 12.0 31.86 66.0 timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:00:31Z 9.0 29.36 -53.0 1 2022-09-30T00:07:17Z 9.0 29.50 -52.0 2 2022-09-30T00:23:47Z 10.0 30.25 -45.0 3 2022-09-30T00:34:24Z 10.0 31.11 -36.0 4 2022-09-30T00:40:31Z 11.0 31.25 -35.0 time date pm2_5 index 0 00:00:00 2022-09-30 9.0 1 01:00:00 2022-09-30 10.0 2 02:00:00 2022-09-30 16.0 3 03:00:00 2022-09-30 19.0 4 04:00:00 2022-09-30 20.0 接著為了統一資料欄位，我們將環保署測站原有的 date 與 time 欄位進行整併，並產生一個新的 timestamp 欄位。\n# 將EPA_DF_need中的「date」和「time」欄位結合，並轉換成時間戳記格式，然後儲存至新的 「timestamp」欄位 EPA_DF_need['timestamp'] = pd.to_datetime( EPA_DF_need[\"date\"] + \"T\" + EPA_DF_need[\"time\"], utc=True ) # 顯示 EPA_DF_need 的前五行以確認結構 print(EPA_DF_need.head()) time date pm2_5 timestamp index 0 00:00:00 2022-09-30 9.0 2022-09-30 00:00:00+00:00 1 01:00:00 2022-09-30 10.0 2022-09-30 01:00:00+00:00 2 02:00:00 2022-09-30 16.0 2022-09-30 02:00:00+00:00 3 03:00:00 2022-09-30 19.0 2022-09-30 03:00:00+00:00 4 04:00:00 2022-09-30 20.0 2022-09-30 04:00:00+00:00 由於空氣盒子與環保署測站的資料時間解析度不同，為了能拉齊兩邊的資料，我們將空氣盒子的資料由原有每五分鐘一筆資料，用每小時取平均值的方式，改為每小時一筆資料。\n# 定義一個函式來進行小時平均計算 def getHourly(DF): # 將資料的索引設為「timestamp」 DF = DF.set_index( pd.DatetimeIndex(DF[\"timestamp\"]) ) # 進行小時資料重新取樣並計算平均值 DF_Hourly = DF.resample('H').mean() # 重設索引 DF_Hourly.reset_index(inplace=True) return DF_Hourly # 使用 getHourly 函式來計算兩個 AirBox 裝置的小時平均 AirBox1_DF_need_Hourly = getHourly( AirBox1_DF_need) AirBox2_DF_need_Hourly = getHourly( AirBox2_DF_need) # 環保署的數據原始就是小時平均，但我們還是使用 getHourly 函式確保結構一致性 EPA_DF_need_Hourly = getHourly( EPA_DF_need) # 釋放原始資料的記憶體空間 del AirBox1_DF_need del AirBox2_DF_need del EPA_DF_need # 顯示計算後的小時平均資料，確認轉換是否正確 print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp s_d0 s_t0 s_h0 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp pm2_5 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 為了方面接下來的運算與識別，我們先把兩個來源的資料欄位，換成容易識別的欄位名稱。\n# 定義欄位名稱的轉換字典: s_d0 轉換為 PM25；s_h0 轉換為相對濕度 (HUM)；s_t0 轉換為溫度 (TEM) Col_rename = {\"s_d0\":\"PM25\", \"s_h0\":\"HUM\", \"s_t0\":\"TEM\"} # 使用定義好的字典來轉換 AirBox 裝置 1 和 2 的欄位名稱 AirBox1_DF_need_Hourly.rename(columns=Col_rename, inplace=True) AirBox2_DF_need_Hourly.rename(columns=Col_rename, inplace=True) # 定義另一個欄位名稱的轉換字典：pm2_5 轉換為 EPA_PM25 Col_rename = {\"pm2_5\":\"EPA_PM25\"} # 使用定義好的字典來轉換環保署的欄位名稱 EPA_DF_need_Hourly.rename(columns=Col_rename, inplace=True) # 顯示轉換後的資料，確認欄位名稱是否已正確轉換 print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp PM25 TEM HUM 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp EPA_PM25 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 由於兩台空氣盒子的硬體相同且擺放位置相同，可以視為同一個資料來源，因此我們將兩台空氣盒子的資料進行合併，產生 AirBoxs_DF 物件，並且將這個新的物件與環保署空品測站的資料用「交集」的方式，以時間欄位為基準進行交集合併，並產生 All_DF 物件。\n# 將兩台 AirBox 的資料進行合併 AirBoxs_DF = pd.concat([AirBox1_DF_need_Hourly, AirBox2_DF_need_Hourly]).reset_index(drop=True) # 根據 timestamp（時間欄位），使用只保留交集部分的方法 (inner)，將 EPA 資料和 Airbox 資料進行合併 All_DF = pd.merge( AirBoxs_DF, EPA_DF_need_Hourly, on=[\"timestamp\"], how=\"inner\" ) # 顯示合併後的資料的前 10 筆，確認資料是否合併正確 print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 NaN 5 2022-09-21 02:00:00+00:00 9.000000 30.418889 -59.222222 NaN 6 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 7 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 8 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 9 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 我們先將資料中存在空值 (NaN) 的狀況予以排除。\n# 移除 All_DF 中包含 NaN 值的所有資料列 All_DF.dropna(how=\"any\", inplace=True) # 重新設定資料框的索引，並移除原始的索引 All_DF.reset_index(inplace=True, drop=True) # 顯示資料框的前 10 筆資料，確認資料處理後的樣貌 print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 最後，由於在進行模型建構時，會使用到每日小時值的資料，因此我們替 All_DF 增加一個 HR 欄位，內容為 timestamp 中的小時值。\n# 定義一個函數，用來從 timestamp 欄位中提取小時數並新增到新的欄位「HR」 def return_HR(row): row['HR'] = int(row[ \"timestamp\" ].hour) return row # 使用 apply 函數將上述函數應用到整個資料框，並於每一行新增「HR」欄位 All_DF = All_DF.apply(return_HR , axis=1) # 顯示資料框的前 10 筆資料，確認「HR」欄位已被成功新增 print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 HR 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 1 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 1 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 3 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 3 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 4 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 4 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 5 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 5 校正模型訓練與驗證 完成資料的整備後，接下來我們開始建立候選的校正模型。由於我們在這個案例中，總共討論 3 種迴歸方式、3 種歷史資料長度與 8 種特徵組合，因此我們總共將產出 3 x 3 x 8 = 72 個候選模型。\n首先，針對在產製校正模型時須參考特定長度的歷史資料，我們設計 SlideDay 函式，根據所輸入的歷史資料長度 day，回傳輸入資料 Hourly_DF 中從 enddate 開始往前共計 day 天的資料。\n# 定義一個函數，目的是從一給定的資料框中，提取介於特定起始日和結束日之間的資料 def SlideDay( Hourly_DF, day, enddate ): # 計算起始日期，這是根據提供的結束日期減去指定的天數得出的 startdate= enddate- timedelta( days= (day-1) ) # 使用 boolean 遮罩 (mask) 來選出介於起始日期和結束日期之間的資料 time_mask= Hourly_DF[\"timestamp\"].between( pd.Timestamp(startdate, tz='utc'), pd.Timestamp(enddate, tz='utc') ) # 返回選出的資料 return Hourly_DF[ time_mask ] 接著，我們將 site 測站自 enddate 起往前 day 天的感測資料 Training_DF，根據 feature 特徵組合整理成為訓練資料。然後我們套用 method 迴歸方法，讓所產製的校正模型所產生的預測值可以逼近訓練資料中的 EPA_PM25 欄位的資料值。我們同時計算校正模型本身的平均絕對誤差 (Mean Absolute Error, MAE) 和均方誤差 (Mean Squared Error, MSE)。其中，平均絕對誤差 (MAE) 為目標值和預測值之差的絕對值之和，可以能反映預測值誤差的實際情況，數值越小代表成效越好。均方誤差 (MSE) 則為預測值和實際觀測值間差的平方的均值，MSE 的值越小，說明預測模型描述實驗資料具有更好的精確度。\n# 定義一個函數用來建立回歸模型 def BuildModel( site, enddate, feature, day, method, Training_DF ): # 從提供的資料框中提取相對應的特徵資料 X_train = Training_DF[ FEATURES_METHOD[ feature ] ] # 提取預測目標欄位，即 EPA 的 PM2.5 數值 Y_train = Training_DF[ \"EPA_PM25\" ] # 初始化一個字典來儲存模型的相關結果 model_result = {} # 設定相對應的資訊到字典中 model_result[\"site\"], model_result[\"day\"], model_result[\"feature\"], model_result[\"method\"] = site, day, feature, method model_result[\"datapoints\"], model_result[\"modelname\"] = X_train.shape[0], (site + \"_\" + str(day) + \"_\" + METHOD_SW[method] + \"_\" + feature) model_result[\"date\"] = enddate.strftime( \"%Y-%m-%d\" ) # 加入建立時間欄位 Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) model_result['create_timestamp_utc'] = Now_Time ### 訓練模型 ### print( \"[BuildR]-\\\"{method}\\\" with {day}/{feature}\".format(method=method, day=day, feature=feature) ) # 使用指定的迴歸方法進行訓練 lm = METHOD_FUNTION[ method ] lm.fit( X_train, Y_train ) # 計算模型的預測結果並計算其性能指標 Y_pred = lm.predict( X_train ) model_result['Train_MSE'] = MSE = sk_metrics.mean_squared_error( Y_train, Y_pred ) model_result['Train_MAE'] = sk_metrics.mean_absolute_error( Y_train, Y_pred ) # 返回模型的結果和模型實例 return model_result, lm 除了針對訓練資料在建立校正模型時的 MAE 和 MSE 評估外，我們也同時考慮所產製的校正模型在非訓練資料上的預測成果表現。因此，我們針對所產製的模型 lm，根據其所需要帶入的特徵資料 feature，帶入測試資料產出預測值，並與測試資料中的 EPA_PM25 欄位進行比對，計算 MAE 和 MSE，以作為後續評估不同校正模型適用性的參考。\n# 定義一個函數用來測試訓練好的回歸模型 def TestModel( site, feature, modelname, Testing_DF, lm ): # 從提供的資料框中提取相對應的特徵資料 X_test = Testing_DF[ FEATURES_METHOD[ feature ] ] # 提取預測目標欄位，即 EPA 的 PM2.5 數值 Y_test = Testing_DF[ \"EPA_PM25\" ] # 加入測試時間欄位 Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) # 使用模型預測測試資料 Y_pred = lm.predict( X_test ) # 計算模型在測試資料上的性能指標 test_result = {} test_result[\"test_MSE\"] = round( sk_metrics.mean_squared_error( Y_test, Y_pred ), 3) test_result[\"test_MAE\"] = round( sk_metrics.mean_absolute_error( Y_test, Y_pred ), 3) # 返回測試結果 return test_result 最後，我們整合剛剛已經完成的 SlideDay、BuildModel 和 TestModel，逐一完成共計 72 個校正模型的產製，並且分別計算其針對訓練資料與測試資料的 MAE 和 MSE，且將所有結果存入 AllResult_DF 物件中。\n# 建立一個空的結果列表 AllResult_list = [] # 遍歷不同的時間範圍設定 for day in DAYS: # 遍歷不同的迴歸方法 for method in METHODS: # 遍歷不同的特徵組合 for feature in FEATURES_METHOD: # 從全資料中取得訓練用的資料 Training_DF = SlideDay(All_DF, day, ENDDATE)[ FEATURES_METHOD[feature] + [\"EPA_PM25\"] ] # 建立模型並取得訓練結果 result, lm = BuildModel( SITE, TESTDATE, feature, day, method, Training_DF ) # 使用模型進行測試並取得測試結果 test_result = TestModel(SITE, feature, result[\"modelname\"], SlideDay(All_DF, 1, TESTDATE), lm) # 將訓練與測試的結果結合起來 R_DF = pd.DataFrame.from_dict( [{ **result, **test_result }] ) # 將這次的結果加入到結果列表中 AllResult_list.append( R_DF ) # 將所有結果組合成一個資料框 AllResult_DF = pd.concat(AllResult_list) # 顯示頭部的結果 AllResult_DF.head() [BuildR]-\"LinearRegression\" with 8/PHTR [BuildR]-\"LinearRegression\" with 8/PH [BuildR]-\"LinearRegression\" with 8/PT [BuildR]-\"LinearRegression\" with 8/PR [BuildR]-\"LinearRegression\" with 8/P [BuildR]-\"LinearRegression\" with 8/PHR [BuildR]-\"LinearRegression\" with 8/PTR [BuildR]-\"LinearRegression\" with 8/PHT [BuildR]-\"RandomForestRegressor\" with 8/PHTR [BuildR]-\"RandomForestRegressor\" with 8/PH [BuildR]-\"RandomForestRegressor\" with 8/PT [BuildR]-\"RandomForestRegressor\" with 8/PR [BuildR]-\"RandomForestRegressor\" with 8/P [BuildR]-\"RandomForestRegressor\" with 8/PHR [BuildR]-\"RandomForestRegressor\" with 8/PTR [BuildR]-\"RandomForestRegressor\" with 8/PHT [BuildR]-\"SVR\" with 8/PHTR [BuildR]-\"SVR\" with 8/PH [BuildR]-\"SVR\" with 8/PT [BuildR]-\"SVR\" with 8/PR [BuildR]-\"SVR\" with 8/P [BuildR]-\"SVR\" with 8/PHR [BuildR]-\"SVR\" with 8/PTR [BuildR]-\"SVR\" with 8/PHT [BuildR]-\"LinearRegression\" with 5/PHTR [BuildR]-\"LinearRegression\" with 5/PH [BuildR]-\"LinearRegression\" with 5/PT [BuildR]-\"LinearRegression\" with 5/PR [BuildR]-\"LinearRegression\" with 5/P [BuildR]-\"LinearRegression\" with 5/PHR [BuildR]-\"LinearRegression\" with 5/PTR [BuildR]-\"LinearRegression\" with 5/PHT [BuildR]-\"RandomForestRegressor\" with 5/PHTR [BuildR]-\"RandomForestRegressor\" with 5/PH [BuildR]-\"RandomForestRegressor\" with 5/PT [BuildR]-\"RandomForestRegressor\" with 5/PR [BuildR]-\"RandomForestRegressor\" with 5/P [BuildR]-\"RandomForestRegressor\" with 5/PHR [BuildR]-\"RandomForestRegressor\" with 5/PTR [BuildR]-\"RandomForestRegressor\" with 5/PHT [BuildR]-\"SVR\" with 5/PHTR [BuildR]-\"SVR\" with 5/PH [BuildR]-\"SVR\" with 5/PT [BuildR]-\"SVR\" with 5/PR [BuildR]-\"SVR\" with 5/P [BuildR]-\"SVR\" with 5/PHR [BuildR]-\"SVR\" with 5/PTR [BuildR]-\"SVR\" with 5/PHT [BuildR]-\"LinearRegression\" with 3/PHTR [BuildR]-\"LinearRegression\" with 3/PH [BuildR]-\"LinearRegression\" with 3/PT [BuildR]-\"LinearRegression\" with 3/PR [BuildR]-\"LinearRegression\" with 3/P [BuildR]-\"LinearRegression\" with 3/PHR [BuildR]-\"LinearRegression\" with 3/PTR [BuildR]-\"LinearRegression\" with 3/PHT [BuildR]-\"RandomForestRegressor\" with 3/PHTR [BuildR]-\"RandomForestRegressor\" with 3/PH [BuildR]-\"RandomForestRegressor\" with 3/PT [BuildR]-\"RandomForestRegressor\" with 3/PR [BuildR]-\"RandomForestRegressor\" with 3/P [BuildR]-\"RandomForestRegressor\" with 3/PHR [BuildR]-\"RandomForestRegressor\" with 3/PTR [BuildR]-\"RandomForestRegressor\" with 3/PHT [BuildR]-\"SVR\" with 3/PHTR [BuildR]-\"SVR\" with 3/PH [BuildR]-\"SVR\" with 3/PT [BuildR]-\"SVR\" with 3/PR [BuildR]-\"SVR\" with 3/P [BuildR]-\"SVR\" with 3/PHR [BuildR]-\"SVR\" with 3/PTR [BuildR]-\"SVR\" with 3/PHT 輸出本日最佳校正模型 在討論「本日最佳校正模型」時，必需先認知所謂的「最佳模型」必須在某一日結束後，才有可能總結 24 小時的資料作為測試資料，依據前述的方式獲得不同候選模型的 MAE 與 MSE 後，經過比較分析才能得到真正的最佳模型。因此，在本日 24 小時尚未結束前，是不可能有系統地產製真正的最佳校正模型的。\n但是，基於實務上的需要，我們常常必須在資料產製時，就需要能有校正模型可以套用；因此在實務上，我們通常會假設「昨日的最佳模型，在今日也會有不錯的表現」，並以此最為今日系統校正的方法。例如，假設我們在決定最佳模型時，是考慮帶入測試資料時能得到最小 MSE 的候選模型，那麼我們可以用下面的語法獲知該模型的資訊：\n# 指定要找的目標欄位，這裡是「test_MSE」，即測試資料的均方誤差 FIELD= \"test_MSE\" # 從所有結果中找出「test_MSE」最小的那一筆（或多筆），代表模型的效果最好 BEST= AllResult_DF[ AllResult_DF[FIELD]== AllResult_DF[FIELD].min() ] # 顯示最好結果的資料 BEST 接著，為了套入本日的情境，我們以昨日最佳校正模型的參數（歷史資料長度、迴歸方法、特徵資料組合），搭配從本日為基準重新計算日期區間的訓練資料與測試資料，重新產製今日的校正模型。\n# 根據最佳模型的參數，重新訓練模型 # 將最佳結果轉換為字典格式，方便後續存取各種參數 BEST_DC= BEST.to_dict(orient=\"index\")[0] # 根據最佳模型的設定，從所有資料中取出相對應的訓練資料 Training_DF= SlideDay(All_DF, BEST_DC[\"day\"], TESTDATE)[ FEATURES_METHOD[BEST_DC[\"feature\"]]+ [\"EPA_PM25\"] ] # 使用上一步取出的訓練資料，根據最佳模型的參數重新訓練模型 result, lm= BuildModel( SITE, TODAY, BEST_DC[\"feature\"], BEST_DC[\"day\"], BEST_DC[\"method\"], Training_DF ) # 顯示新模型的訓練結果 result [BuildR]-\"SVR\" with 3/PHT {'site': 'wanhua', 'day': 3, 'feature': 'PHT', 'method': 'SVR', 'datapoints': 80, 'modelname': 'wanhua_3_SVR_PHT', 'date': '2022-09-30', 'create_timestamp_utc': '2022-09-30 11:19:48', 'Train_MSE': 3.91517342356589, 'Train_MAE': 1.42125724796098} 從這個範例中，我們可以看到新產製出來的校正模型，其 Train_MSE 為 3.915，比昨日的 2.179 增加不少。然而，由於我們並沒有方法在本日結束前獲知本日真正的最佳模型，在沒有其他更好選擇的情況下，我們只能先以昨日最佳模型進行套用，並以下列語法匯出 .joblib 檔案，提供模型的開放分享使用（詳細使用方法請見參考資料）。\n# 將訓練好的模型儲存起來 # 根據模型結果的名稱，建立模型儲存的檔名 model_dumpname= result[\"modelname\"]+ \".joblib\" # 定義模型儲存的路徑，此處先預設為當前路徑 MODEL_OUTPUT_PATH= \"\" # 嘗試將訓練好的模型儲存到指定的路徑 try: joblib.dump( lm, MODEL_OUTPUT_PATH+ model_dumpname ) # 儲存模型 print( \"[BuildR]-dump {}\".format( MODEL_OUTPUT_PATH+model_dumpname ) ) # 顯示儲存成功的訊息 except Exceptionas e: # 若儲存過程中發生任何錯誤 print( \"ERROR! [dump model] {}\".format( result[\"modelname\"] ) ) # 顯示發生錯誤的模型名稱 error_msg(e) # 顯示錯誤訊息 [BuildR]-dump wanhua_3_SVR_PHT.joblib 校正成果簡介 本章節所介紹的系統校正方法，自 2020/5 起已正式套用在民生公共物聯網中的中研院校園空品微型感測器系統上，並將每日產出的校正模型發布在 Dynamic Calibration Model 網站上。在套用的過程中，共計選定 31 個環保署空品測站分別裝置兩台空氣盒子，並於每日考量三種歷史資料長度、八種資料特徵組合、七種迴歸方法，共計 3 x 8 x 7 = 168 種排列組合，針對這 31 個測站位置分別產製每日的最佳校正模型；接著，針對每一台空氣盒子的感測資料，皆參考其地理位置最接近的測站最佳校正模型，作為其資料校正的套用模型，並產製與發佈各式的資料。從本機制上線後的實際運作觀察，確實有效拉近微型空品感測與環保署空品感測的數據差異（如下圖所示），對於民生公共物聯網在空品領域中的跨系統資料整合，建立良好的合作模式。\n參考資料 Dynamic Calibration Model Status Report (https://pm25.lass-net.org/DCF/) scikit-learn: machine learning in Python (https://scikit-learn.org/stable/) Joblib: running Python functions as pipeline jobs (https://joblib.readthedocs.io/) Jason Brownlee, Save and Load Machine Learning Models in Python with scikit-learn, Machine Learning Mastery (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/) ",
    "description": "我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "6.3. 感測器聯合校正",
    "uri": "/ch6/ch6.3/"
  },
  {
    "content": "\nTable Of Contents 套件安裝與引入 讀取資料 Leafmap 基本操作 資料基本呈現方法 資料叢集呈現方法 更改 Leafmap 底圖 整合 OSM 資源 Heatmap 應用 分割視窗功能 Leafmap 成果網頁化 小結 參考資料 我們在之前的章節中，已經示範了如何使用程式語言針對地理屬性的資料進行分析，同時也示範了如何使用 GIS 軟體進行簡易的地理資料分析與呈現。接下來我們介紹如何使用 Python 語言中的 Leafmap 套件進行 GIS 應用，以及 Streamlit 套件進行網站開發。最後，我們將結合 Leafmap 和 Streamlit，自行製作簡單的網頁 GIS 系統，並將資料處理與分析後的結果，透過網頁方式來呈現。\n套件安裝與引入 在本章節中，我們將會使用到 pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder 及 pyCIOT 等套件，這些套件除了 pandas 外，在我們使用的開發平台 Colab 上皆沒有預先提供，因此我們需要先自行安裝。由於本次安裝的套件數量較多，為了避免指令在執行後產生大量的文字輸出訊息，因此我們在安裝的指令中，增加了 ‘-q’ 的參數，可以讓畫面的輸出更精簡。\n# 安裝 geopandas 用於地理空間資料處理 !pip install -q geopandas # 安裝 leafmap 用於地圖視覺化 !pip install -q leafmap # 安裝 ipyleaflet 也是用於地圖視覺化，但有更多互動功能 !pip install -q ipyleaflet # 安裝 osmnx 用於從 OpenStreetMap 下載、建模、分析街道網絡 !pip install -q osmnx # 安裝 streamlit 用於快速建立 web 應用 !pip install -q streamlit # 安裝 geocoder 用於地理編碼和逆地理編碼 !pip install -q geocoder # 安裝 pyCIOT !pip install -q pyCIOT 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\n# 引入 pandas 用於資料處理和分析 import pandas as pd # 引入 geopandas 用於地理空間資料處理 import geopandas as gpd # 引入 leafmap 用於地圖視覺化 import leafmap # 引入 ipyleaflet 也是用於地圖視覺化，但有更多互動功能 import ipyleaflet # 引入 osmnx 用於從 OpenStreetMap 下載、建模、分析街道網絡 import osmnx # 引入 geocoder 用於地理編碼和逆地理編碼 import geocoder # 引入 streamlit 用於快速建立 web 應用 import streamlit # 引入 pyCIOT from pyCIOT.data import * 讀取資料 在本章的範例中，我們使用民生公共物聯網資料平台上的環保署空品測站資料，以及中央氣象局與國震中心地震監測站資料。\n在環保署空品測站的部分，我們使用 pyCIOT 套件擷取所有環保署空品測站的最新一筆量測結果，並且透過 pandas 套件的 json_normalize() 方法，將所獲得的 JSON 格式資料，轉換成 DataFrame 格式，並且只留下其中測站名稱、緯度、經度與臭氧 (O3) 濃度資訊，以待後面操作時使用。這部分資料擷取與處理的程式碼如下：\n# 引入 pyCIOT 中的 Air 類別來取得環保署（EPA）觀測站的數據 epa_station = Air().get_data(src=\"OBS:EPA\") # 將 JSON 格式的數據轉換為 DataFrame df_air = pd.json_normalize(epa_station) # 初始化一個新的欄位 'O3'（臭氧），並設定其初始值為 0 df_air['O3'] = 0 # 迴圈歷遍 DataFrame 的每一行，提取臭氧（O3）數據 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] # 篩選我們需要的欄位：站點名稱、緯度、經度和 O3 數據 df_air = df_air[['name','location.latitude','location.longitude','O3']] 接著我們用類似的方法，擷取中央氣象局與國震中心地震監測站的測站資料，並且只留下其中測站名稱、緯度與經度資訊，以待後面操作時使用。這部分資料擷取與處理的程式碼如下：\n# 引入 pyCIOT 中的 Quake 類別來取得地震觀測站的數據，來源為中央氣象局和國家地震工程研究中心 quake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") # 將 JSON 格式的數據轉換為 DataFrame df_quake = pd.json_normalize(quake_station) # 篩選我們需要的欄位：站點名稱、緯度和經度 df_quake = df_quake[['name','location.latitude','location.longitude']] # 顯示篩選結果 df_quake 以上我們已經成功示範空品資料 (air) 和地震資料 (quake) 的讀取範例，在接下來的探討中，我們將利用這些資料進行 leafmap 套件的操作與應用，相同的方法也可以輕易改成使用其他民生公共物聯網資料平台上的其他資料而得到類似的結果，大家可以自行嘗試看看。\nLeafmap 基本操作 資料基本呈現方法 根據我們目前已處理好的空品測站資料 df_air 與地震測站資料 df_quake，我們首先這兩份資料的格式，從原本 pandas 套件所提供的 DataFrame 格式，轉成支援地理資訊屬性的 geopandas 套件所提供的 GeoDataFrame 格式；接著我們利用 leafmap 的 add_gdf() 方法，將兩份資料分為兩個圖層一次加入地圖。\n# 將空氣品質站和地震觀測站的 DataFrame 轉換為 GeoDataFrame gdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') # 初始化一個 leafmap 地圖，設定中心點和控制項 m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 在地圖上添加空氣品質站和地震觀測站的 GeoDataFrame m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") # 顯示結果 m1 從程式輸出的地圖中，我們可以在地圖的右上角看到兩份資料的內容，已經依照兩個圖層的方式加入地圖，使用者可以依照自己的需求點選所要查詢的圖層進行瀏覽。然而，當我們想同時瀏覽兩個圖層的資料時，會發現兩個圖層使用相同的圖示進行呈現，因此在地圖上將產生混淆。\n為了解決這個問題，我們介紹另外一種資料呈現的方式，使用 ipyleaflet 套件所提供的 GeoData 圖層資料格式，並使用 leafmap 的 add_layer() 方法將 GeoData 圖層加入地圖。為了方便辨認，我們使用藍色的小圓形圖案表示空品測站的資料，並使用紅色的小圓形圖案表示地震測站的資料。\n# 使用 ipyleaflet.GeoData 來定義地理資料的顯示方式（例如：點的樣式）對於空氣品質站和地震觀測站 geo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) # 初始化另一個 leafmap 地圖，設定中心點和控制項 m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 在地圖上添加空氣品質站和地震觀測站的圖層 m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) # 顯示結果 m2 資料叢集呈現方法 在某些資料應用場合，當地圖上的資料點位數量太多時，反而不容易進行觀察，這時我們可以使用叢集 (cluster) 的方式來呈現資料，也就是當小範圍內資料點位數量太多時，會將這些點位聚集在一起，並且顯示點位的數量；當使用者拉近 (zoom in) 地圖時，隨著地圖比例尺的修改，這些原本叢集的點位會被慢慢抽離，當小範圍內只剩下一個點位時，便可以直接看到點位的資訊。\n我們使用地震測站的資料進行示範，使用 leafmap 的 add_points_from_xy() 方法，便能將 df2 的資料以叢集的方式放上地圖。\n# 初始化一個新的 leafmap 地圖，設定中心點和其他控制項 m3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 使用 add_points_from_xy 方法直接從 DataFrame 中添加地震觀測站的座標點 # 這個方法會自動把座標點轉換成地圖上的點並添加到圖層 m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") # 顯示結果 m3 更改 Leafmap 底圖 Leafmap 使用 OpenStreetMap 的圖層做為預設的地圖底圖，但是也提供了超過百種的其他底圖選項，使用者可以依照自己的喜好與需求進行更改，並且可以使用下列語法獲知目前 leafmap 所支援的底圖：\n# 獲取 leafmap 中所有基礎地圖的名稱，存儲到變數 'layers' 中 layers = list(leafmap.basemaps.keys()) layers 我們從這些底圖中挑選 SATELLITE 和 Stamen.Terrain 作為示範，使用 leafmap 套件的 add_basemap() 方法將底圖加入成為新的圖層，加入後 leafmap 預設會開啟所有圖層，並按照加入順序進行疊加，但仍然可以透過右上角的圖層選單，點選自己所要使用的圖層。\n# 創建一個新的 leafmap 地圖，中心點設在 (23.8, 121)，並關閉工具欄和開啟圖層控制 m4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 將環保署的空氣監測站資料（GeoDataFrame）加到地圖上，命名為 \"EPA Station\" m4.add_gdf(gdf_air, layer_name=\"EPA Station\") # 在地圖上加入衛星影像圖層 m4.add_basemap(\"SATELLITE\") # 在地圖上加入 Stamen.Terrain 圖層 m4.add_basemap(\"Stamen.Terrain\") # 顯示結果 m4 除了使用 leafmap 所提供的底圖外，也可以使用 Google Map 的 XYZ Tiles 服務，加入 Google 衛星影像的圖層，其方法如下：\n# 加入 Google 的衛星圖層到地圖上 m4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) # 顯示結果 m4 整合 OSM 資源 Leafmap 除了內建的多項資源外，也整合了許多外部的地理資訊資源，其中 OSM (OpenStreetMap) 便是一個著名且內容豐富的開源地理資訊資源，有關 OSM 所提供的各式資源，可以在 OSM 網站中查詢完整的屬性列表。\n在下列的範例中，我們透過 leafmap 套件提供的 add_osm_from_geocode() 方法，示範如何獲取城市的邊界輪廓，作為地圖呈現使用。我們以台中市為例，搭配之前使用的環保署空品測站點位資料，可以清楚看到哪些測站位於台中市內。\n# 定義一個變數用於儲存城市名稱，這裡選擇的是台中 city_name = \"Taichung, Taiwan\" # 建立一個新的 leafmap 地圖實例，並設置中心點和控制項 # 添加先前定義的空氣品質監測站資料 # 利用地名來從 OpenStreetMap 中獲取地圖資料，並添加到地圖中 m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) # 顯示結果 m5 我們接著繼續使用 leafmap 套件提供的 add_osm_from_place() 方法，針對台中市境內進一步尋找特定的設施，並加入地圖圖層。下方的程式以工廠設施為例，透過 OSM 的土地利用資料，找出台中市內的相關工廠地點與區域，可以與空品測站位置搭配進行分析判讀使用。有關 OSM 更多的設施種類，可以參考完整的屬性列表。\n# 使用 add_osm_from_place 方法添加特定於「工業用地」的圖層 # 這裡選擇的是台中市的工業用地 m5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") # 顯示結果 m5 此外，leafmap 套件也提供以特定地點為中心，搜尋 OSM 鄰近設施的方法，對於分析與判讀資料，提供十分便利的功能。例如，在以下的範例中，我們使用 add_osm_from_address() 方法，搜尋台中清水車站 (Qingshui Station, Taichung) 方圓 1,000 公尺內的相關宗教設施 (屬性為 “amenity”: “place_of_worship”)；同時，我們使用 add_osm_from_point() 方法，搜尋台中清水車站 GPS 座標 (24.26365, 120.56917) 方圓 1,000 公尺內的相關學校設施 (屬性為 “amenity”: “school”)。最後，我們將這兩項查詢的結果，分別用不同的圖層疊加到既有的地圖上。\n# 使用 add_osm_from_address 方法添加以「台中清水車站」為中心的「宗教設施」圖層 # 這裡設置的搜索半徑是 1000 公尺 m5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) # 使用 add_osm_from_point 方法添加以給定座標（24.26365, 120.56917）為中心的「學校」圖層 # 這裡設置的搜索半徑是 1000 公尺 m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) # 顯示結果 m5 Heatmap 應用 熱力圖是一種以顏色變化來顯示事件強度的一種二維空間表示法，將熱力圖與地圖搭配時，可以根據所使用的地圖比例不同，表達不同尺度下的事件強度狀態，是資料視覺化表示方法中一個非常普遍使用與功能強大的工具。然而，在繪製熱力圖時，使用者必須確認資料本身的特性適合使用熱力圖來呈現，否則極易與我們在單元五所介紹的 IDW 和 Kriging 等圖形化資料內插表示法產生混淆。例如，我們以上面空品測站資料的 O3 濃度資料為例，繪製對應的熱力圖如下：\n# 初始化一個新的地圖，中心點設在 (23.8, 121)，並將其命名為 m6 m6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 在地圖 m6 上添加 EPA 空氣站的圖層 m6.add_layer(geo_data_air) # 使用 add_heatmap 方法，在地圖上添加一個熱力圖層來顯示臭氧（O3）的濃度 # 這裡用 df_air 的 'location.latitude' 和 'location.longitude' 列作為座標， # 用 臭氧'O3' 列作為熱力圖的值。設置半徑為 100 m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) # 顯示結果 m6 這張圖乍看之下並無明顯的問題，但如果我們將地圖拉近，放大台中市的區域後，便會發現熱力圖的樣貌發生極大的變化，在不同的尺度下呈現完全不同的結果。\n上述的範例其實便是一個熱力圖被誤用的案例，因為 O3 濃度的資料反映的是當地的 O3 濃度，其數值並不能因為地圖比例更改而隨著測站叢集後直接進行累加，同時也不能因為地圖比例更改而將累加的數值均勻分散給地圖上的鄰近區域；因此，範例中所使用的 O3 濃度資料，並不適用於熱力圖來表示，而應使用第五章所介紹的地理內插方法來繪製圖形。\n為了呈現熱力圖的真實效果，我們改用地震測站的點位資料，並且加入一個新的欄位 num，且預設值設為 10，接著我們用下列的程式碼產製台灣地震測站狀態的熱力圖。\n# 為地震站數據（df_quake）新增一列 'num'，並設置其值為 10 df_quake['num'] = 10 # 初始化一個新的地圖，中心點設在 (23.8, 121)，並將其命名為 m7 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 在地圖 m7 上添加地震站的圖層 m7.add_layer(geo_data_quake) # 使用 add_heatmap 方法，在地圖上添加一個熱力圖層來顯示地震站的數量 # 這裡用 df_quake 的 'location.latitude' 和 'location.longitude' 列作為座標， # 用 'num' 列作為熱力圖的值。設置半徑為 200 m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) # 顯示結果 m7 分割視窗功能 在資料分析判讀的過程中，有時常需要切換不同的底圖以獲取不同的地理資訊，leafmap 套件因此提供 split_map() 的方法，可以將原本的地圖輸出分為所有兩個子畫面，並且各自套用不同的底圖，以方便地圖資訊的閱讀。其範例程式碼如下：\n# 初始化一個新的地圖，中心點設在 (23.8, 121)，並將其命名為 m8 m8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) # 在地圖 m8 上添加環保局（EPA）的觀測站圖層 m8.add_gdf(gdf_air, layer_name=\"EPA Station\") # 使用 split_map 方法，將地圖拆分為兩個面板。 # 左側面板使用 \"SATELLITE\" 圖層，右側面板使用 \"Stamen.Terrain\" 圖層。 m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) # 顯示結果 m8 Leafmap 成果網頁化 Leafmap 套件的功能十分強大，為了能將處理好的地圖資訊快速分享出去，也特別提供了整合 Streamlit 套件的方式，可以將 Leafmap 的地理資訊 GIS 技術專長與 Streamlit 的 Web 技術專長結合，快速打造 Web GIS 系統。以下我們用一個簡單的範例進行示範，大家可以按照此原則自行擴充，建構自己的 Web GIS 服務。\n在 Streamlit 套件的使用中，建構一個網頁系統分為兩個步驟：\n將所要執行的 Python 程式內容打包成一個 Streamlit 物件，並且將打包過程寫入 app.py 檔案 在系統上執行 app.py 由於我們的操作過程都是使用 Google Colab 平台，在這個平台中我們可以直接將 app.py 用特殊的語法 %%writefile 寫入特殊的暫存區中，接著再由 Colab 直接從暫存區中讀取與執行。因此，針對步驟一的檔案寫入部分，我們可以按照下列的方式進行：\n# 引入所需的模組 import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * # 初始化 Streamlit 應用的標題和內容 st.title('Streamlit Demo') st.write(\"## Leafmap Example\") content = \"\"\" Hello World! \"\"\" st.markdown(content) # 從 EPA 獲取空氣品質數據，並將其轉換為 GeoDataFrame epa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') # 在 Streamlit 應用中創建一個可展開的程式碼區塊 with st.expander(\"See source code\"): # 使用 st.echo 函式來顯示源程式碼 with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") # 將 leafmap 地圖嵌入到 Streamlit 應用中 m.to_streamlit() 針對步驟二的部分，我們則用下列的指令執行：\n# 使用 Streamlit 運行 app.py，並將其放在後台執行 # 使用 npx 來運行 localtunnel，將本地 8501 端口暴露到外網 !streamlit run app.py \u0026 npx localtunnel --port 8501 執行後便會出現類似下方的執行結果：\n接著可以點選其中 “your url is:” 這個字串後面的這個網址，便會在瀏覽器中出現類似下方的內容\n最後，我們點選 “Click to Continue”，便能成功開啟 app.py 中所打包的 Python 程式碼內容，在這個範例中，便能看到 leafmap 套件所呈現的環保署空品測站分佈圖。\n小結 在本章節我們介紹了如何用 Python 語言的 Leafmap 套件進行地理資料的呈現與資源整合，也同時介紹了如何結合 Streamlit 套件的網頁功能，在 Google Colab 平台上建立簡易的網頁化地理資訊系統服務，必須補充說明的是，Leafmap 還有許多更進階的功能，以及與前面其他章節內容的整併與呈現，皆受限於文章篇幅無法在這篇文章中涵蓋到，這些更進一步的探索學習，皆可參考其他章節的內容，或下方的其他參考資料做更多加深加廣的學習。\n參考資料 Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit 超快速又輕鬆建立網頁 Dashboard (https://blog.jiatool.com/posts/streamlit/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "我們介紹 leafmap 套件在民生公共物聯網資料平台中使用不同類型數據進行地理信息表示和空間分析的能力，並演示了 leafmap 和 streamlit 套件的結合共同構建 Web GIS 的應用。透過跨域與跨工具的資源整合，將能拓展讀者對數據分析和信息服務未來的想像。",
    "tags": [
      "Python",
      "空",
      "地"
    ],
    "title": "7.3 Leafmap 應用",
    "uri": "/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/tags/api/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Authors",
    "uri": "/authors/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "intermediate",
    "uri": "/levels/intermediate/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "水",
    "uri": "/tags/%E6%B0%B4/"
  },
  {
    "content": "民生公共物聯網資料應用 「民生公共物聯網」是源於政府對於整合並靠近民生公共服務的努力。為了回應民眾在空氣品質、地震、水資源和災防等領域的迫切需求，於民國 106 年推出的「前瞻基礎建設 - 數位建設」計畫，結合了科技部、交通部、經濟部、內政部、環保署、中央研究院和農委會的力量，建立了這個跨部會的大型計畫。透過大數據、人工智慧和物聯網技術，該計畫旨在建立多項智慧生活服務系統，協助政府和民眾應對環境變遷的挑戰。此外，計畫還特別考慮了不同使用者群的需求，從政府到學界、產業及大眾，以期達到智慧治理、產業及學術發展和提升民眾福祉的目標。\n為了整合並提供由民生公共物聯網系統產出的大量資料，「民生公共物聯網資料服務平台」被特別規劃出來。這個平台不僅確保提供穩定和高品質的感測資料，還致力於降低資訊落差，為民眾提供完整和實時的環境數據。透過平台，民眾可以隨時查詢其周邊環境的最新情報，而此資料亦可作為產業加值和開發的基礎，促進民間的創新活動和優質服務的生成。\n為了進一步鞏固「民生公共物聯網」和其資料平台的發展基礎，並擴大其影響，「民生公共物聯網資料應用專案」確立了三大目標：\n向下扎根：針對大專生和高中生，提供跨領域的自學教材，以促進資訊、地理、地球科學和人文社會的學習； 示範應用：展示現有的應用例子，降低初學者的入門門檻，並鼓勵深入的創新； 橫向擴展：提供與目前資料平台不同的資料存取方式，尤其是針對大數據分析常用的 Python 語言，進行資料存取套件的開發，以吸引更多技術人才參與，進一步豐富該資料平台的使用者群體。 ",
    "description": "",
    "tags": null,
    "title": "民生公共物聯網資料應用",
    "uri": "/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "地",
    "uri": "/tags/%E5%9C%B0/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "吳姃家",
    "uri": "/authors/%E5%90%B3%E5%A7%83%E5%AE%B6/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "沈姿雨",
    "uri": "/authors/%E6%B2%88%E5%A7%BF%E9%9B%A8/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "災",
    "uri": "/tags/%E7%81%BD/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "空",
    "uri": "/tags/%E7%A9%BA/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "洪軾凱",
    "uri": "/authors/%E6%B4%AA%E8%BB%BE%E5%87%B1/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "高慧君",
    "uri": "/authors/%E9%AB%98%E6%85%A7%E5%90%9B/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "陳伶志",
    "uri": "/authors/%E9%99%B3%E4%BC%B6%E5%BF%97/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "陳宏穎",
    "uri": "/authors/%E9%99%B3%E5%AE%8F%E7%A9%8E/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "彭昱齊",
    "uri": "/authors/%E5%BD%AD%E6%98%B1%E9%BD%8A/"
  },
  {
    "content": "評論政策 我們歡迎對我們的文章所發表的評論，也很想聆聽您的意見，但請記住：\n友好、禮貌和相關； 不允許褻瀆、辱罵、人身攻擊或不當內容； 如果您騷擾他人或取笑他人的不幸，您的意見將被刪除； 不允許垃圾郵件文章、洗版或試圖出售任何東西的文章。 這裡的主要規則很簡單：尊重他人如同尊重自己。我們不希望看到發布煽動性內容、破壞性言論或其他不當內容的情事，這都將導致您的言論被禁止。\n我們的管理員將盡力不刪除評論，因為我們相信社群內的每個人都有各自獨立，且往往是強烈的意見和觀點。但是，我們保留出於上述原因以及以下原因刪除評論的權利：\n任何違反隱私的行為； 任何基於種族、宗教、膚色、國籍、殘疾、性取向的貶損和/或以任何可察覺的方式貶損的內容； 任何其他對社群有害的評論。 我們的管理員將負責捕捉不當內容，但我們也希望我們的社群可以一同幫助我們。如果您看到不當評論，您可以點擊留言右上角的旗標向我們舉報，我們將審核評論並決定是否保留或刪除該評論。\n在網站上找不到您的評論？ 有時我們會遇到技術困難，您的評論可能不會出現在我們的網站上。 我們可能沒有批准它，因為它違反了上述規則之一。 我們可能已將其刪除，因為它違反了我們的評論政策。 為什麼我被禁止發表評論？ 雖然我們已經盡最大的努力，讓每個人都可以在我們的社群中發表意見，但有時評論者仍會越界。我們會不時阻止和禁止違反規則的評論者，並且使用各種技術和資料來執行阻止，包括電子郵件、IP 地址和任何其他可用資料。\n這些措施可以防止辱罵性評論者未來在網站上發表評論，即使未來或其他評論不是辱罵性的。根據我們當時使用的軟體，禁止措施也可能導致該評論者之前的其他評論消失。\n",
    "description": "",
    "tags": null,
    "title": "評論政策",
    "uri": "/comment-policy/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "黃仁暐",
    "uri": "/authors/%E9%BB%83%E4%BB%81%E6%9A%90/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "概述",
    "uri": "/tags/%E6%A6%82%E8%BF%B0/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "鄭宇伸",
    "uri": "/authors/%E9%84%AD%E5%AE%87%E4%BC%B8/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "鍾明光",
    "uri": "/authors/%E9%8D%BE%E6%98%8E%E5%85%89/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "羅泉恆",
    "uri": "/authors/%E7%BE%85%E6%B3%89%E6%81%86/"
  }
]
