[
  {
    "content": "1. 教學網站簡介 Table Of Contents 環境感知 開放資料 民生公共物聯網 民生公共物聯網資料服務平台 參考資料 環境感知 人類好奇自然界的各種變化，大概可以追溯自遠古農業文明仰望天象、夜觀星辰的活動，迨至西元十六世紀文藝復興時代，哥白尼依據既有的天文資料推導出「地動說」，隨後歷經伽利略與牛頓的接棒發展，更是近代科學的濫觴。來到現代，由於半導體製程的一日千里，人們用來感知生活環境變化的工具愈來愈多元、精密與微型化，搭上資訊科技日新月異的時代潮流，結合感測器的即時感測，以及網際網路的無遠弗屆資料傳輸，因而產生大量的觀測數據。面對如此汪洋的數據大海，科學家們如何從中找出環境（因子）變化的規律，探究這些規律與災害間的關聯，進而達到預測結果的目標，促進人們生活品質的提升、災害防救的效率，甚至人類與環境之間的友善互動，從而打造美好永續的生活環境，更成為當代無論個人、群體、社會、國家乃是全世界共同關注的主要議題。\n開放資料 「開放資料」使一種將電子化的資料（包含但不限於文字、數據、圖片、影片、聲音等），透過一定的程序與格式，公開允許並鼓勵各界使用。根據 Open Knowledge Foundation 的定義，開放資料必須滿足下列的條件：\n開放授權或開放狀態 (Open license or status)：開放資料的內容必須在公眾領域的範疇，並且使用開放授權對外釋出，不存在任何額外的限制。 自由存取 (Access)：開放資料的內容必須可以從網際網路上自由存取，但在合理的條件規範下，開放資料的存取也可以允許一次性的收費，並且允許有條件的附帶條款。 機器可讀 (Machine readability)：開放資料的內容必須讓電腦可以容易存取、處理與修改。 開放格式 (Open format)：開放資料的內容必須使用開放的格式，可以利用免費、自由或開放的軟體進行存取。 隨著開放原始碼、開放政府、開放科學等開放潮流的盛行，開放資料也逐漸成為政府與科學界在從事各項政策推動與學術研究工作時所奉行的圭臬。而近期盛起的環境感知物聯網，由於其廣佈於公眾領域，且其所觀測的環境資訊與民眾息息相關，更成為眾所期待的開放資料項目之一。\n目前網路上常見的開放資料多使用 JSON、CSV 或 XML 的資料格式：\nJSON 格式的全名為 Javascript Object Notation，是一種輕量級的結構化資料交換標準，其內容由屬性與值組成，易於讓電腦或使用者閱讀與使用，常用於網站上的資料呈現、傳輸與儲存。 CSV 格式的全名為 Comma-Separated Values，顧名思義是一種純文字形式的儲存表格資料，其中每一筆資料為一列，資料中的每一個屬性為一欄，在儲存時所有的屬性必須按照一定的順序排列，並將每個屬性的值以純文字方式呈現，並在不同屬性間插入特定符號作為間隔，常用於應用程式的檔案匯入與匯出、網路資料的傳遞，以及歷史資料的儲存。 XML 格式的全名為 Extensible Markup Language，是一種從標準通用標記語言 (Standard Generalized Markup Language, SGML) 所簡化衍生的一種可延伸標記式語言，允許使用者自行定義所需的標籤 (tags) ，可用來建立包含結構化資訊的文件，常用於文件檔案資料的呈現，以及網路資料的交換。 目前臺灣常用的開放資料，多彙整於下列平台：\n政府資料開放平臺 (https://data.gov.tw/) 氣象資料開放平臺 (https://opendata.cwb.gov.tw/devManual/insrtuction) 環保署環境資料開放平臺 (https://data.epa.gov.tw/) 民生公共物聯網 有鑒於環境感知的重要與相關科技的發展趨勢，政府為整合與貼近民生公共相關服務，擬定民眾四大迫切需求，包括空氣品質、地震、水資源，以及災防等議題，於民國 106 年政府的「前瞻基礎建設 - 數位建設」計畫中，集結國科會、交通部、經濟部、內政部、環保署、中央研究院、農委會，共同建構的政府大型跨部會計畫 - 「民生公共物聯網」，應用大數據、人工智慧、物聯網技術，建置各項智慧生活服務系統，協助政府與民眾共同面對環境變化所帶來的挑戰；同時，此計畫亦考量不同使用者的經驗，包括政府決策單位、學界、產業，以及一般民眾，以提供政府智慧化治理目標，協助產業/學界的發展，提升民眾的幸福感。\n目前民生公共物聯網主要涵蓋的四大領域分別是：\n水資源：民生公共物聯網結合中央與地方政府的水利單位，研發佈建多元化的水資源感測器，並且建置各式水文觀測設施與農田灌溉水情感測器，有效整合地面、地下與新興水源資訊，以及相關監測資訊，建立水資源物聯網入口網與灌溉配水動態分析管理平台，透過大數據收集與雲端分析運算，強化各河川局防汛作業系統，並建立污水下水道雲端管理雲，提供各級相關單位進行遠端、自動化及智慧化管理，以達到聯合運用的目標，促進水源智慧調控管理、灌溉配水動態分析管理、污水下水道雲加值應用、路面淹水示警等多元應用，同時透過資訊公開與資料開放，帶動公私協力的水資源資料應用與決策輔助進展。 空氣品質：民生公共物聯網結合環保署、經濟部、國科會與中央研究院，從空氣品質感測的基礎設施佈建開始，研發空氣品質感測器，建立感測器性能測試驗證中心，並於全台廣佈大量不同目的用途的空氣品質感測器，透過大量更小時間與空間尺度的感測資料收集，建立空品物聯網運算營運平台與智慧環境感測數據中心，並搭建空品感測資料展示平台，一方面促進空氣品質感測物聯網的產業開展，另一方面提供智慧環保稽查的事證資料。藉由智慧化環境監測，提供國人即時且在地的周遭環境空氣品質感測資料，並且利用高解析度感測數據，協助執法人員鑑別污染熱區，有效進行環境稽查與環境治理，並於計畫期間同時強化國內自有技術研發能量，確立國產自主化空品感測技術的發展。 地震：民生公共物聯網針對台灣常見的地震活動，除了大幅增加現地型地震速報主站，以提供高密度且高品質的地震速報資訊外，也同時增設與升級地震與地球物理觀測站，提升包含全球導航衛星系統 (Global Navigation Satellite System, GNSS) 站、井下地震站、強震站、地磁站、地下水站等觀測資料的品質與解析度；此外，針對大屯火山的區域監測，也強化其全球導航衛星系統站、井下地震站及無人機 (Unmanned Aerial Vehicle, UAV) 觀測等項目，並且也針對台灣特有的海島特性，強化強震與海嘯測報效能，擴建海底纜線與相關海底和陸上測站。另外，透過大數據的整合運用，除了建立台灣地震與地球物理資料整合與查詢平台，並且建立整合現地型與區域型地震速報資訊的複合式地震速報平台，可以強化台灣斷層帶及大屯火山區的調查觀測，並且更快速完整地傳遞地震速報資訊，提供民眾強震即時警報，進而促進地震防災產業的開展。 防救災：透過民生公共物聯網的介接整合，將全台包含空、水、地、災與民生各類別共計達58種示警資料集結在單一「民生示警公開資訊平台」，提供民眾即時防救災資訊，並透過應變管理資訊雲端系統 (EMIC2.0)，以及相關的決策圖台，提供防災人員各項災情、通報與救災資源，輔助各項決策工作，同時將相關歷史資料利用統一個資料格式彙整與釋出，提供防災產業分析使用，促進災害情資產業鏈的發展。 民生公共物聯網資料服務平台 此外，為了收納所有民生公共物聯網系統所產出的各式資料，提供穩定、高品質的感測資料作為各項環境治理用途；同時也為了降低環境資訊落差，提供更即時與全面的環境資料數據，使民眾可隨時查詢生活周遭環境的即時感測資訊和時空變化，並做為產業加值應用開發的基礎，讓民間創意能量得以發揮，產出能解決民眾問題之優質服務。在「民生公共物聯網」計畫中也特別規劃「民生公共物聯網資料服務平台」，以開放資料的精神，使用統一的資料格式，提供即時資料介接與歷史資料查詢服務，並且提高使用者瀏覽與搜尋的速度，建立感測資料儲存機制，提供模擬分析或人工智慧之應用。\n目前民生公共物聯網資料服務平台使用 OGC SensorThings API 的開放資料格式，相關的資料格式說明，以及各領域資料內容說明，可參考下列的投影片與影像說明：\n資料服務平台簡介 [PPT] 資料服務平台與SensorThings API簡介 [Video] 水資源領域 [PPT] 水資源物聯網 [Video] 空氣品質領域 [PPT] 環境品質感測物聯網 [Video] [PPT] PM2.5微型感測器布建 [Video] 地震領域 [PPT] 海陸地震聯合觀測 [Video] [PPT] 複合式地震速報 [Video] 防救災領域 [PPT] 民生示警公開資料 [Video] [PPT] 災害防救資訊系統整合 [Video] 為了擴大全民對於民生公共物聯網與其資料平台的參與，自 2018 年起，民生公共物聯網計畫也陸續舉辦了一系列的資料應用競賽、資料創新馬拉松、實體與虛擬展覽，同時也設計了一系列的訓練教材與商業輔導，從團隊的建立與養成，創意的發起到成形，應用服務的構思到落地，在過去這幾年間已累積多年的能量，並且成功發展出成功的案例，讓民生公共物聯網不僅僅只是政府單位的硬體佈建，更已成功轉化為民生公共的基礎資訊建設，提供源源不絕高品質的感測資料，便利民眾的生活，開創更多便民、有感、體貼的資訊服務。\n有關民生公共物聯網資料應用服務在各個領域的解決方案範例，可參考下列的網站資源：\n民生公共物聯網資料應用服務與解決方案－水資源 民生公共物聯網資料應用服務與解決方案－空氣品質 民生公共物聯網資料應用服務與解決方案－地震 民生公共物聯網資料應用服務與解決方案－防救災 參考資料 Open Definition: defining open in open data, open content and open knowledge. Open Knowledge Foundation. (https://opendefinition.org/od/2.1/en/) 民生公共物聯網 (https://ci.taiwan.gov.tw) 民生公共物聯網線上虛擬特展：在萬物相連中對話 (https://ci.taiwan.gov.tw/dialogue-in-civil-iot) 民生公共物聯網資料應用服務與解決方案指南 (https://www.civiliottw.tadpi.org.tw) 民生公共物聯網資料服務平台 (https://ci.taiwan.gov.tw/dsp/) XML - Wikipedia (https://en.wikipedia.org/wiki/XML) JSON - Wikipedia (https://en.wikipedia.org/wiki/JSON) Comma-separated values - Wikipedia (https://en.wikipedia.org/wiki/Comma-separated_values) Standard Generalized Markup Language - Wikipedia (https://en.wikipedia.org/wiki/Standard_Generalized_Markup_Language) OGC SensorThings API Documentation (https://developers.sensorup.com/docs/) ",
    "description": "教學網站簡介",
    "tags": [
      "概述"
    ],
    "title": "1. 教學網站簡介",
    "uri": "/ch1/"
  },
  {
    "content": "2. 整體課程前言 在這個主題中，我們介紹民生公共物聯網資料應用教材的整體架構，以及所使用到的程式語言 Python 和開發平台 Google Colab。除了概念上的描述外，我們也提供由淺入深的大量延伸學習資源，讓有興趣的讀者，可以依據需求自行進行更進一步地探索學習。\n2.1. 課程架構說明民生公共物聯網資料應用的整體架構介紹\n2.2. 課程軟體工具簡介民生公共物聯網資料應用所使用到的程式語言 Python 和開發平台 Google Colab 簡介\nPrevious Next     Page: / ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "2. 整體課程前言",
    "uri": "/ch2/"
  },
  {
    "content": "3. 資料取用 在這個主題中，我們介紹如何透過本教材提供的教材工具，使用簡易的 Python 語法，直接取用民生公共物聯網開放資料平台的資料。為了更深入的透過範例介紹不同的資料存取方法，我們將此主題切分出兩個單元，進行更深入的介紹：\n3.1. 基本資料存取方法我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。\n3.2. 存取特定時空條件的資料我們介紹如何在民生公共物聯網資料平台中獲取特定時間或時間段的資料，以及特定地理區域的資料，並透過簡單的案例演示其應用。\nPrevious Next     Page: / ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "3. 資料取用",
    "uri": "/ch3/"
  },
  {
    "content": "4. 時間維度資料分析 在這個主題中，我們針對物聯網資料所具備的時序特性，介紹一系列的時間維度資料分析方法。透過民生公共物聯網資料平台的使用，我們將發展下列三項單元，進行更深入的介紹。\n4.1. 時間序列資料處理我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與殘差波動，同時套用既有的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料，並探討其背後所可能隱含的意義。\n4.2. 時間序列資料預測我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件 (例如 scikit-learn、Kats 等)，用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。\n4.3. 時間序列屬性分群我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，並且探討不同的資料集與不同時間解析度的資料分群，在真實場域所代表的意義，以及可能衍生的應用。\nPrevious Next     Page: / ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "4. 時間維度資料分析",
    "uri": "/ch4/"
  },
  {
    "content": "5. 空間維度資料分析 在這個主題中，我們針對物聯網資料所具備的地理空間特性，介紹一系列的空間維度資料處理與資料分析方法。透過民生公共物聯網資料平台的使用，我們將發展下列兩項單元，進行更深入的介紹。\n5.1. 地理空間篩選我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。\n5.2. 地理空間分析我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。\nPrevious Next     Page: / ",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "5. 空間維度資料分析",
    "uri": "/ch5/"
  },
  {
    "content": "6. 資料應用 在這個主題中，我們將著重在存取民生公共物聯網開放資料後的衍生應用，透過其他函式庫套件與分析演算法的導入，強化民生公共物聯網開放資料的價值與應用服務。我們預計發展的單元內容包含：\n6.1. 機器學習初探我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行資料分類與資料分群的分析。我們將示範機器學習的標準流程，並且介紹如何透過資料分類進一步進行資料預測，以及如何透過資料分群進行對資料進一步的深入探究。\n6.2. 異常資料偵測我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。\n6.3. 感測器聯合校正我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。\n",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "6. 資料應用",
    "uri": "/ch6/"
  },
  {
    "content": "7. 系統整合應用 在這個主題中，我們將著重在民生公共物聯網開放資料與其他應用軟體的整合應用，透過其他應用軟體的專業功能，進一步深化與發揮民生公共物聯網開放資料的價值。\n7.1. QGIS 應用我們介紹使用 QGIS 系統進行的地理資料呈現，並且以Civil IoT Taiwan 的資料當作範例，利用點擊拖拉的方式，進行地理空間分析。同時我們也討論 QGIS 軟體的優缺點與使用時機。\n7.2. Tableau 應用我們介紹使用 Tableau 工具呈現民生公共物聯網的開放資料，並使用空品資料和災害通報資料進行兩個範例演示。我們介紹如何使用工作表、儀表板和文本來建立互動式的資料視覺化系統，方便使用者近一步深入探索。我們同時提供豐富的參考資料供使用者學習參考。\n7.3. Leafmap 應用我們介紹 leafmap 套件在民生公共物聯網資料平台中使用不同類型數據進行地理信息表示和空間分析的能力，並演示了 leafmap 和 streamlit 套件的結合共同構建 Web GIS 的應用。透過跨域與跨工具的資源整合，將能拓展讀者對數據分析和信息服務未來的想像。\n",
    "description": "",
    "tags": [
      "概述"
    ],
    "title": "7. 系統整合應用",
    "uri": "/ch7/"
  },
  {
    "content": "上傳文章 資訊 介紹如何上傳文章\n其他參考資料 相關開放網課 哈爸 自學政府開放資料應用實戰-以LASS 總統盃水專案為例\n本釋出幾乎包含整個課程的所有東西，含（投影片，錄影，相關文件/素材，範例程式碼，QGIS包，學習歷程以及教自學心得）\n哈爸 高中生自學用 python 寫遊戲\n本釋出幾乎包含整個課程的所有東西，含（投影片，錄影，相關文件/素材，範例程式碼，復刻遊戲專案）\n",
    "description": "",
    "tags": null,
    "title": "附錄一：上傳文章",
    "uri": "/upload/"
  },
  {
    "content": "本套課程包含「教學網站簡介」、「整體課程前言」、「資料取用方法」、「時間維度資料分析」、「空間維度資料分析」、「資料應用案例」與「系統整合案例」等七大主題，其具體內容分別說明如下：\n教學網站簡介 我們介紹這整個計畫的核心 - 「民生公共物聯網」，介紹台灣過去幾年在「民生公共物聯網」的各項成果，以及「民生公共物聯網開放資料」涵蓋水、空、地、災四大面向資料，所能帶來在日常生活上的各項應用與服務。我們同時呈現各項既有的文章、影片與成功案例，讓學員與閱讀者能深刻體認「民生公共物聯網」對民生用途的重要性。\n整體課程前言 我們介紹這整套課程的架構，以及本套課程所使用的 Python程式語言與 Google Colab程式開發平台，透過淺顯的文字介紹，引導學員快速了解整體課程的架構，並且提供豐富的外部資源清單，讓有興趣與需求的學員，能更進一步深入探索相關的技術。\n資料取用方法 我們介紹如何透過我們開發的工具，可以使用簡易的 Python 語法，直接取用民生公共物聯網開放資料平台的資料，並且根據不同的需求，切割出兩個單元進行更深入的介紹：\n基本資料存取方法：我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。 存取特定時空條件的資料：我們介紹如何獲取民生公共物聯網資料平台中某測站特定時間或時間區段的資料、尋找最鄰近的測站當下最新的一筆資料，以及尋找某位置座標周圍固定區域所有測站當下最新的一筆資料等應用。 在這兩個單元中，除了介紹資料的存取方法外，我們也穿插基本的探索式資料分析 (Exploratory Data Analysis, EDA) 方法，藉由常用的統計方式描述資料不同面向的資料特性，並繪製簡易的圖表，讓讀者透過動手做、做中學的方式，體驗掌握資料與資料分析的成就感。\n時間維度資料分析 我們針對物聯網資料所具備的時序特性，設計三個單元介紹一系列的時間維度資料分析方法。\n時間序列資料處理：我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與隨機波動四大部分，同時套用既有常見的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料中，有關變點檢測與異常值檢測，其背後所可能隱含的意義。 時間序列資料預測：我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件（例如 scikit-learn、Kats 等），用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。 時間序列屬性分群：我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，探討資料集在不同時間解析度所代表的意義，以及可能衍生的應用。 空間維度資料分析 我們針對物聯網資料所具備的地理空間特性，根據不同的分析需求與應用，介紹一系列的空間維度資料處理與資料分析方法。\n地理空間篩選：我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。 地理空間分析：我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。 資料應用案例 我們在這個主題中，將著重在存取民生公共物聯網開放資料後的衍生應用，透過其他函式庫套件與分析演算法的導入，強化民生公共物聯網開放資料的價值與應用服務。我們依照由淺入深的原則，依序發展下列三個子主題：\n機器學習初探：我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行感測值的預測分析。我們將示範機器學習的標準流程，並且介紹預測分析的成效評估方法，以及如何避免機器學習容易產生的偏差 (Bias) 與過度學習 (Overfitting) 問題。 感測器異常偵測：我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。 感測器動態校正模型：我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。 系統整合案例 在這個主題中，我們著重在民生公共物聯網開放資料與其他應用軟體的整合應用，透過其他應用軟體的專業功能，進一步深化與發揮民生公共物聯網開放資料的價值。我們發展的單元內容包含：\nQGIS 應用：QGIS 是一套免費且開源的 GIS 軟體，使用者可以藉由這套軟體，整理、分析與繪製不同的地理空間資料與主題地圖，呈現地理現象和資訊型態的分佈狀況。我們示範導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，利用 QGIS 的圖資進行套疊，用點擊拖拉的方式，進行在「地理空間篩選」與「地理空間分析」進行過的各種分析，並且討論 QGIS 軟體的優缺點與使用時機。 Tableau Public 應用：Tableau 是一套操作簡單且功能強大的資料視覺化軟體，可以輕易地連結資料庫與各種格式資料檔案，並製作各種精美的統計圖表。我們示範如何利用拖拉點選的方式，導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，並且針對數值資料進行簡單的圖表製作與相互比對，對於空間資料則結合地圖圖資進行套疊與製圖，對於時間序列資料則透過圖表方式顯示數值變化的趨勢。此外，透過外部資料源的介接，我們可以將多元且相異來源的資料集，整合在單一地圖製圖中，在極短的時間內產製精美的報表。 使由 leafmap 與 Streamlit 自建簡易的 GIS 資訊服務：leafmap是一套Python 開發套件，用於和 Google Earth Engine 進行深度整合，並能在類似 Google Colab 或 Jupyter Notebook 平台上直接進行操作與視覺化呈現分析成果；Streamlit則是一個提供 Python 使用者迅速搭建架構網頁應用程式的套件。我們示範導入民生公共物聯網資料平台中水、空、地、災四大面向的資料，藉由 leafmap 的地理資訊圖台與空間分析能力，搭配 Google Earth Engine 豐富的衛星影像，建構簡單的 GIS 資訊服務，同時借助 Streamlit 的友善介面操作，整合成網頁版的 GIS 資訊服務，擴展讀者對於資料分析與資訊服務的未來想像。 參考資料 民生公共物聯網 (https://ci.taiwan.gov.tw) 民生公共物聯網資料服務平台 (https://ci.taiwan.gov.tw/dsp/) QGIS - A Free and Open Source Geographic Information System (https://qgis.org/) Tableau - Business Intelligence and Analytics Software (https://www.tableau.com/) Leafmap - A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit - The fastest way to build and share data apps (https://streamlit.io/) Google Colaboratory (https://colab.research.google.com/) ",
    "description": "民生公共物聯網資料應用的整體架構介紹",
    "tags": [
      "概述"
    ],
    "title": "2.1. 課程架構說明",
    "uri": "/ch2/ch2.1/"
  },
  {
    "content": "\nTable Of Contents pyCIOT API 是什麼 pyCIOT API 使用方法 下載函式庫 使用函式庫 pyCIoT API 獲取資料方式 空氣資料存取 獲取專案代碼 Air().get_source() 獲取所有測站列表 Air().get_station() 獲取測站資料 Air().get_data() 水源資料存取 獲取專案代碼 Water().get_source() 獲取所有測站列表 Water().get_station() 獲取測站資料 Water().get_data() 地震資料存取 獲取專案代碼 Quake().get_source() 獲取地震監測站列表 Quake().get_station() 獲取地震資料 Quake().get_data() 獲取單一地震資料Quake().get_data() 天氣資料存取 獲取專案代碼 Weather().get_source() 獲取所有測站列表 Weather().get_station() 獲得測站資料 Weather().get_data() 影像資料存取 獲取專案代碼 CCTV().get_source() 獲取影像資料 CCTV().get_data() 災難警示資料存取 取得災情示警 Disaster().get_alert() 獲取災情通報歷史資料 Disaster().get_notice() 參考資料 本章節涵蓋 pyCIOT API 使用方法，以及空氣、水源、地震、天氣、影像資料、以及災難警示資料的基本存取方法。包含單一測站的最新一筆感測資料、獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。\n本章節需要讀者有基本的終端機操作能力及接觸過 Python 程式基本語法。\npyCIOT API 是什麼 政府開放資料現已有非常多種類的資料及窗口讓我們查詢，而不同的窗口有各自不同的資料存取方式。即便這些資料逐漸使用開放授權，但在進行資料搜集時，因為下載取得資料的方式各有差異，若要將這些資料整理會變得極為麻煩。本函式庫為了解決窗口不一的困難，將所有有提供 API 的政府民生開放資料的開放搜集至此，嘗試讓開放資料獲取門檻降低，降低自動化及二手資料處理成本。****\npyCIOT API 使用方法 下載函式庫 為了使用 pyCIOT API 服務，我們需要先將下載此服務的函式庫。pip 是一個以 Python 寫成的軟體包管理系統，用來安裝和管理 Python 軟體包。而這次使用的 pyCIOT 函式庫則是交由 Python Package Index (pypi) 管理，而我們可以在終端機上用這行指令將 pyCIOT 函式庫下載到本地，同時也會將其他必須的套件一起下載：\n!pip install pyCIOT 使用函式庫 欲使用本函式庫，僅需輸入匯入語法匯入 pyCIOT.data 即可：\n# Import pyCIOT.data from pyCIOT.data import * 根據匯入函式庫的方式不同，呼叫函式的方法也不一樣。若是使用 from ... import ... 的語法，呼叫函式時不需加上前綴；但若是使用 import ... 的話，則是要在每次呼叫其函式庫下的函式時加上前綴，而使用 import ... as ... 則是可以根據在 as 後自定義前綴方便撰寫程式。\n# 引入函式的三種方式 import pyCIOT.data a = pyCIOT.data.Air().get_source() # ~~~~~~~~~~~~ 加虛線上的文字 import pyCIOT.data as CIoT a = CIoT.Air().get_source() # ~~~~~ 可自行定義前綴 from pyCIOT.data import * a = Air().get_source() # 在匯入大量函式庫時盡量少用，避免函式庫名稱衝撞的問題 pyCIoT API 獲取資料方式 大部分的 API 資料格式都能夠透過下列方式獲取，包含空氣、水源、地震、天氣、影像資料等：\n.get_source() 獲取專案代碼回傳在民生公共物聯網開放資料平台中有的專案代碼，返回格式為 array。 .get_station(src='') 獲取回傳各測站資料的基本資訊及位置，返回格式為 array；可選擇性帶入 src 參數，指名所要查詢的專案代碼。 .get_data(src='', stationIds=[]) 獲取所有測站列表返回格式為 array，回傳各測站的基本資訊，位置及感測資料；可選擇性帶入 src 參數，指名所要查詢的專案代碼，或選擇性帶入 stationIds 參數，指名所要查詢的機器代碼。 災害警示資料則適用於以下：\n.get_alert() 獲取警示資料返回格式為 json，包括該事件的相關示警資訊。 .get_notice() 獲取通報歷史資料返回格式為 json，包括該事件的相關通報資訊。 若與 pyCIOT Package Document 內容相悖，以其為準。\n空氣資料存取 獲取專案代碼 Air().get_source() # 回傳所有空氣相關的專案代碼 a = Air().get_source() print(a) ['OBS:EPA', 'OBS:EPA_IoT', 'OBS:AS_IoT', 'OBS:MOST_IoT', 'OBS:NCNU_IoT'] 專案代碼轉換列表\nOBS:EPA: 環保署國家空品測站 OBS:EPA_IoT: 環保署智慧城鄉空品微型感測器 OBS:AS_IoT: 中研院校園空品微型感測器 OBS:MOST_IoT: 科技部智慧園區空品測站 OBS:NCNU_IoT: 暨南大學在地空品微型感測器 獲取所有測站列表 Air().get_station() # 獲取環保署智慧城鄉空品微型感測的檢測站列表 b = Air().get_station(src=\"OBS:EPA_IoT\") b[0:5] [ { 'name': '智慧城鄉空品微型感測器-10287974676', 'description': '智慧城鄉空品微型感測器-10287974676', 'properties': { 'city': '新北市', 'areaType': '社區', 'isMobile': 'false', 'township': '鶯歌區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10287974676', 'locationId': 'TW040203A0507221', 'Description': '廣域SAQ-210', 'areaDescription': '鶯歌區' }, 'location': { 'latitude': 24.9507, 'longitude': 121.3408416, 'address': None } }, ... ] 獲取測站資料 Air().get_data() f = Air().get_data(src=\"OBS:EPA_IoT\", stationIds=[\"11613429495\"]) f [ {'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 30.6}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-28T06:23:08.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-28T06:22:08.000Z', 'value': 9.8}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}} ] print(f[0]['description']) for f_data in f[0]['data']: if f_data['description'] == '溫度': print(f_data['description'], ': ', f_data['values'][0]['value'], ' (', f_data['values'][0]['timestamp'], ')', sep='') 智慧城鄉空品微型感測器-11613429495 溫度: 30.6 (2022-08-28T06:22:08.000Z) 水源資料存取 獲取專案代碼 Water().get_source() 根據引數不同會回傳不同種類的專案名稱：\nwater_level_station: 回傳水位站的專案名稱（目前合法的代碼僅有 WRA, WRA2 和 IA)。 gate: 回傳閘門的專案名稱（目前合法的代碼僅有 WRA, WRA2 和 IA)。 pumping_station: 回傳抽水站的專案名稱（目前合法的代碼有 WRA2 和 TPE)。 sensor: 回傳各式感測器的專案名稱（目前合法的代碼有 WRA, WRA2, IA 和 CPAMI)。 `` (無輸入): 回傳所有水資源相關的專案名稱。 wa = Water().get_source() wa ['WATER_LEVEL:WRA_RIVER', 'WATER_LEVEL:WRA_GROUNDWATER', 'WATER_LEVEL:WRA2_DRAINAGE', 'WATER_LEVEL:IA_POND', 'WATER_LEVEL:IA_IRRIGATION', 'GATE:WRA', 'GATE:WRA2', 'GATE:IA', 'PUMPING:WRA2', 'PUMPING:TPE', 'FLOODING:WRA', 'FLOODING:WRA2'] 專案代碼轉換列表：\nWRA: 水利署 WRA2: 水利署（與縣市政府合建） IA: 農田水利署 CPAMI: 營建署 TPE: 臺北市 獲取所有測站列表 Water().get_station() wa = Water().get_station(src=\"WATER_LEVEL:WRA_RIVER\") wa[0] { 'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None} } 獲取測站資料 Water().get_data() wa = Water().get_data(src=\"WATER_LEVEL:WRA_RIVER\", stationID=\"01790145-cd7e-4498-9240-f0fcd9061df2\") wa [{'name': '01790145-cd7e-4498-9240-f0fcd9061df2', 'description': '現場觀測', 'properties': {'authority': '水利署水文技術組', 'stationID': '01790145-cd7e-4498-9240-f0fcd9061df2', 'stationCode': '2200H007', 'stationName': '延平', 'authority_type': '水利署'}, 'data': [{'name': '水位', 'description': ' Datastream_id=016e5ea0-7c7f-41a2-af41-eabacdbb613f, Datastream_FullName=延平.水位, Datastream_Description=現場觀測, Datastream_Category_type=河川水位站, Datastream_Category=水文', 'values': [{'timestamp': '2022-08-28T06:00:00.000Z', 'value': 157.41}]}], 'location': {'latitude': 22.8983536, 'longitude': 121.0845795, 'address': None}}] 地震資料存取 獲取專案代碼 Quake().get_source() q = Quake().get_source() q ['EARTHQUAKE:CWB+NCREE'] 專案代碼轉換列表：\nEARTHQUAKE:CWB+NCREE: 中央氣象局與國震中心地震監測站（因資料儲存方式不同，CWB 及 NCREE 的測站資料無法分別查詢，因此在每個事件中，會回傳來源中所有測站的監測資料） 獲取地震監測站列表 Quake().get_station() q = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") q[0:2] [{'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'properties': {'authority': '中央氣象局', 'stationID': 'EGC', 'deviceType': 'FBA', 'stationName': 'Jiqi'}, 'location': {'latitude': 23.708, 'longitude': 121.548, 'address': None}}, {'name': '地震監測站-Xilin-ESL', 'description': '地震監測站-Xilin-ESL', 'properties': {'authority': '中央氣象局', 'stationID': 'ESL', 'deviceType': 'FBA', 'stationName': 'Xilin'}, 'location': {'latitude': 23.812, 'longitude': 121.442, 'address': None}}] 獲取地震資料 Quake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\") q[-1] {'name': '第2022083號地震', 'description': '第2022083號地震', 'properties': {'depth': 43.6, 'authority': '中央氣象局', 'magnitude': 5.4}, 'data': [ { 'name': '地震監測站-Jiqi-EGC', 'description': '地震監測站-Jiqi-EGC', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18815)' },{ 'name': '地震監測站-Taichung City-TCU', 'description': '地震監測站-Taichung City-TCU', 'timestamp': '2022-07-28T08:16:10.000Z', 'value': 'http://140.110.19.16/STA_Earthquake_v2/v1.0/Observations(18816)' }, ... ] 'location': {'latitude': 22.98, 'longitude': 121.37, 'address': None}} 獲取單一地震資料Quake().get_data() q = Quake().get_data(src=\"EARTHQUAKE:CWB+NCREE\", eventID=\"2022083\") q # 獲得資料之格式和上述相同 天氣資料存取 獲取專案代碼 Weather().get_source() w = Weather().get_source() w ['GENERAL:CWB', 'GENERAL:CWB_IoT', 'RAINFALL:CWB', 'RAINFALL:WRA', 'RAINFALL:WRA2', 'RAINFALL:IA', 'IMAGE:CWB'] 根據引數不同會回傳不同種類的專案名稱：\nGENERAL: 回傳氣象站的專案代碼 RAINFALL: 回傳雨量站/雨量感測器的專案代碼 IMAGE: 回傳雷達回波圖的專案代碼 `` (無輸入): 回傳所有氣象相關的專案代碼。 專案代碼轉換列表：\nGENERAL:CWB: 中央氣象局局屬氣象站 GENERAL:CWB_IoT: 中央氣象局自動氣象站 RAINFALL:CWB:中央氣象局雨量站 RAINFALL:WRA: 水利署雨量感測器 RAINFALL:WRA2: 水利署（與縣市政府合建）雨量感測器 RAINFALL:IA: 農田水利署雨量感測器 IMAGE:CWB: 中央氣象局雷達整合回波圖 獲取所有測站列表 Weather().get_station() w = Weather().get_station(src=\"RAINFALL:CWB\") w [{'name': '雨量站-C1R120-上德文', 'description': '雨量站-C1R120-上德文', 'properties': {'city': '屏東縣', 'township': '三地門鄉', 'authority': '中央氣象局', 'stationID': 'C1R120', 'stationName': '上德文', 'stationType': '局屬無人測站'}, 'location': {'latitude': 22.765, 'longitude': 120.6964, 'address': None}}, ... ] 獲得測站資料 Weather().get_data() # 南投縣 雨量站-U2HA40-臺大內茅埔 的雨量資料 w = Weather().get_data(src=\"RAINFALL:CWB\", stationID=\"U2HA40\") w [{'name': '雨量站-U2HA40-臺大內茅埔', 'description': '雨量站-U2HA40-臺大內茅埔', 'properties': {'city': '南投縣', 'township': '信義鄉', 'authority': '中央氣象局', 'stationID': 'U2HA40', 'stationName': '臺大內茅埔', 'stationType': '中央氣象局'}, 'data': [{'name': 'HOUR_12', 'description': '12小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'MIN_10', 'description': '10分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'RAIN', 'description': '60分鐘累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_6', 'description': '6小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_3', 'description': '3小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'HOUR_24', 'description': '24小時累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'NOW', 'description': '本日累積雨量', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 0.0}]}, {'name': 'ELEV', 'description': '高度', 'values': [{'timestamp': '2022-08-28T05:30:00.000Z', 'value': 507.0}]}], 'location': {'latitude': 23.6915, 'longitude': 120.843, 'address': None}}] 影像資料存取 獲取專案代碼 CCTV().get_source() cctv = CCTV().get_source() cctv ['IMAGE:EPA', 'IMAGE:WRA', 'IMAGE:COA'] 專案代碼轉換列表：\nIMAGE:EPA: 環保署空品監測即時影像器 IMAGE:WRA: 水利署水利防災用影像器 IMAGE:COA: 行政院農委會土石流觀測站影像 獲取影像資料 CCTV().get_data() cEPA = CCTV().get_data(\"IMAGE:EPA\") cEPA[2] { 'name': '環保署空品監測即時影像器-萬里', 'description': '環保署-萬里-空品監測即時影像器', 'properties': { 'city': '新北市', 'basin': '北部空品區', 'authority': '環保署', 'stationName': '萬里', 'CCDIdentifier': '3'}, 'data': [ { 'name': '即時影像', 'description': '環保署-萬里-空品監測即時影像器', 'values': [ { 'timestamp': '2022-06-13T09:00:00.000Z', 'value': 'https://airtw.epa.gov.tw/AirSitePic/20220613/003-202206131700.jpg' } ] } ], 'location': {'latitude': 25.179667, 'longitude': 121.689881, 'address': None} } cCOA = CCTV().get_data(\"IMAGE:COA\") cCOA[0] {'name': '行政院農委會土石流觀測站影像-大粗坑下游攝影機', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'properties': {'city': '新北市', 'township': '瑞芳鎮', 'StationId': '7', 'authority': '行政院農委會', 'stationName': '大粗坑下游攝影機', 'CCDIdentifier': '2'}, 'data': [{'name': '即時影像', 'description': '行政院農委會-大粗坑下游攝影機-土石流觀測站影像', 'values': [{'timestamp': '2099-12-31T00:00:00.000Z', 'value': 'http://dfm.swcb.gov.tw/debrisFinal/ShowCCDImg-LG.asp?StationID=7\u0026CCDId=2'}]}], 'location': {'latitude': 25.090878, 'longitude': 121.837815, 'address': '新北市瑞芳鎮弓橋里大粗坑'}} 災難警示資料存取 民生公共物聯網資料平台上提供的災情示警有 58 個項目以上，而災情通報共計有 41 個項目以上；不同災情示警個由不同單位負責，而災情通報則為中央災害應變中心負責。\n因專案代碼轉換列表過長，可參考 pyCIOT Package Document。\n取得災情示警 Disaster().get_alert() d = Disaster().get_alert(\"5\") d {'id': 'https://alerts.ncdr.nat.gov.tw/Json.aspx', 'title': 'NCDR_CAP-即時防災資訊(Json)', 'updated': '2021-10-12T08:32:00+08:00', 'author': {'name': 'NCDR'}, 'link': {'@rel': 'self', '@href': 'https://alerts.ncdr.nat.gov.tw/JSONAtomFeed.ashx?AlertType=5'}, 'entry': [ { 'id': 'CWB-Weather_typhoon-warning_202110102030001', 'title': '颱風', 'updated': '2021-10-10T20:30:05+08:00', 'author': {'name': '中央氣象局'}, 'link': {'@rel': 'alternate', '@href': 'https://b-alertsline.cdn.hinet.net/Capstorage/CWB/2021/Typhoon_warnings/fifows_typhoon-warning_202110102030.cap'}, 'summary': { '@type': 'html', '#text': '1SEA18KOMPASU圓規2021-10-10T12:00:00+00:0018.20,126.702330992150輕度颱風TROPICAL STORM2021-10-11T12:00:00+00:0019.20,121.902835980220輕度颱風 圓規（國際命名 KOMPASU）10日20時的中心位置在北緯 18.2 度，東經 126.7 度，即在鵝鑾鼻的東南東方約 730 公里之海面上。中心氣壓 992 百帕，近中心最大風速每秒 23 公尺（約每小時 83 公里），相當於 9 級風，瞬間最大陣風每秒 30 公尺（約每小時 108 公里），相當於 11 級風，七級風暴風半徑 150 公里，十級風暴風半徑 – 公里。以每小時21公里速度，向西進行，預測11日20時的中心位置在北緯 19.2 度，東經 121.9 度，即在鵝鑾鼻的南南東方約 320 公里之海面上。根據最新資料顯示，第18號颱風中心目前在鵝鑾鼻東南東方海面，向西移動，其暴風圈正逐漸向巴士海峽接近，對巴士海峽將構成威脅。巴士海峽航行及作業船隻應嚴加戒備。第18號颱風外圍環流影響，易有短延時強降雨，今(10日)晚至明(11)日基隆北海岸、宜蘭地區及新北山區有局部大雨發生的機率，請注意。＊第18號颱風及其外圍環流影響，今(10日)晚至明(11)日巴士海峽及臺灣附近各海面風浪逐漸增大，基隆北海岸、東半部（含蘭嶼、綠島）、西南部、恆春半島沿海易有長浪發生，前往海邊活動請特別注意安全。＊第18號颱風外圍環流影響，今(10日)晚至明(11)日臺南以北、東半部(含蘭嶼、綠島)、恆春半島、澎湖、金門、馬祖沿海及空曠地區將有9至12級強陣風，內陸地區及其他沿海空曠地區亦有較強陣風，請注意。＊第18號颱風外圍環流沉降影響，明(11)日南投、彰化至臺南及金門地區高溫炎熱，局部地區有36度以上高溫發生的機率，請注意。＊本警報單之颱風半徑為平均半徑，第18號颱風之7級風暴風半徑西南象限較小約60公里，其他象限約180公里，平均半徑約為150公里。'}, 'category': {'@term': '颱風'} },{ ... }, ... ] } 獲取災情通報歷史資料 Disaster().get_notice() d = Disaster().get_notice(\"ERA2_F1\") # 交通災情通報表（道路、橋梁部分） d \"maincmt\":{ \"prj_no\":\"專案代號\", \"org_name\":\"填報機關\", \"rpt_approval\":\"核定人\", \"rpt_phone\":\"聯絡電話\", \"rpt_mobile_phone\":\"行動電話\", \"rpt_no\":\"通報別\", \"rpt_user\":\"通報人\", \"rpt_time\":\"通報時間\" }, \"main\":{ \"prj_no\":\"2014224301\", \"org_name\":\"交通部公路總局\", ... }, \"detailcmt\":{ \"trfstatus\":\"狀態\", ... }, ... } 參考資料 Python pyCIOT pypi package (https://pypi.org/project/pyCIOT/) Python pyCIOT Document (https://hackmd.io/@cclljj/pyCIOT_doc) ",
    "description": "我們介紹如何取用民生公共物聯網開放資料平台中，有關水、空、地、災不同面向單一測站的最新一筆感測資料，如何獲取所有測站的列表，以及如何獲取所有測站當下最新的一筆感測資料。",
    "tags": [
      "Python",
      "API",
      "水",
      "空",
      "地",
      "災"
    ],
    "title": "3.1. 基本資料存取方法",
    "uri": "/ch3/ch3.1/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引用 讀取資料 空品資料 水位資料 氣象資料 資料視覺化 (Visualization) 重新採樣 (resample) 移動平均 (moving average) 多曲線圖 日曆熱力圖 資料品質檢測與處理 離群值偵測 (Outlier detection) 改變點偵測 (Change point detection) 缺失資料 (missing data) 處理 資料分解(Decomposition) 參考資料 時間序列資料是依照時間上發生的先後順序形成的資料，通常在資料上的時間間隔會一樣（例如：五分鐘一筆資料、一小時一筆資料），應用的領域相當廣泛，如：金融資訊、太空工程、訊號處理等，在分析上也有許多統計相關的工具可以使用。 同時也可以發現時序資料是很貼近日常生活的，隨著全球氣候變遷的日益加劇，這幾年全球的平均氣溫越來越高，在夏天時更是讓人熱到非常有感，也越來越難以忍受；又或是在一年中某些季節的空氣品質往往特別差，或者某些時間的空氣品質往往比其他時間來的差等。如果想要更加了解這些生活環境的改變，以及其對應的感測器數值是如何變化的，就會運用到時間序列資料的分析，也就是觀察資料與時間的關係，進而得出結果。本章節將會使用三種資料（空氣品質、水資源、氣象）示範。\n章節目標 使用作圖工具觀察時序資料 檢測與處理時序資料 分解時序資料得到趨勢與週期性 套件安裝與引用 在本章節中，我們將會使用到 pandas, matplotlib, numpy, seaborn, statsmodels, warnings 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 並未預先安裝好的套件：kats 和 calplot，需使用下列的方式自行安裝：\n!pip install --upgrade pip # Kats !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 # calplot !pip install calplot 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport warnings import calplot import pandas as pd import numpy as np import matplotlib as mpl import matplotlib.pyplot as plt import statsmodels.api as sm import os, zipfile from datetime import datetime, timedelta from dateutil import parser as datetime_parser from statsmodels.tsa.stattools import adfuller, kpss from statsmodels.tsa.seasonal import seasonal_decompose from kats.detectors.outlier import OutlierDetector from kats.detectors.cusum_detection import CUSUMDetector from kats.consts import TimeSeriesData, TimeSeriesIterator from IPython.core.pylabtools import figsize 讀取資料 我們使用 pandas 進行資料的處理，pandas 是 Python 語言常使用到的資料科學套件，也可以想成是程式語言中類似 Microsoft Excel 的試算表，而 pandas 所提供的 dataframe 物件，更可以想成是一種二維的資料結構，可以用行、列的方式儲存資料，方便各式的資料處理與運算。\n本章節的探討主題為時序資料 (time series data) 的分析處理，因此我們將分別以民生公共物聯網資料平台上的空品、水位和氣象資料進行資料讀取的演示，接著再使用空品資料進行更進一步的資料分析。其中，每一類別的資料處理都將使用其中一個測站長期以來觀測到的資料作為資料集，而在 dataframe 的時間欄位名稱則設為 timestamp，由於時間欄位的數值具有唯一性，因此我們也將使用此欄位作為 dataframe 的索引 (index)。\n空品資料 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的歷史資料，並存入 Air 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Air 資料夾中。\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') 現在 CSV_Air 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 74DA38C7D2AC 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 air 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 水位資料 和空品資料的範例一樣，由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「水利署地下水位站」的歷史資料，並存入 Water 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Water 資料夾中。\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') 現在 CSV_Water 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 338c9c1c-57d8-41d7-9af2-731fb86e632c 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 water 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 氣象資料 我們從民生公共物聯網資料平台的歷史資料庫下載「中央氣象局自動氣象站」的歷史資料，並存入 Weather 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Weather 資料夾中。\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') 現在 CSV_Weather 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 C0U750 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 weather 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN 以上我們已經成功示範空品資料 (air)、水位資料 (water) 和氣象資料 (weather) 的讀取範例，在接下來的探討中，我們將以空品資料示範初步的時間序列資料處理，相同的方法也可以輕易改成使用水位資料或氣象資料而得到類似的結果，大家可以自行嘗試看看。\n資料視覺化 (Visualization) 時間序列資料處理的第一個步驟，不外乎就是將資料依照時間順序一筆一筆的呈現出來，讓使用者可以看到整體資料的變化，並且衍生更多資料分析的想法與概念，其中使用折線圖進行資料的展示，是最常使用的一種資料視覺化方法。例如若以空品資料為例：\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() 重新採樣 (resample) 從上圖空品資料的時序圖中，可以看到其實資料的分佈是很密集的，同時資料數值的變化有時很小有時卻很劇烈，這是因為目前空品資料的採樣頻率約略是每五分鐘一筆，同時採集的環境是生活中的周遭環境資訊，因此資料密集與起伏不定其實是必然的。由於每五分鐘一筆資料的採樣過於頻繁，不易呈現環境空品的整體變化趨勢，因此我們採用重新採樣的方法，在固定的時間間隔內計算資料的平均值，便能呈現資料資料在不同時間尺度的整體變化狀況。例如我們根據現有空品資料的特性，利用下列的語法以較大尺度 (小時、日、月) 的採樣率重新取樣：\nair_hour = air.resample('H').mean() #每小時的平均 air_day = air.resample('D').mean() #每日的平均 air_month = air.resample('M').mean() #每月的平均 print(air_hour.head()) print(air_day.head()) print(air_month.head()) PM25 timestamp 2018-08-01 00:00:00 18.500000 2018-08-01 01:00:00 20.750000 2018-08-01 02:00:00 24.000000 2018-08-01 03:00:00 27.800000 2018-08-01 04:00:00 22.833333 PM25 timestamp 2018-08-01 23.384615 2018-08-02 13.444444 2018-08-03 14.677419 2018-08-04 14.408451 2018-08-05 NaN PM25 timestamp 2018-08-31 21.704456 2018-09-30 31.797806 2018-10-31 37.217788 2018-11-30 43.228939 接著我們使用每小時平均的重新採樣後資料再次進行作圖，可以看到線條曲線變得較為清晰，但曲線的波動還是很大。\nplt.figure(figsize=(15, 10), dpi=60) plt.plot(air_hour[:][\"PM25\"]) plt.xlabel(\"Date\") plt.ylabel(\"PM2.5\") plt.title(\"PM2.5 Time Series Plot\") plt.tight_layout() plt.show() 移動平均 (moving average) 針對原始資料的變化圖，如果想看到更平滑的曲線變化趨勢圖，可以套用移動平均的方法，其中最主要的觀念就是在原始資料的時間軸上設定一個取樣窗 (window)，並在取樣窗內計算所有數值的平均數，並且用平滑移動的方式滑動取樣窗的位置，計算當下資料與窗內前幾筆資料的平均值。例如，若取樣窗的大小為 10，就代表每次要將當下的資料和前面 9 次的資料做平均，經過這樣的處理後每筆資料所代表的意義就不只是某一個時間點，而是原本時間點加上前幾個時間點的平均，如此一來，可以消除突發的變化讓整體曲線的呈現更加圓滑，也更容易觀察整體的變化趨勢。\n# plt.figure(figsize=(15, 10), dpi=60) MA = air_hour MA10 = MA.rolling(window=500, min_periods=1).mean() MA.join(MA10.add_suffix('_mean_500')).plot(figsize=(20, 15)) # MA10.plot(figsize(15, 10)) 上圖中的藍色線為原始資料，橘色線則是經過移動平均後的曲線，可以清楚發現橘色線更能代表整體數值的變化趨勢，同時也存在某種程度的規律起伏，值得後續進一步的分析。\n多曲線圖 除了將原始資料用單純的折線圖方式呈現外，另一個常見的資料時覺化手法，則是依照時間維度上的週期性，將資料切割為數個連續的片段，分別繪製折線圖，並疊加在同一張多曲線圖中。例如，我們可以將前述的空品資料，依照年份的不同切割出 2019, 2020, 2021, 和 2022 四個子資料集，並分別繪製各自的折線圖於同一張多曲線圖中，如下圖所示。\nair_month.reset_index(inplace=True) air_month['year'] = [d.year for d in air_month.timestamp] air_month['month'] = [d.strftime('%b') for d in air_month.timestamp] years = air_month['year'].unique() print(air) np.random.seed(100) mycolors = np.random.choice(list(mpl.colors.XKCD_COLORS.keys()), len(years), replace=False) plt.figure(figsize=(15, 10), dpi=60) for i, y in enumerate(years): if i \u003e 0: plt.plot('month', 'PM25', data=air_month.loc[air_month.year==y, :], color=mycolors[i], label=y) plt.text(air_month.loc[air_month.year==y, :].shape[0]-.9, air_month.loc[air_month.year==y, 'PM25'][-1:].values[0], y, fontsize=12, color=mycolors[i]) # plt.gca().set(xlim=(-0.3, 11), ylim=(2, 30), ylabel='PM25', xlabel='Month') # plt.yticks(fontsize=12, alpha=.7) # plt.title('Seasonal Plot of PM25 Time Series', fontsize=20) plt.show() 在這張多曲線圖中，我們可以發現 2019 年的資料有大片段的缺失值，2022 年的資料只記載到本文寫作時的七月份，同時也可以發現在這四個年份的折線圖中，不同年份的曲線皆在夏季時達到最低點，秋季時 PM2.5 數值則開始攀升，並且在冬季時達到最高點，呈現約略相同的變化趨勢。\n日曆熱力圖 日曆熱力圖是一種結合日曆圖 (calendar map) 與熱力圖 (heat map) 的資料視覺化呈現方式，可以更直覺瀏覽資料的分佈狀況，並從中尋找不同時間尺度的規律性。我們使用 calplot 這個 Python 語言的日曆熱力圖套件，將每日的 PM2.5 平均值輸入後，再選擇搭配的顏色 (參數名稱為 cmap，在接下來的範例中，我們先設為 GnBu，詳細的顏色選項說明，可參閱參考資料)，便能得到下圖的效果，其中藍色代表數值較高，綠色或白色代表數值較低，如果沒有塗色或是數值為 0 則代表當天沒有資料。從產生的圖中，我們可以發現中間部分的月份（夏季）顏色較淡，左邊部分的月份（冬季）顏色#較深，恰與我們之前使用多曲線圖的觀察結果一致。\n# cmap: 設定呈現的顏色色盤 (https://matplotlib.org/stable/gallery/color/colormap_reference.html) # textformat: 設定圖中數字呈現的樣式 pl1 = calplot.calplot(data = air_day['PM25'], cmap = 'GnBu', textformat = '{:.0f}', figsize = (24, 12), suptitle = \"PM25 by Month and Year\") 資料品質檢測與處理 在時序資料的基本視覺化呈現後，我們接下來介紹資料品質的檢測與基本處理方法，我們將使用 kats 這個 Python 語言的資料處理與分析套件，並依序進行離群值偵測、改變點偵測與缺失資料處理。\n離群值偵測 (Outlier detection) 離群值指的是資料中某些數值與其他數值存在顯著的差異，這些差異可能會影響我們對資料的判斷與分析結果，因此需要將離群值找出來後予以標示、刪除、或特別處理。\n我們首先將原本使用 dataframe 資料格式的 air_hour 轉換成 kats 套件所使用的 TimeSeriesData 格式，並將轉換後的資料存成 air_ts 這個變數名稱。然後我們再次繪製原始時序資料的折線圖。\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') air_ts.plot(cols=[\"PM25\"]) 接著我們使用 kats 套件中的 OutlierDetector 工具偵測時序資料中的離群值。其中，離群值指的是小於第一四分位數 (Q1) 減 1.5 倍四分位距 (IQR) 或大於第三四分位數 (Q3) 加 1.5 倍四分位距的數值。\noutlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outlierDetection.outliers [[Timestamp('2018-08-10 16:00:00'), Timestamp('2018-08-10 17:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-23 03:00:00'), Timestamp('2018-08-23 04:00:00'), Timestamp('2018-09-02 11:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-13 14:00:00'), Timestamp('2018-09-13 15:00:00'), Timestamp('2018-09-13 16:00:00'), Timestamp('2018-09-15 08:00:00'), Timestamp('2018-09-15 09:00:00'), Timestamp('2018-09-15 10:00:00'), Timestamp('2018-09-15 11:00:00'), Timestamp('2018-09-22 05:00:00'), Timestamp('2018-09-22 06:00:00'), Timestamp('2018-10-26 01:00:00'), Timestamp('2018-11-06 13:00:00'), Timestamp('2018-11-06 15:00:00'), Timestamp('2018-11-06 16:00:00'), Timestamp('2018-11-06 19:00:00'), Timestamp('2018-11-06 20:00:00'), Timestamp('2018-11-06 21:00:00'), Timestamp('2018-11-06 22:00:00'), Timestamp('2018-11-07 07:00:00'), Timestamp('2018-11-07 08:00:00'), Timestamp('2018-11-07 09:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-09 01:00:00'), Timestamp('2018-11-09 02:00:00'), Timestamp('2018-11-09 03:00:00'), Timestamp('2018-11-10 02:00:00'), Timestamp('2018-11-10 03:00:00'), Timestamp('2018-11-16 01:00:00'), Timestamp('2018-11-16 02:00:00'), Timestamp('2018-11-16 03:00:00'), Timestamp('2018-11-16 04:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-21 18:00:00'), Timestamp('2018-11-21 19:00:00'), Timestamp('2018-11-25 08:00:00'), Timestamp('2018-11-30 14:00:00'), Timestamp('2018-12-01 06:00:00'), Timestamp('2018-12-01 16:00:00'), Timestamp('2018-12-01 17:00:00'), Timestamp('2018-12-15 02:00:00'), Timestamp('2018-12-19 03:00:00'), Timestamp('2018-12-19 04:00:00'), Timestamp('2018-12-19 05:00:00'), Timestamp('2018-12-19 06:00:00'), Timestamp('2018-12-19 07:00:00'), Timestamp('2018-12-19 08:00:00'), Timestamp('2018-12-19 10:00:00'), Timestamp('2018-12-19 11:00:00'), Timestamp('2018-12-19 12:00:00'), Timestamp('2018-12-19 13:00:00'), Timestamp('2018-12-19 14:00:00'), Timestamp('2018-12-19 15:00:00'), Timestamp('2018-12-19 16:00:00'), Timestamp('2018-12-19 17:00:00'), Timestamp('2018-12-20 03:00:00'), Timestamp('2018-12-20 04:00:00'), Timestamp('2018-12-20 05:00:00'), Timestamp('2018-12-20 06:00:00'), Timestamp('2018-12-20 07:00:00'), Timestamp('2018-12-20 08:00:00'), Timestamp('2018-12-20 11:00:00'), Timestamp('2018-12-20 12:00:00'), Timestamp('2018-12-20 13:00:00'), Timestamp('2018-12-20 14:00:00'), Timestamp('2018-12-20 15:00:00'), Timestamp('2019-01-05 02:00:00'), Timestamp('2019-01-05 08:00:00'), Timestamp('2019-01-05 09:00:00'), Timestamp('2019-01-05 22:00:00'), Timestamp('2019-01-19 06:00:00'), Timestamp('2019-01-19 07:00:00'), Timestamp('2019-01-19 08:00:00'), Timestamp('2019-01-19 09:00:00'), Timestamp('2019-01-19 13:00:00'), Timestamp('2019-01-19 14:00:00'), Timestamp('2019-01-19 15:00:00'), Timestamp('2019-01-25 18:00:00'), Timestamp('2019-01-25 19:00:00'), Timestamp('2019-01-25 20:00:00'), Timestamp('2019-01-26 00:00:00'), Timestamp('2019-01-26 01:00:00'), Timestamp('2019-01-26 02:00:00'), Timestamp('2019-01-26 03:00:00'), Timestamp('2019-01-26 04:00:00'), Timestamp('2019-01-30 06:00:00'), Timestamp('2019-01-30 11:00:00'), Timestamp('2019-01-30 12:00:00'), Timestamp('2019-01-30 13:00:00'), Timestamp('2019-01-30 14:00:00'), Timestamp('2019-02-02 16:00:00'), Timestamp('2019-02-02 17:00:00'), Timestamp('2019-02-02 18:00:00'), Timestamp('2019-02-02 19:00:00'), Timestamp('2019-02-02 20:00:00'), Timestamp('2019-02-03 03:00:00'), Timestamp('2019-02-03 04:00:00'), Timestamp('2019-02-03 05:00:00'), Timestamp('2019-02-03 06:00:00'), Timestamp('2019-02-03 07:00:00'), Timestamp('2019-02-03 10:00:00'), Timestamp('2019-02-03 11:00:00'), Timestamp('2019-02-03 12:00:00'), Timestamp('2019-02-03 13:00:00'), Timestamp('2019-02-03 22:00:00'), Timestamp('2019-02-03 23:00:00'), Timestamp('2019-02-07 05:00:00'), Timestamp('2019-02-07 06:00:00'), Timestamp('2019-02-16 22:00:00'), Timestamp('2019-02-16 23:00:00'), Timestamp('2019-02-18 18:00:00'), Timestamp('2019-02-18 20:00:00'), Timestamp('2019-02-18 21:00:00'), Timestamp('2019-02-19 10:00:00'), Timestamp('2019-02-19 11:00:00'), Timestamp('2019-02-19 12:00:00'), Timestamp('2019-02-19 13:00:00'), Timestamp('2019-02-19 14:00:00'), Timestamp('2019-02-19 15:00:00'), Timestamp('2019-02-19 16:00:00'), Timestamp('2019-02-19 23:00:00'), Timestamp('2019-02-20 00:00:00'), Timestamp('2019-02-20 03:00:00'), Timestamp('2019-03-02 17:00:00'), Timestamp('2019-03-03 06:00:00'), Timestamp('2019-03-05 13:00:00'), Timestamp('2019-03-09 23:00:00'), Timestamp('2019-03-12 01:00:00'), Timestamp('2019-03-16 01:00:00'), Timestamp('2019-03-16 02:00:00'), Timestamp('2019-03-16 03:00:00'), Timestamp('2019-03-20 00:00:00'), Timestamp('2019-03-20 01:00:00'), Timestamp('2019-03-20 02:00:00'), Timestamp('2019-03-20 03:00:00'), Timestamp('2019-03-20 11:00:00'), Timestamp('2019-03-27 00:00:00'), Timestamp('2019-03-27 01:00:00'), Timestamp('2019-04-05 03:00:00'), Timestamp('2019-04-18 17:00:00'), Timestamp('2019-04-20 16:00:00'), Timestamp('2019-05-10 07:00:00'), Timestamp('2019-05-22 20:00:00'), Timestamp('2019-05-23 03:00:00'), Timestamp('2019-05-23 16:00:00'), Timestamp('2019-05-26 18:00:00'), Timestamp('2019-05-27 05:00:00'), Timestamp('2019-07-28 01:00:00'), Timestamp('2019-08-23 08:00:00'), Timestamp('2019-08-24 02:00:00'), Timestamp('2019-08-24 03:00:00'), Timestamp('2019-08-24 04:00:00'), Timestamp('2019-08-24 05:00:00'), Timestamp('2019-08-24 07:00:00'), Timestamp('2019-08-24 08:00:00'), Timestamp('2019-12-10 11:00:00'), Timestamp('2019-12-10 12:00:00'), Timestamp('2019-12-10 13:00:00'), Timestamp('2019-12-10 20:00:00'), Timestamp('2019-12-11 04:00:00'), Timestamp('2019-12-16 20:00:00'), Timestamp('2019-12-17 11:00:00'), Timestamp('2020-01-03 15:00:00'), Timestamp('2020-01-05 08:00:00'), Timestamp('2020-01-05 09:00:00'), Timestamp('2020-01-06 08:00:00'), Timestamp('2020-01-07 10:00:00'), Timestamp('2020-01-07 15:00:00'), Timestamp('2020-01-10 11:00:00'), Timestamp('2020-01-15 08:00:00'), Timestamp('2020-01-22 14:00:00'), Timestamp('2020-01-22 17:00:00'), Timestamp('2020-01-22 22:00:00'), Timestamp('2020-01-22 23:00:00'), Timestamp('2020-01-23 00:00:00'), Timestamp('2020-01-23 01:00:00'), Timestamp('2020-01-23 02:00:00'), Timestamp('2020-01-23 10:00:00'), Timestamp('2020-01-23 11:00:00'), Timestamp('2020-01-23 12:00:00'), Timestamp('2020-01-23 13:00:00'), Timestamp('2020-01-23 15:00:00'), Timestamp('2020-01-23 16:00:00'), Timestamp('2020-01-23 17:00:00'), Timestamp('2020-01-23 18:00:00'), Timestamp('2020-01-23 20:00:00'), Timestamp('2020-01-23 21:00:00'), Timestamp('2020-01-23 22:00:00'), Timestamp('2020-01-23 23:00:00'), Timestamp('2020-01-24 00:00:00'), Timestamp('2020-01-24 01:00:00'), Timestamp('2020-01-24 02:00:00'), Timestamp('2020-01-24 03:00:00'), Timestamp('2020-02-12 10:00:00'), Timestamp('2020-02-12 11:00:00'), Timestamp('2020-02-12 12:00:00'), Timestamp('2020-02-12 13:00:00'), Timestamp('2020-02-12 14:00:00'), Timestamp('2020-02-12 19:00:00'), Timestamp('2020-02-12 20:00:00'), Timestamp('2020-02-12 22:00:00'), Timestamp('2020-02-12 23:00:00'), Timestamp('2020-02-13 20:00:00'), Timestamp('2020-02-14 00:00:00'), Timestamp('2020-02-14 01:00:00'), Timestamp('2020-02-15 10:00:00'), Timestamp('2020-02-19 08:00:00'), Timestamp('2020-02-19 09:00:00'), Timestamp('2020-02-19 10:00:00'), Timestamp('2020-02-25 02:00:00'), Timestamp('2020-02-25 03:00:00'), Timestamp('2020-03-09 07:00:00'), Timestamp('2020-03-18 21:00:00'), Timestamp('2020-03-18 22:00:00'), Timestamp('2020-03-19 01:00:00'), Timestamp('2020-03-20 04:00:00'), Timestamp('2020-03-21 09:00:00'), Timestamp('2020-03-21 10:00:00'), Timestamp('2020-03-28 22:00:00'), Timestamp('2020-04-15 03:00:00'), Timestamp('2020-04-28 03:00:00'), Timestamp('2020-04-28 04:00:00'), Timestamp('2020-05-01 13:00:00'), Timestamp('2020-05-01 15:00:00'), Timestamp('2020-05-01 23:00:00'), Timestamp('2020-05-02 00:00:00'), Timestamp('2020-11-17 14:00:00'), Timestamp('2020-11-17 20:00:00'), Timestamp('2020-11-17 21:00:00'), Timestamp('2020-11-17 22:00:00'), Timestamp('2020-11-18 19:00:00'), Timestamp('2020-11-18 20:00:00'), Timestamp('2020-11-18 23:00:00'), Timestamp('2020-11-19 00:00:00'), Timestamp('2020-11-19 01:00:00'), Timestamp('2020-12-21 15:00:00'), Timestamp('2020-12-27 14:00:00'), Timestamp('2020-12-27 15:00:00'), Timestamp('2020-12-27 16:00:00'), Timestamp('2020-12-27 21:00:00'), Timestamp('2021-01-16 09:00:00'), Timestamp('2021-01-16 10:00:00'), Timestamp('2021-01-16 11:00:00'), Timestamp('2021-02-01 10:00:00'), Timestamp('2021-02-03 09:00:00'), Timestamp('2021-02-03 10:00:00'), Timestamp('2021-02-06 11:00:00'), Timestamp('2021-02-06 17:00:00'), Timestamp('2021-02-08 11:00:00'), Timestamp('2021-02-11 14:00:00'), Timestamp('2021-02-25 22:00:00'), Timestamp('2021-03-12 08:00:00'), Timestamp('2021-03-19 15:00:00'), Timestamp('2021-03-19 20:00:00'), Timestamp('2021-03-29 13:00:00'), Timestamp('2021-04-06 07:00:00'), Timestamp('2021-04-12 15:00:00'), Timestamp('2021-04-13 16:00:00'), Timestamp('2021-11-04 14:00:00'), Timestamp('2021-11-04 15:00:00'), Timestamp('2021-11-04 23:00:00'), Timestamp('2021-11-05 00:00:00'), Timestamp('2021-11-05 01:00:00'), Timestamp('2021-11-05 05:00:00'), Timestamp('2021-11-05 06:00:00'), Timestamp('2021-11-05 11:00:00'), Timestamp('2021-11-05 15:00:00'), Timestamp('2021-11-28 15:00:00'), Timestamp('2021-11-29 10:00:00'), Timestamp('2021-12-21 11:00:00')]] 最後我們把偵測出來的離群值從原始資料中刪除，然後再次做圖並與一開始的折線圖進行比較，便可以很清楚地發現一些異常值（例如 2022-07 有一個異常的高峰) 都被移除了。\noutliers_removed = outlierDetection.remover(interpolate=False) outliers_removed outliers_removed.plot(cols=['y_0']) 改變點偵測 (Change point detection) 改變點是資料中突然發生重大改變的時間點，代表的是事件的發生、資料狀態的轉變或資料分布的轉變，因此改變點偵測也常被視為資料分析與資料預測的重要前處理步驟。\n在以下的範例中，我們使用空品資料的日平均值來進行改變點偵測，同時也使用 kats 套件的 TimeSeriesData 資料格式來儲存資料，並使用 kats 提供的 CUSUMDetector 偵測器來進行偵測，並在作圖中用紅色的點來代表偵測到的改變點。很不湊巧的是，在本次的範例中一如肉眼觀察的結果，並無明顯的改變點存在，建議讀者可參考這次的範例，帶入其他的資料做更多的演練與偵測。\nair_ts = TimeSeriesData(air_day.reset_index(), time_col_name='timestamp') detector = CUSUMDetector(air_ts) change_points = detector.detector(change_directions=[\"increase\", \"decrease\"]) # print(\"The change point is on\", change_points[0][0].start_time) # plot the results plt.xticks(rotation=45) detector.plot(change_points) plt.show() 缺失資料 (missing data) 處理 在我們進行資料分析時，常常不免會遇上缺失資料的問題，這些缺失資料有的是在資料收取時便已經缺失 （例如感測器故障、網路斷線等原因），有些則是在資料前處理時不得不把某些資料刪除（離群值或明顯異常值），但對於後續的資料處理與分析而言，我們又往往需要資料能維持固定的採樣率，以方便各項方法工具的套用，因此便衍生出各種不同的缺失資料填值方法。以下我們介紹三種常見的方法：\n將該筆缺失資料標示為 Nan (Not a number)：Nan 代表非數，用來表示未定義或不可表示的值，如果已知後續的資料分析會額外處理這些 Nan 的特例，便可採用此方法以維護資料的真實性。 Forward fill 法：如果 Nan 對後續的資料分析有困難，必須將缺失的值填補適當的數值資料，最簡單的方法就是 forward fill，亦即用前一個數值來填補當下的缺失值。 # forward fill df_ffill = air.ffill(inplace=False) df_ffill.plot() 3. K-Nearest Neighbor (KNN) 法：顧名思義，KNN 的方法是尋找距離缺失值最近的 k 個數值進行平均，並用來填補這個缺失值。\ndef knn_mean(ts, n): out = np.copy(ts) for i, val in enumerate(ts): if np.isnan(val): n_by_2 = np.ceil(n/2) lower = np.max([0, int(i-n_by_2)]) upper = np.min([len(ts)+1, int(i+n_by_2)]) ts_near = np.concatenate([ts[lower:i], ts[i:upper]]) out[i] = np.nanmean(ts_near) return out # KNN df_knn = air.copy() df_knn['PM25'] = knn_mean(air.PM25.to_numpy(), 5000) df_knn.plot() 資料分解(Decomposition) 在前面的基本資料處理範例中，我們已能約略觀察資料數值的變化趨勢，並且發現可能的規律變化，為了能更近一步深入探討時序資料的變化規律性，我們接著介紹時序資料所常使用的資料分解分法，將原本的時序資料拆解成趨勢波 (trend)、週期波 (seasonal) 及殘差波(residual)。\n我們首先將空品資料的每日平均資料另外複製一份為 air_process，並採用 forward fill 方法進行缺失資料的處理，然後把原始資料直接用圖表的方式呈現。\nair_process = air_day.copy() # new.round(1).head(12) air_process.ffill(inplace=True) air_process.plot() 接著我們使用 seasonal_decompose 方法對 air_process 資料進行分解，其中我們需要設定一個 period 參數，指的是資料被拆解的週期，我們先設定為 30天，接著在執行後便會依序產出四張圖：原始資料、趨勢圖、週期性圖與殘差圖。\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=30) decompose.plot().set_size_inches((15, 15)) plt.show() 在趨勢圖 (trend) 中，我們也可以發現與原始資料的圖表有著十分雷同的特性，在一月附近的數值較高，七月附近的數值則較低；而在週期性圖 (Seasonal) 中，我們可以發現資料在每個週期 (30天) 內存在固定的週期變化，代表空品資料存在著以月為週期的變動。\n倘若我們將 period 變數改為 365，亦即以較大的時間尺度 (一年) 來進行資料分解，我們可以從週期性圖中發現一月附近數值較高，而七月附近數值較低的趨勢，而且這個趨勢變化是規律地週期性發生的；同時，在趨勢圖中也可以看到整體緩降的趨勢，說明 PM2.5 的濃度在大趨勢下是逐漸好轉降低的，從這邊也可以理解到為什麼在尋找 change point 時無法成功，因為 PM2.5 的趨勢變化是平順的遞減，並沒有發生突如其來的變化。\ndecompose = seasonal_decompose(air_process['PM25'],model='additive', period=365) decompose.plot().set_size_inches((15, 15)) plt.show() 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Matplotlib - Colormap reference (https://matplotlib.org/stable/gallery/color/colormap_reference.html) Decomposition of time series - Wikipedia (https://en.wikipedia.org/wiki/Decomposition_of_time_series) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057?gi=36d1c3d8372) Decomposition in Time Series Data | by Abhilasha Chourasia | Analytics Vidhya | Medium (https://medium.com/analytics-vidhya/decomposition-in-time-series-data-b20764946d63) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，引導讀者了解移動式平均 (Moving Average) 的使用方法，以及進行時序資料的週期性分析，並進而將時序資料拆解出長期趨勢、季節變動、循環變動與殘差波動，同時套用既有的 Python 語言套件，進行變點檢測 (Change Point Detection) 與異常值檢測 (Outlier Detection)，用以檢視現有民生公共物聯網資料，並探討其背後所可能隱含的意義。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "4.1. 時間序列資料處理",
    "uri": "/ch4/ch4.1/"
  },
  {
    "content": "\nTable Of Contents 交集 (Intersect) 緩衝區 (Buffer) 多重緩衝區 (Multi-ring buffer) 距離矩陣 (Distance Matrix) 小結 民生公共物聯網的測站都有其空間位置，由於同一區域內常有類似的環境因子，所以它們的感測數值也會具有類似的起伏趨勢，而這也就是地理學的第一定律：“All things are related, but nearby things are more related than distant things.” (Waldo R. Tobler)\n此外，單一測站的感測數據有可能因為局部干擾因子的影響，而產生較大的起伏，所以為進一步確認數據的可信度，我們會需要以單一測站為中心，依照其所屬的行政區或是指定距離 (半徑)，選取鄰近的測站ID與數值，並將其以表單或地圖的方式呈現，以便進行比對。\n這個章節中，我們會利用環保署的空品測站、氣象局的局屬測站以及水利署在各縣市佈建的淹水感測器，示範如何利用地理空間篩選測站，並以其「位置」與「數值」為基礎，轉化成可利用的空間資訊。\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # 前往政府開放資料庫下載 縣市界線(TWD97經緯度) 的資料，並解壓縮到名為 shp 的資料夾 !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # 以水利署淹水感測器資料為例，其中，資料集 gpd 為感測器數值與位置資料、basemap 為 county.shp (台灣縣市邊界) # 以pyCIOT取得資料 wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") wea_list = CIoT.Weather().get_station('GENERAL:CWB') county = gpd.read_file('county.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] # 整理資料並轉成 geopandas.GeoDataFrame 格式 flood_list = wa + wa2 flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") weather_df = pd.DataFrame([],columns = ['name','lon', 'lat']) for i in wea_list: #print(i['data'][0]) df = pd.DataFrame([[i['name'],i['location']['longitude'],i['location']['latitude']]],columns = ['name','lon', 'lat']) weather_df = pd.concat([weather_df,df]) #print(df) result_df = weather_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) gdf_weather = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") 交集 (Intersect) 一般而言，手邊有很多的測站點位時，我們可以利用村里或鄉鎮等行政區界為範圍，利用資料的在空間上的交集 (intersect) 以篩選出特定行政區內的測站ID，並以API擷取這些測站的：瞬時值、小時平均值、日平均值、週平均值，這樣我們就可以檢視同一行政區內的測站，是否有類似的數值趨勢，抑或哪些測站的數值與其他測站有明顯的差異。\nimport matplotlib.pyplot as plt import seaborn as sns fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=gdf_weather) # this is plotting the datapoints from the EPA dataframe basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); # plotting the city's boundaries here, with facecolor = none to # remove the polygon's fill color plt.tight_layout(); basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_weather, basemap, how='intersection') # selecting the polygon's geometry field to filter out points that # are not overlaid # 資料剩下感測器與地理邊界交集重疊的點位 fig, ax = plt.subplots(figsize=(6, 10)) ax = sns.scatterplot(x='lon', y='lat', data=intersected_data) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); 緩衝區 (Buffer) 此外，部分測站也有可能是位在兩個行政界的邊界處，所以用行政界當成選取的標準，就有可能出現偏誤。所以這種狀況下，我們可以利用緩衝區 (buffer) 的概念，以測站的座標位置為中心，並指定一個距離半徑以建立一個虛擬的圓形 (圖1)，並以此範圍去查找有幾何相交的測站位置。\n當我們掌握了緩衝區的概念後，我們也可以某個地標 (例如：學校、公園、工廠…）標為中心，去查找鄰近的測站。甚至，我們可以用一個線段 (Line 例如：道路、河流…) 或一個區域 (polygon 例如：公園、工業區)，去建立一個查找範圍，以更具體地找出自己想要的測站點位。在這個範例中，我們以氣象局的局屬氣象站為中心，去建立緩衝區。\n# 設定buffer 緩衝帶邊界距離 （此處設定0.05，因為x,y 為經緯度，1單位距離約100km） fig, ax = plt.subplots(figsize=(6, 10)) buffer = intersected_data.buffer(0.05) buffer.plot(ax=ax, alpha=0.5) intersected_data.plot(ax=ax, color='red', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); 多重緩衝區 (Multi-ring buffer) 當然，我們也可以設定不同的距離半徑 ，進而把緩衝區畫成多個同心圓 (圖2)，這樣就可以利用不同的遠近/階層，以將鄰近的測站分組，從而檢視越鄰近的測站是否有越相近的數值趨勢。在這個案例中，我們可以用氣象局的局屬氣象站去建立多重緩衝區，並檢視不同的緩衝區內的水利署淹水感測器，從而探勘：雨量、水平距離與淹水高度之間的關係。\n# 利用不同的經緯度作為半徑以建立多重緩衝區，並依照 0.05度=藍色；0.1度=綠色；0.2度=橘色；0.3=紅色 進行顏色設定 fig, ax = plt.subplots(figsize=(6, 10)) buffer_03 = intersected_data.buffer(0.3) buffer_03.plot(ax=ax, color='red', alpha=1) buffer_02 = intersected_data.buffer(0.2) buffer_02.plot(ax=ax, color='orange', alpha=1) buffer_01 = intersected_data.buffer(0.1) buffer_01.plot(ax=ax, color='green', alpha=1) buffer_005 = intersected_data.buffer(0.05) buffer_005.plot(ax=ax, alpha=1) intersected_data.plot(ax=ax, color='black', alpha=0.5) # 用最大的 buffer 跟淹水感測器 intersect buffer = gpd.GeoDataFrame(buffer_03,geometry=buffer_03) buffer = buffer.to_crs(4326) intersected_flood = gpd.overlay(gdf_flood, buffer, how='intersection') intersected_flood.plot(ax=ax, color='lightgray', alpha=0.5) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); plt.tight_layout(); 距離矩陣 (Distance Matrix) 最後，因為每一個測站都有具體的座標數值，所以我們也可以三角函數結合座標數值，取得兩兩測站間的絕對距離，從而建立所有測站間的距離矩陣 (distance matrix)，而這樣的矩陣可以協助我們快速檢視兩兩測站間的距離關係，並從中確認測站是否具有鄰近性，從而以更一步確認感測間的數值趨勢是否與其距離具有相關性 (圖3)。在這個範例中，我們可以利用台北市內的淹水感測器位置，並將其轉換成為距離矩陣，以協助我們掌握淹水是否具有鄰近關係。\n# 製作每個感測器位置之間的距離矩陣，首先將資料整理成座標資料 gdf_weather[\"ID\"] = gdf_weather.index df = pd.DataFrame(gdf_weather, columns=['ID','lon', 'lat']) df.set_index('ID') # 計算感測器位置彼此的距離，並轉化為距離矩陣 from scipy.spatial import distance_matrix pd.DataFrame(distance_matrix(df.values, df.values), index=df.index, columns=df.index) 小結 透過前述的方式，我們可以從地理空間 (行政區或鄰近性) 進行測站的篩選機制，並從而能從地理空間的特性檢視測站間的數值相關性。\nReferences Geopanda Documentation (https://geopandas.org/en/stable/docs.html) Shpfile 格式介紹 (https://en.wikipedia.org/wiki/Shapefile) 臺灣的大地基準及座標系統 (https://wiki.osgeo.org/wiki/Taiwan_datums) WGS 84 (EPSG:4326) 參數介紹 (https://epsg.io/4326) TWD 97 (EPSG:3826) 參數介紹 (https://epsg.io/3826) ",
    "description": "我們使用民生公共物聯網資料平台的地震和防救災資料，套疊從政府開放資料平臺取得的行政區域界線圖資，篩選特定行政區域內的資料，以及產製套疊地圖後的資料分布位置圖片檔案。除此之外，我們同時示範如何套疊特定的幾何拓撲區域，並將套疊的成果輸出成檔案與進行繪圖動作。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "5.1. 地理空間篩選",
    "uri": "/ch5/ch5.1/"
  },
  {
    "content": "\nTable Of Contents 基本介紹 分類問題 (Classification) 分群問題 (Clustering) 套件安裝與引用 案例一：空品資料的場所型態分類 資料清理 移除離群值 區別訓練資料與測試資料 使用 Sklearn 預建模型 案例二：空品資料的分群 資料下載與前處理 動態時間校正 (Dynamic Time Warping, DTW) 使用 K-Mean 分群演算法 探究資料分群與地理位置的關係 探究資料分群與風場風向的關係 案例三：結合氣象與水資源資料的分類與分群 資料下載與前處理 計算特定淹水感測器與雨量測站資料的相似度 資料分群並探究相關性高的雨量站 資料分類並由雨量計資料預測淹水可能性 參考資料 基本介紹 我們在之前的章節中，已經介紹民生公共物聯網豐富的開放資料內容，同時介紹從時間維度與空間維度的角度出發，進行各種不同的資料分析與處理，在這個章節中，我們將開進一步初探機器學習的應用，介紹兩個經典的機器學習問題，分別是分類問題 (Classification) 與分群問題 (Clustering)。\n分類問題 (Classification) 分類問題是機器學習理論中的一個經典問題，若用比較數學的方式來描述這個問題，我們可以假設有一組已經分類好的數據 X，以及每一筆資料在分類之後所得到的標籤集合 Y，而分類問題就是希望能透過這組分類好的數據與標籤，建構一個有效的分類器 (Classifier)，可以將尚未分類的數據 X’，並找到其中每一筆資料相對應得標籤 Y’。\n因此，分類問題的重點，就是要構造一個有效的分類器 (Classifier)，為了達到這個目的，我們會先建立一個模型，利用已經標籤好 (labeled) 的數據進行訓練，並且讓這個模型盡可能地貼近與適應 (fit) 這些數據分佈狀態，再使用最後成品的模型做為分類器，用來推測未知數據的標籤。\n這個建立分類器的過程，在機器學習中被稱為監督式學習 (supervised learning)，而常見的分類器模型有 Nearest Neighbors、SVM Classifier、Decision Tree、*Random Forest *****等，在我們稍後的文章中，我們並不會針對每一種模型進行深入的講解，只會把這些模型直接拿來當工具使用，對於這些模型有興趣的讀者，可以參考相關資源，自行再做更深入的探究。\n分群問題 (Clustering) 分群問題和分類問題非常相似，主要差異在於分類問題是用已知的標籤數據資料推論未知的數據資料，而分群問題則是完全「無中生有」，從數據資料中自行歸類出不同的群組。\n若以比較數學的方式來描述這個問題，我們可以假設有一組完全沒有標記的數據 X，而分類問題就是希望能透過某種的演算法，將 X 的資料區分為 k 個群組，其中每一個群組內的資料彼此相似度大，而不同群組的資料彼此相異度大。\n因此，分群問題的演算法主要就是根據數據的特性，不斷地判斷資料間的相似與相異度，並且讓相似的資料群聚在一起，讓相異的資料在分佈中彼此互斥，而常見的分群演算法有 K-Means、DBSCAN、Hierarchical Clustering、BIRCH ******等，我們一樣不會針對每一種演算法進行深入的講解，只會把這些演算直接拿來當工具使用，對於這些模型有興趣的讀者，可以參考相關資源，自行再做更深入的探究。\n套件安裝與引用 在本章節中，我們除了使用 pyCIOT 套件獲取相關的民生公共物聯網開放資料外，將會使用到 pandas, numpy, matplotlib, json, os, glob, math, seaborn, warnings, tqdm, datetime, geopy, scipy 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 並未預先安裝好的套件：fastdtw 和 sklearn，以及為了讓輸出的圖形美觀而使用 TaipeiSansTCBeta-Regular 字型，需使用下列的方式進行安裝：\n!pip3 install fastdtw --quiet !pip3 install scikit-learn --quiet !pip3 install pyCIOT --quiet !wget -q -O TaipeiSansTCBeta-Regular.ttf https://drive.google.com/uc?id=1eGAsTN1HBpJAkeVM57_C7ccp7hbgSz3_\u0026export=download 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nfrom pyCIOT.data import * import json, os, glob, math import numpy as np import pandas as pd import matplotlib as mpl import matplotlib.pyplot as plt from matplotlib.font_manager import fontManager from tqdm import tqdm_notebook as tqdm from datetime import datetime, timedelta import seaborn as sns sns.set(font_scale=0.8) fontManager.addfont('TaipeiSansTCBeta-Regular.ttf') mpl.rc('font', family='Taipei Sans TC Beta') import warnings warnings.simplefilter(action='ignore') import geopy.distance from scipy.spatial.distance import euclidean from fastdtw import fastdtw import sklearn from sklearn.metrics import accuracy_score, confusion_matrix from sklearn.neural_network import MLPClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.svm import SVC from sklearn.gaussian_process.kernels import RBF from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn.cluster import KMeans 安裝與引用完畢後，我們接下來依照民生公共物聯網的空品資料與水資源資料兩個案例解析，介紹資料分類與分群。\n案例一：空品資料的場所型態分類 在這個案例中，我們使用民生公共物聯網資料平台中，環保署智慧城鄉空品微型感測器 (‘OBS:EPA_IoT’) 的資料，進行資料分類的示範。\n我們首先使用 pyCIOT 所提供的 Air().get_data() 方法，下載所有環保署智慧城鄉空品微型感測器的最近一筆感測資料。注意，由於內容數量龐大，因此這個步驟可能需要等多一點的時間。\nSource = 'OBS:EPA_IoT' Data = Air().get_data(src=Source) Data = [datapoint for datapoint in Data if len(datapoint['data']) == 3 and 'areaType' in datapoint['properties'].keys()] print(json.dumps(Data[0], indent=4, ensure_ascii=False)) { \"name\": \"智慧城鄉空品微型感測器-10287974676\", \"description\": \"智慧城鄉空品微型感測器-10287974676\", \"properties\": { \"city\": \"新北市\", \"areaType\": \"社區\", \"isMobile\": \"false\", \"township\": \"鶯歌區\", \"authority\": \"行政院環境保護署\", \"isDisplay\": \"true\", \"isOutdoor\": \"true\", \"stationID\": \"10287974676\", \"locationId\": \"TW040203A0507221\", \"Description\": \"廣域SAQ-210\", \"areaDescription\": \"鶯歌區\" }, \"data\": [ { \"name\": \"Relative humidity\", \"description\": \"相對溼度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 70.77 }, { \"name\": \"Temperature\", \"description\": \"溫度\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 33.78 }, { \"name\": \"PM2.5\", \"description\": \"細懸浮微粒 PM2.5\", \"timestamp\": \"2022-08-05T06:51:29.000Z\", \"value\": 9.09 } ], \"location\": { \"latitude\": 24.9507, \"longitude\": 121.3408416, \"address\": null } } 我們可以發現每一個感測器的資料中，都同時有溫度、相對濕度、細懸浮微粒濃度等感測資料，同時也記載了感測器的基本資訊，例如所在城市、所在鄉鎮、機器編號、位置編號、場所型態等。在我們這個範例中，我們將以「場所型態」為標籤資料，並搭配感測資料（溫度、相對濕度、細懸浮微粒濃度）進行分類器的訓練。我們首先觀察目前「場所型態」的內容狀態。\nLabel = list(dict.fromkeys([datapoint['properties']['areaType'] for datapoint in Data if datapoint['properties']['areaType']])) count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"Before data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) Before data cleaning, There are 8620 records. { \"社區\": 223, \"交通\": 190, \"一般社區\": 2021, \"工業\": 12, \"測站比對\": 66, \"工業區\": 3333, \"交通區\": 683, \"鄰近工業區社區\": 948, \"輔助區\": 165, \"特殊區(民眾陳情熱區)\": 143, \"特殊區(敏感族群聚集區)\": 200, \"特殊區(測站比對)\": 32, \"輔助區(無測站區)\": 4, \"工業感測\": 196, \"特殊感測\": 4, \"輔助感測\": 295, \"交通感測\": 102, \"機動感測\": 2, \"社區感測\": 1 } 資料清理 由於目前資料中共有 8620 筆資料散佈在 19 種場所型態中，為了符合資料的意義，我們首先將類似的場所型態進行合併，同時也為了讓分類器的示範能更順利，我們只專注在一般社區、交通區、工業區、鄰近工業區社區等四大場所型態。因此，我們用下列的程式進行資料的重新整理：\nfor datapoint in Data: if datapoint['properties']['areaType'] == '社區': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '社區感測': datapoint['properties']['areaType'] = '一般社區' elif datapoint['properties']['areaType'] == '交通': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '交通感測': datapoint['properties']['areaType'] = '交通區' elif datapoint['properties']['areaType'] == '工業': datapoint['properties']['areaType'] = '工業區' elif datapoint['properties']['areaType'] == '工業感測': datapoint['properties']['areaType'] = '工業區' if not datapoint['properties']['areaType'] in ['一般社區', '交通區', '工業區', '鄰近工業區社區']: datapoint['properties']['areaType'] = None Data = [datapoint for datapoint in Data if datapoint['properties']['areaType'] != None] Label = ['一般社區', '交通區', '工業區', '鄰近工業區社區'] count = dict.fromkeys(Label, 0) for datapoint in Data: count[datapoint['properties']['areaType']] += 1 print(\"After data cleaning, There are {} records.\".format(len(Data))) print(json.dumps(count, indent=4, ensure_ascii=False)) After data cleaning, There are 7709 records. { \"一般社區\": 2245, \"交通區\": 975, \"工業區\": 3541, \"鄰近工業區社區\": 948 } 經過資料清理後，總共剩下 7709 筆資料，並且散佈在四大類的場所型態中。針對這些資料，我們接著考慮每筆資料的溫度、相對濕度、細懸浮微粒濃度感測值，並用不同的顏色代表不同場所型態的資料，繪製成一張三維的資料分佈圖。\nDataX, DataY = [], [] for datapoint in Data: TmpX = [None]*3 TmpY = None for rawdata_array in datapoint['data']: if(rawdata_array['name'] == 'Temperature'): TmpX[0] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'Relative humidity'): TmpX[1] = rawdata_array['values'][0].get('value') if(rawdata_array['name'] == 'PM2.5'): TmpX[2] = rawdata_array['values'][0].get('value') TmpY = Label.index(datapoint['properties']['areaType']) DataX.append(TmpX) DataY.append(TmpY) DataX_Numpy = np.array(DataX) DataY_Numpy = np.array(DataY) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature') ax.set_ylabel('Relative humidity') ax.set_zlabel('PM2.5') plt.show() 從資料分布圖中，我們可以發現絕大多數的資料都群聚在某一個特定的空間內，但仍有少數的資料零散地分布在很外圍的地方，這些遠離群體的資料我們稱為離群值 (Outlier)，對於資料分類或資料分群而言，離群值很容易導致我們的模型或演算法走向極端化，因而失去其通用性，因此我們需要先將這些資料予以移除。\n移除離群值 移除離群值的方法，不外乎運用一些資料的統計特徵，可以依據不同的應用情境需求自行定義。在我們的範例中，我們定義如果一筆資料在其中一種感測資料的數值，距離平均值超過兩個標準差以上，該筆資料便歸類為離群值。我們將這些離群值的資料移除後，照例繪製一張三維空間的分佈圖觀察其分布狀況。\ndef Outlier_Filter(arr, k): Boolean_Arr = np.ones(arr.shape[0], dtype=bool) for i in range(arr.shape[1]): Boolean_Arr = Boolean_Arr \u0026 (abs(arr[:,i] - np.mean(arr[:,i])) \u003c k*np.std(arr[:,i])) return Boolean_Arr OutlierFilter = Outlier_Filter(DataX_Numpy, 2) DataX_Numpy = DataX_Numpy[OutlierFilter] DataY_Numpy = DataY_Numpy[OutlierFilter] print(\"After removing Outliers, there are {} records left.\".format(DataX_Numpy.shape[0])) plt.rc('legend',fontsize=\"xx-small\") fig = plt.figure(figsize=(8, 6), dpi=150) ax = fig.add_subplot(projection='3d') for i in range(len(Label)): ax.scatter(DataX_Numpy[DataY_Numpy==i][:,0],DataX_Numpy[DataY_Numpy==i][:,1],DataX_Numpy[DataY_Numpy==i][:,2], s=0.1, label=Label[i]) ax.legend() ax.set_xlabel('Temperature') ax.set_ylabel('Relative humidity') ax.set_zlabel('PM2.5') plt.show() After removing Outliers, there are 7161 records left. 從最後呈現的結果中，我們共移除了 7709 - 7161 = 548 筆離群值資料，而最後留下的資料在三維空間的分佈也較為集中，不再有偏移在外圍的狀況發生。為了更容易觀察，我們以每次挑選兩個維度的方式，分別繪製三張資料在不同維度間的分佈狀況。\nplt.rc('legend',fontsize=\"large\") fig, axes = plt.subplots(1,3,figsize=(24, 6)) for i in range(DataX_Numpy.shape[1]): for j in range(len(Label)): axes[i].scatter(DataX_Numpy[DataY_Numpy==j][:,i%3],DataX_Numpy[DataY_Numpy==j][:,(i+1)%3], s=1, label=Label[j]) axes[i].legend(loc=2) Axis_label = ['Temperature', 'Relative humidity', 'PM2.5'] axes[i].set_xlabel(Axis_label[i%3]) axes[i].set_ylabel(Axis_label[(i+1)%3]) plt.tight_layout() 從這三張圖中，我們已可發現不同顏色（場所型態）的資料隱約中似乎存在某種關係，雖然用肉眼很難直接敘明，但接下來我們將帶入分類模型來建構專屬的分類器。\n區別訓練資料與測試資料 在進入分類器的模型訓練前，我們還有一個步驟需要處理，那就是拆分數據，將現有的資料集分為訓練資料和測試資料。顧名思義，訓練資料將被用於調校分類器的模型，而測試資料則是用來測試所建構出來的分類器，在處理新資料時的效果。我們使用下列的範例程式，將資料集按照 4:1 的比例，切割成訓練資料與測試資料。\nindices = np.random.permutation(DataX_Numpy.shape[0]) Train_idx, Test_idx = indices[:int(DataX_Numpy.shape[0]*0.8)], indices[80:(DataX_Numpy.shape[0] - int(DataX_Numpy.shape[0]*0.8))] TrainX, TestX = DataX_Numpy[Train_idx,:], DataX_Numpy[Test_idx,:] TrainY, TestY = DataY_Numpy[Train_idx], DataY_Numpy[Test_idx] 使用 Sklearn 預建模型 我們直接使用 Scikit learn (sklearn) 這個 Python 套件所提供的分類器模型來進行訓練與測試，在系列的程式範例中，我們總共使用 Nearest neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, Neural Net, Adaboost, Naive Bayes, QDA 共計九種模型，我們依次帶入訓練資料進行調校後，接著帶入測試資料進行預測，並且將測試資料與預測結果中的標籤內容進行比對，並用混淆矩陣 (confusion matrix) 的方式，呈現不同標籤組合的分類結果。\nclassifier_names = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(TrainX, TrainY) Result = model.predict(TestX) mat = confusion_matrix(TestY, Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=Label, yticklabels=Label, ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(classifier_names[i], round(accuracy_score(Result, TestY), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() 在這九種分類模型的分類結果中，我們發現 RBF SVM 可以達到將近 7成的分類成功率，事實上，這只是我們使用原始資料尚未進行更進一步處理分析的結果，讀者們若對於分類器有興趣，可以參考相關資源，進行更深入的探究，將能更加提升分類器對不同種類資料的分類能力。\n案例二：空品資料的分群 在這個案例中，我們使用民生公共物聯網開放資料中的環保署國家空品測站的感測資料，並且透過歷史資料的分析，利用資料分群的方式，將這些空品測站依照感測資料的變化趨勢關係予以分群，讓每一群的測站具有類似的感測資料變化趨勢。\n資料下載與前處理 我們使用下列的程式碼，從民生公共物聯網開放資料平台的歷史資料庫中下載 2021 年環保署國家空品測站的所有感測資料，並將下載回來的壓縮檔解開後，置於 /content 的目錄下。\n!wget -O 'EPA_OD_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FnkrDkv53nvbJf5ZyL5a6256m65ZOB5ris56uZL0VQQV9PRF8yMDIxLnppcA%3D%3D\" !unzip -q 'EPA_OD_2021.zip' \u0026\u0026 rm 'EPA_OD_2021.zip' !unzip -q '/content/EPA_OD_2021/EPA_OD_202112.zip' -d '/content' !rm -rf '/content/EPA_OD_2021' 我們先選用 2021 年 12月的資料，先把不需要的欄位 Pollutant、SiteId、Status、SO2_AVG 刪除，並且將感測資料的數值資料型態改為浮點數，以利後續的處理。\nDataframe = pd.read_csv(\"/content/EPA_OD_202112.csv\", parse_dates=['PublishTime']) Dataframe = Dataframe.drop(columns=[\"Pollutant\", \"SiteId\", \"Status\", \"SO2_AVG\"]) Numerical_ColumnNames = list(Dataframe.columns.values) for ColumnName in ['SiteName', 'County', 'PublishTime']: Numerical_ColumnNames.remove(ColumnName) for Numerical_ColumnName in Numerical_ColumnNames: Dataframe[Numerical_ColumnName] = pd.to_numeric(Dataframe[Numerical_ColumnName], errors='coerce').astype('float64') Dataframe = Dataframe.dropna() Dataframe.head() 由於一個月的資料量十分龐大，為了精簡範例程式的執行時間，我們抽取其中 2021-12-13 至 2021-12-17 共計五天的資料 (FiveDay_Dataframe) 作為接下來的範例，並且把 Country 與 SiteName 兩個欄位合併，同時根據合併後的欄位和資料發佈時間進行排序。\nFiveDay_Dataframe = Dataframe.loc[(Dataframe['PublishTime'] \u003c= '2021-12-17 23:00:00') \u0026 (Dataframe['PublishTime'] \u003e= '2021-12-13 00:00:00')] FiveDay_Dataframe['CountyAndSiteName'] = FiveDay_Dataframe['County'] + FiveDay_Dataframe['SiteName'] FiveDay_Dataframe = FiveDay_Dataframe.drop(columns=[\"County\", \"SiteName\"]) FiveDay_Dataframe = FiveDay_Dataframe.sort_values(by=['CountyAndSiteName','PublishTime']) FiveDay_Dataframe = FiveDay_Dataframe.set_index(keys = ['CountyAndSiteName']) FiveDay_Dataframe 動態時間校正 (Dynamic Time Warping, DTW) 接下來，我們必須判斷兩個測站之間的「相似度」，並且將其量化成一個數字。最基本的相似度量測方式，是直接將兩測站的資料，按照感測時間對齊後，計算兩兩之間空氣汙染物質感測數據的差距；但是，若考量測站資料具備時間序列的特性，空氣污染在各個測站間可能發生的順序不一，影響的時間長度亦不一定相同，需要更有彈性的推估兩個測站之間的相似度，因此我們先扣除掉資料中的風向、感測時間、經緯度等資訊後，選用動態時間校正 (DTW) 方法進行相似度量測，若兩個測站資料的 DTW 距離越小，代表兩者的相似度越高。\nSite_TimeSeriesData = dict() for Site in np.unique(FiveDay_Dataframe.index.values): tmp = FiveDay_Dataframe[FiveDay_Dataframe.index == Site] tmp = tmp.groupby(['CountyAndSiteName', 'PublishTime'], as_index=False).mean() tmp = tmp.loc[:,~tmp.columns.isin(['CountyAndSiteName', 'PublishTime'])] Site_TimeSeriesData[Site] = tmp.to_numpy() DictKeys = Site_TimeSeriesData.keys() Sites_DTW = dict() for i, key1 in enumerate(DictKeys): for j, key2 in enumerate(DictKeys): if i \u003e= j: continue else: # 計算扣除風向、感測時間、經度、緯度後資料的 DTW 距離 Sites_DTW[str(key1)+\" \"+str(key2)] = fastdtw(Site_TimeSeriesData[key1][:,:-4], Site_TimeSeriesData[key2][:,:-4], dist=euclidean)[0] Sites_DTW_keys = np.array(list(Sites_DTW.keys())) Site_DTW_Numpy = np.array([[value] for _, value in Sites_DTW.items()]) 我們將所有測站兩兩間的 DTW 距離畫在數線上，可以得到下方的圖形；其中 DTW 距離從小到大皆有，若要進一步處理，便需要開始使用分群演算法來分析。\nfig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) ax.scatter(Site_DTW_Numpy[:,0], [1]*len(Sites_DTW.items()), s=0.05) ax.get_yaxis().set_visible(False) 使用 K-Mean 分群演算法 我們使用 sklearn 套件中的 K-Means 模組來進行資料分群，由於分群演算法需事先設定最後要產生的群組數目，我們先設定為 3。我們使用下列的程式碼進行分群，並將結果依照資料所歸屬的群組組別 (Y 軸) 以及與其他資料的 DTW 相似度值 (X 軸) 繪製出來。\nfrom sklearn.cluster import KMeans model = KMeans(n_clusters=3, random_state=0).fit([[value] for _, value in Sites_DTW.items()]) Result = model.labels_ for i in np.unique(Result): print(\"Number of Cluster{} : {}\".format(i,len(Result[Result==i]))) # 將結果繪圖呈現 fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): ax.scatter(Site_DTW_Numpy[Result==i][:,0],[i]*len(Site_DTW_Numpy[Result==i]), s=0.05) ax.get_yaxis().set_visible(False) Number of Cluster0 : 1165 Number of Cluster1 : 994 Number of Cluster2 : 542 從分群結果中，我們可以看到 K-Means 演算法將原始資料分為三個群組，分別有 1165、994、542 筆資料，為了更進一步了解這三個群組的成因，我們繼續追查每個群組形成的可能原因。\n探究資料分群與地理位置的關係 我們首先假設空氣品質的變化具有地域性，因此探究資料分群的結果，是否和空品測站的地理位置有關。我們首先擷取測站的 GPS 經緯度座標，並換算出兩倆地理位置的實體距離，接著按照資料分群的結果，把不同群組的實體距離進行簡單的統計分析，並繪製在下方圖片中。\nDist_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Dist_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') # 獲取兩個測站的經緯度座標 coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] # 計算兩個測站的實體位置距離 Dist_for_Cluster.append(geopy.distance.geodesic(coord1, coord2).km) Dist_for_Cluster = np.array(Dist_for_Cluster) Dist_for_Clusters[i] = Dist_for_Cluster Dist_for_Clusters = np.array(Dist_for_Clusters) # for Dist_for_Cluster in Dist_for_Clusters: # print(np.mean(Dist_for_Cluster)) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): gtMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003enp.mean(Dist_for_Clusters[i])] ltMean = Dist_for_Clusters[i][Dist_for_Clusters[i]\u003cnp.mean(Dist_for_Clusters[i])] print(\"Mean Distance between Sites for Cluster{} : {}\".format(i, np.mean(Dist_for_Clusters[i]))) print(\"In Cluster{} there are {:.2%} less than mean, and {:.2%} greater than mean.\\n\".format(i, len(ltMean)/len(Dist_for_Clusters[i]), len(gtMean)/len(Dist_for_Clusters[i]))) ax.scatter(gtMean, [i]*len(gtMean), s=0.05, color=\"orange\") ax.scatter(ltMean, [i]*len(ltMean), s=0.05, color=\"pink\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=0.5) ax.get_yaxis().set_visible(False) Mean Distance between Sites for Cluster0 : 84.34126465234523 In Cluster0 there are 60.09% less than mean, and 39.91% greater than mean. Mean Distance between Sites for Cluster1 : 180.26230465399215 In Cluster1 there are 54.53% less than mean, and 45.47% greater than mean. Mean Distance between Sites for Cluster2 : 234.89206124762546 In Cluster2 there are 39.48% less than mean, and 60.52% greater than mean. 從分析的結果中，我們可以發現對於 DTW 數值較高（時間序列相似度較低）的群組，其測站間的距離平均值亦較高，反之則距離平均值較低，由此可知測站資料的相似度確實與地理位置的差異有關，也應證了我們的假設，確認了空氣污染物的擴散確實會受到地理位置距離的影響。\n探究資料分群與風場風向的關係 我們接著假設空氣品質的變化受到環境風場的影響，因此探究資料分群的結果，是否和空品測站所在地的風向有關。我們首先擷取測站的 GPS 經緯度座標，並換算出兩倆地理位置的方位角關係，接著按照資料分群的結果，根據地理位置方位角與現場風向計算兩者的相關性，並將所獲得的數值進行簡單的統計分析，再繪製於下圖。\ndef get_bearing(lat1, long1, lat2, long2): dLon = (long2 - long1) x = math.cos(math.radians(lat2)) * math.sin(math.radians(dLon)) y = math.cos(math.radians(lat1)) * math.sin(math.radians(lat2)) - math.sin(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.cos(math.radians(dLon)) brng = np.arctan2(x,y) brng = np.degrees(brng) return brng def Check_Wind_Dirc(brng, wind_dirc): if brng \u003e 180: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng - 180 \u003c wind_dirc + 45) and (brng - 180 \u003e wind_dirc - 45)) else: return ((brng \u003c wind_dirc + 45) and (brng \u003e wind_dirc - 45)) or ((brng + 180 \u003c wind_dirc + 45) and (brng + 180 \u003e wind_dirc - 45)) Brng_for_Clusters = [None]*len(np.unique(Result)) Boolean_WindRelated_for_Clusters = [None]*len(np.unique(Result)) for i in np.unique(Result): Brng_for_Cluster = [] Boolean_WindRelated_for_Cluster = [] Cluster = Sites_DTW_keys[Result==i] for Sites in Cluster: Site1, Site2 = Sites.split(' ') coord1 = Site_TimeSeriesData[Site1][0,-1], Site_TimeSeriesData[Site1][0,-2] coord2 = Site_TimeSeriesData[Site2][0,-1], Site_TimeSeriesData[Site2][0,-2] Brng_Between_Site = get_bearing(coord1[0], coord1[1], coord2[0], coord2[1]) Brng_for_Cluster.append(Brng_Between_Site) MeanWindDirc1 = np.mean(Site_TimeSeriesData[Site1][:,-3]) MeanWindDirc2 = np.mean(Site_TimeSeriesData[Site2][:,-3]) Boolean_WindRelated_for_Cluster.append(Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc1) or Check_Wind_Dirc(Brng_Between_Site, MeanWindDirc2)) Brng_for_Cluster = np.array(Brng_for_Cluster) Boolean_WindRelated_for_Cluster = np.array(Boolean_WindRelated_for_Cluster) Boolean_WindRelated_for_Clusters[i] = Boolean_WindRelated_for_Cluster Brng_for_Clusters[i] = Brng_for_Cluster Brng_for_Clusters = np.array(Brng_for_Clusters) Boolean_WindRelated_for_Clusters = np.array(Boolean_WindRelated_for_Clusters) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) for i in np.unique(Result): print(\"Relevance for Cluster{} : {:.2%}\".format(i, len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True])/len(Dist_for_Clusters[i]))) ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == True]), s=2, color=\"green\") ax.scatter(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False],\\ [i]*len(Dist_for_Clusters[i][Boolean_WindRelated_for_Clusters[i] == False]), s=0.05, color=\"violet\") ax.axvline(np.mean(Dist_for_Clusters[i]), ymin = 0.45*i, ymax = 0.45*i+0.1, color = \"red\", linewidth=2) ax.get_yaxis().set_visible(False) Relevance for Cluster0 : 54.08% Relevance for Cluster1 : 39.24% Relevance for Cluster2 : 22.69% 從分析的結果中，我們可以發現對於 DTW 數值較低 （時間序列相似度較高）的群組，其測站間方位角與風向的相關性較高，反之則相關性較低，由此可知測站資料的相似度確實與環境風場風向有關，也印證了我們的假設，確認空氣污染物的擴散確實會受到環境風場風向的影響。\n案例三：結合氣象與水資源資料的分類與分群 在這個案例中，我們結合資料分群與資料分類同時進行演練。我們使用民生公共物聯網開放資料中的中央氣象局雨量站和水利署（與縣市政府合建）淹水感測器的感測資料，並且透過歷史資料的分析，利用資料分群的方式，尋找和降雨變化最為相關的河川水位站群組，並且利用資料分類的方式，在僅有雨量資料的狀況時，得以進行特定地區是否造成淹水的預測。\n資料下載與前處理 我們使用下列的程式碼，從民生公共物聯網開放資料平台的歷史資料庫中下載 2021 年中央氣象局雨量站和水利署（與縣市政府合建）淹水感測器的所有感測資料，並將下載回來的壓縮檔解開後，置於 /content 的目錄下。\n# 下載中央氣象局雨量站歷史資料 !wget -O 'Rain_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZLzIwMjEuemlw\" !wget -O 'Rain_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Zuo6YeP56uZL3JhaW5fc3RhdGlvbi5jc3Y%3D\" !unzip -q 'Rain_2021.zip' \u0026\u0026 rm 'Rain_2021.zip' !find '/content/2021' -name '*.zip' -exec unzip -q {} -d '/content/Rain_2021_csv' \\; !rm -rf '/content/2021' # 下載水利署（與縣市政府合建）淹水感測器資料 !wget -O 'Flood_2021.zip' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoLzIwMjEuemlw\" !wget -O 'Flood_Stataion.csv' -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbLvvIjoiIfnuKPluILmlL%2FlupzlkIjlu7rvvIlf5re55rC05oSf5ris5ZmoL3N0YXRpb25f5rC05Yip572y77yI6IiH57ij5biC5pS%2F5bqc5ZCI5bu677yJX%2Ba3ueawtOaEn%2Ba4rOWZqC5jc3Y%3D\" !unzip -q 'Flood_2021.zip' \u0026\u0026 rm 'Flood_2021.zip' !find '/content/2021' -name '*_QC.zip' -exec unzip -q {} -d '/content/Flood_2021_csv' \\; !rm -rf '/content/2021' 接下來我們先處理降雨量資料，將所下載的 2021 年所有測站的資料逐一讀取，並把接下來不會使用到的 MIN_10、HOUR_6、HOUR_12、NOW 欄位刪除，同時移除 11 月以後的資料後，彙整為 Rain_df 物件，同時讀入測站資訊成為 Rain_Station_df 物件。由於這個步驟所處理的資料量十分龐大，因此會花費較多的時間，請耐心等候。\ncsv_files = glob.glob(os.path.join(\"/content/Rain_2021_csv\", \"*.csv\")) csv_files.sort() Rain_df = pd.DataFrame() # 將所有資料逐一讀取，並把接下來不會使用到的 MIN_10、HOUR_6、HOUR_12、NOW 欄位刪除 for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['obsTime']) tmp_df.drop(['MIN_10','HOUR_6', 'HOUR_12', 'NOW'], axis=1, inplace=True) try: tmp_df = tmp_df.loc[tmp_df['obsTime'].dt.minute == 00] Rain_df = pd.concat([Rain_df, tmp_df]) except: print(csv_file) continue Rain_df = Rain_df.loc[Rain_df['obsTime'] \u003c \"2021-11-01 00:00:00\"] num = Rain_df._get_numeric_data() num[num \u003c 0] = 0 Rain_df.dropna(inplace=True) Rain_df.sort_values(by=['station_id','obsTime'], inplace=True) Rain_Station_df = pd.read_csv('/content/Rain_Stataion.csv') Rain_df 我們接下來處理淹水感測器的資料，將所下載的 2021 年所有測站的資料逐一讀取，並移除 11 月以後的資料，以及含有缺失值的資料後，將資料儲存為 Flood_df 物件，同時讀入測站資訊成為 Flood_Station_df 物件。這個步驟所處理的資料量一樣十分龐大，因此會花費較多的時間，請務必耐心等候。\ncsv_files = glob.glob(os.path.join(\"/content/Flood_2021_csv\", \"*_QC.csv\")) csv_files.sort() Flood_df = pd.DataFrame() # 將所有資料逐一讀取，並彙整成 Flood_df for csv_file in tqdm(csv_files): tmp_df = pd.read_csv(csv_file, parse_dates=['timestamp']) tmp_df = tmp_df.loc[(tmp_df['PQ_unit'] == 'cm')] Flood_df = pd.concat([Flood_df,tmp_df], axis=0, ignore_index=True) Flood_df = Flood_df.loc[Flood_df['timestamp'] \u003c \"2021-11-01 00:00:00\"] Flood_df.replace(-999.0,0.0, inplace=True) Flood_df.dropna(inplace=True) Flood_df.sort_values(by=['timestamp'], inplace=True) Flood_Station_df = pd.read_csv('/content/Flood_Stataion.csv') Flood_df 計算特定淹水感測器與雨量測站資料的相似度 由於淹水感測器的資料量十分龐大，因此我們先挑選一個位於雲林縣且編號為 43b2aec1-69b0-437b-b2a2-27c76a3949e8 的淹水感測器，將其資料取出後存在 Flood_Site_df 物件中，作為後續處理的範例。\nFlood_Site_df = Flood_df.loc[Flood_df['station_id'] == '43b2aec1-69b0-437b-b2a2-27c76a3949e8'] Flood_Site_df.head() 我們接著計算雨量站資料與選定的淹水感測器之間的相似度，我們一樣使用動態時間校正 (Dynamic Time Warping, DTW) 進行量測，由於 DTW 的值越小代表相似度越大，為了能更直觀表達相似度，我們在這個案例中，將相似度定義為 DTW 值的倒數，並且計算這個選定的淹水感測器與所有雨量站資料的相似度。\nFlood_Sensor_np = np.array([[v,v,v] for v in Flood_Site_df['value'].to_numpy()]) Site_dtw_Dist = dict() Rain_tmp_df = Rain_df.loc[(Rain_df['obsTime'].dt.hour == 1)] for Site in tqdm(np.unique(Rain_Station_df.loc[Rain_Station_df['city']=='臺南市']['station_id'].to_numpy())): tmp_df = Rain_tmp_df.loc[(Rain_tmp_df['station_id'] == Site)] if tmp_df.empty: continue tmp_np = tmp_df[['RAIN','HOUR_3','HOUR_24']].to_numpy() Site_dtw_Dist[Site] = (1/fastdtw(Flood_Sensor_np, tmp_np, dist=euclidean)[0]) Site_dtw_Dist = dict(sorted(Site_dtw_Dist.items(), key=lambda item: item[1])) print(json.dumps(Site_dtw_Dist, indent=4, ensure_ascii=False)) { \"88K590\": 4.580649481044748e-05, \"C0K560\": 4.744655320519647e-05, \"C0K490\": 4.79216996101994e-05, \"C0K520\": 5.038234963332513e-05, \"C0K420\": 5.0674082994877385e-05, \"C0K250\": 5.1021366345465985e-05, \"C0K280\": 5.118406054309105e-05, \"A0K420\": 5.1515699268157996e-05, \"C0K400\": 5.178763059243615e-05, \"O1J810\": 5.2282255279259976e-05, \"C0K240\": 5.2470804312991397e-05, \"01J960\": 5.334885670256585e-05, \"C0K470\": 5.438256498969844e-05, \"81K580\": 5.45854441959214e-05, \"C0K410\": 5.5066753408217084e-05, \"A2K570\": 5.520214274022887e-05, \"01J970\": 5.529887546186233e-05, \"C0K480\": 5.5374254960644355e-05, \"C0K460\": 5.5657892623955056e-05, \"72K220\": 5.5690175197363816e-05, \"C0K510\": 5.5742273217039165e-05, \"C1K540\": 5.618025674136218e-05, \"C0K550\": 5.621240903075098e-05, \"C0K450\": 5.62197509062689e-05, \"C0K291\": 5.6380522616008906e-05, \"C0K330\": 5.638960953991442e-05, \"C0K530\": 5.6525582441919285e-05, \"C0K500\": 5.6825555408648244e-05, \"C0K440\": 5.692254536595e-05, \"C0K430\": 5.697351917955081e-05, \"01J930\": 5.7648109890427854e-05, \"C0K580\": 5.770344946580767e-05, \"C0K390\": 5.782553930260475e-05, \"01J100\": 5.7933240408734325e-05, \"01K060\": 5.8343415644572526e-05 } 資料分群並探究相關性高的雨量站 如同前面的作法，我們藉由分群演算法來幫我們依照相似度的關係，將雨量計分為三個群組，並且找出其中相似度最高的群組，以及群組中的雨量計代碼。在我們的範例中，我們找到的三個群組各自有 9、23、3 個雨量計，同時第二個群組的時間序列資料與淹水感測器的資料相似度最高。\ncluster_model = KMeans(n_clusters=3).fit([[value] for _, value in Site_dtw_Dist.items()]) Result = cluster_model.labels_ for i in np.unique(Result): print(\"Number of Cluster {} : {}\".format(i,len(Result[Result==i]))) fig = plt.figure(figsize=(4, 3), dpi=150) ax = fig.add_subplot(1,1,1) Site_DTW_Numpy = np.array([value for _, value in Site_dtw_Dist.items()]) Site_Name_Numpy = np.array([key for key, _ in Site_dtw_Dist.items()]) Mean_Dis_For_Cluster = [None] * len(np.unique(Result)) for i in np.unique(Result): Mean_Dis_For_Cluster[i] = (np.mean(Site_DTW_Numpy[Result==i])) ax.scatter(Site_DTW_Numpy[Result==i],[i]*len(Site_DTW_Numpy[Result==i]), s=10) print(\"Mean Distance of Cluster {} : {}\".format(i,Mean_Dis_For_Cluster[i])) ax.get_yaxis().set_visible(False) Best_Cluster = np.where(Mean_Dis_For_Cluster == np.amax(Mean_Dis_For_Cluster))[0] Best_Site = Site_Name_Numpy[Result == Best_Cluster] print(Best_Site) Number of Cluster 0 : 9 Number of Cluster 1 : 23 Number of Cluster 2 : 3 Mean Distance of Cluster 0 : 5.1629678408018994e-05 Mean Distance of Cluster 1 : 5.6307994901628334e-05 Mean Distance of Cluster 2 : 4.7058249208614454e-05 ['C0K470' '81K580' 'C0K410' 'A2K570' '01J970' 'C0K480' 'C0K460' '72K220' 'C0K510' 'C1K540' 'C0K550' 'C0K450' 'C0K291' 'C0K330' 'C0K530' 'C0K500' 'C0K440' 'C0K430' '01J930' 'C0K580' 'C0K390' '01J100' '01K060'] 為了更進一步了解淹水感測器與這 23 個雨量計之間的相互關係，我們將這個選定的淹水感測器的感測資料按照時間順序繪製成圖。\ntmp= Flood_Site_df['value'].to_numpy() fig= plt.figure(figsize=(6, 3), dpi=150) ax= fig.add_subplot(1,1,1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) 接著我們將最相似群組的這 23 個雨量計的每小時降雨資料按照時間順序分別繪製成 23 張圖，由圖中所示，可以發現在淹水感測器出現高值時，雨量計的數值確實也都有增加的狀態發生，兩者的變化趨勢確實具備極高的相似度，符合我們的常理預期。\nfig = plt.figure(figsize=(8, 2*(len(Best_Site)//4+1)), dpi=150) for i, Site in enumerate(Best_Site): tmp = Rain_df.loc[Rain_df['station_id']==Site]['RAIN'] ax = fig.add_subplot(len(Best_Site)//4+1,4,i+1) ax.plot(range(len(tmp)),tmp, linewidth=0.5) 資料分類並由雨量計資料預測淹水可能性 接下來我們利用資料分類的方法，以所選定的淹水感測器所記載的淹水資料作為標籤，搭配最佳相似群組的雨量計，建構一個簡單的分類器，預測淹水感測器所在地是否發生淹水現象。我們將原有 2021 年 1 到 10 月的資料區分為前 7 個月的訓練資料和後 2 個月的測試資料，並將前七個月中淹水感測器中數值大於 0 的資料標記為淹水事件（註：本範例乃基於方便使用的原則，採取最寬鬆的標準將水位大於 0 的事件皆認定為淹水事件；然而，有關淹水事件的判定其實有更嚴謹的相關規定，在正式使用時，建議仍應遵照相關法規的規範判定），數值等於 ０的資料標記為無淹水事件，並將整理完畢的資料，儲存在訓練資料 Train_DataSet 物件中。\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003c \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=10*len(Flooding)).values Train_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Train_DataSet['x'].append(tmp_x) Train_DataSet['y'].append(0) 運用相同的方法，我們將 2021 年 8 到 10 月中淹水感測器中數值大於 0 的資料標記為淹水事件，數值等於 ０的資料標記為無淹水事件，並將整理完畢的資料，儲存在測試資料 Test_DataSet 物件中。\nFlooding = Flood_Site_df.loc[(Flood_Site_df['value'] \u003e 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']].values Not_Flooding = Flood_Site_df.loc[(Flood_Site_df['value'] == 0.0) \u0026 (Flood_Site_df['timestamp'] \u003e \"2021-08-01 00:00:00\")][['timestamp', 'value']]\\ .sample(n=2*len(Flooding)).values Test_DataSet = {'x':[], 'y':[]} for timestamp, _ in tqdm(Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) while len(tmp_x) \u003c len(Best_Site): tmp_x.append(tmp_x[0]) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(1) for timestamp, _ in tqdm(Not_Flooding): tmp_x = [] tmp_df = Rain_df.loc[(Rain_df['obsTime'] \u003c (timestamp - timedelta(hours=1))) \u0026 (Rain_df['obsTime'] \u003e (timestamp - timedelta(hours=60)))] for Site in Best_Site: Site_tmp_df = tmp_df.loc[(tmp_df['station_id']==Site)] if not Site_tmp_df.empty: tmp_x.append(Site_tmp_df.tail(24)[['RAIN', 'HOUR_3']].values.flatten()) tmp_x = np.array(tmp_x).flatten() if len(tmp_x) == 24*len(Best_Site)*2: Test_DataSet['x'].append(tmp_x) Test_DataSet['y'].append(0) 最後我們使用 Scikit learn (sklearn) 這個 Python 套件所提供的九個分類器模型 (Nearest neighbors, Linear SVM, RBF SVM, Decision tree, Random forest, Neural net, Adaboost, Naive Bayes, QDA) ，並且依次帶入訓練資料進行調校後，接著帶入測試資料進行淹水與否的預測，並且將預測結果與測試資料中的標籤內容進行比對，再用混淆矩陣 (confusion matrix) 的方式，呈現不同標籤組合的分類結果。\nnames = [ \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\", \"Naive Bayes\", \"QDA\", ] classifiers = [ KNeighborsClassifier(3), SVC(kernel=\"linear\", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), MLPClassifier(alpha=1, max_iter=1000), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), ] fig, axes = plt.subplots(3,3,figsize=(18, 13.5)) for i, model in enumerate(classifiers): model.fit(Train_DataSet['x'], Train_DataSet['y']) Result = model.predict(Test_DataSet['x']) mat = confusion_matrix(Test_DataSet['y'], Result) sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False, xticklabels=[\"不淹水\",\"淹水\"], yticklabels=[\"不淹水\",\"淹水\"], ax = axes[i//3][i%3]) axes[i//3][i%3].set_title(\"{}, Accuracy : {}\".format(names[i], round(accuracy_score(Result, Test_DataSet['y']), 3)), fontweight=\"bold\", size=13) axes[i//3][i%3].set_xlabel('true label', fontsize = 10.0) axes[i//3][i%3].set_ylabel('predicted label', fontsize = 10.0) plt.tight_layout() 在這個案例中，我們發現 Nearest Neighbors 方法可以達到將近 7.3 成的分類成功率，事實上，這只是我們使用原始資料尚未進行更進一步處理分析的結果，倘若我們對於資料本身進行更多的分析，擷取更多的特徵，還有可能將分類成功率繼續提升，讀者們若對於分類器有興趣，可以參考相關資源，進行更深入的探究，將能更加提升分類器對不同種類資料的分類能力。\n參考資料 Ibrahim Saidi, Your First Machine Learning Project in Python, Medium, Jan, 2022, (https://ibrahimsaidi.com.au/your-first-machine-learning-project-in-python-e3b90170ae41) Esmaeil Alizadeh, An Illustrative Introduction to Dynamic Time Warping, Medium, Oct. 2020, (https://towardsdatascience.com/an-illustrative-introduction-to-dynamic-time-warping-36aa98513b98) Jason Brownlee, 10 Clustering Algorithms With Python, Machine Learning Mastery, Aug. 2020, (https://machinelearningmastery.com/clustering-algorithms-with-python/) Alexandra Amidon, How to Apply K-means Clustering to Time Series Data, TOwards Data Science, July 2020, (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) scikit-learn Tutorials (https://scikit-learn.org/stable/tutorial/index.html) FastDTW - A Python implementation of FastDTW, (https://github.com/slaypni/fastdtw) Nearest Neighbors - Wikipedia, (https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) SVM Classifier - Wikipedia, (https://en.wikipedia.org/wiki/Support-vector_machine) Decision Tree - Wikipedia, (https://en.wikipedia.org/wiki/Decision_tree) Random Forest - Wikipedia, (https://en.wikipedia.org/wiki/Random_forest) K-Means - Wikipedia, (https://en.wikipedia.org/wiki/K-means_clustering) DBSCAN - Wikipedia, (https://en.wikipedia.org/wiki/DBSCAN) Hierarchical Clustering - Wikipedia, (https://en.wikipedia.org/wiki/Hierarchical_clustering) BIRCH - Wikipedia, (https://en.wikipedia.org/wiki/BIRCH) ",
    "description": "我們使用空品和水位類別資料，結合天氣觀測資料，利用資料集的時間欄位進行連結，帶入機器學習的套件，進行資料分類與資料分群的分析。我們將示範機器學習的標準流程，並且介紹如何透過資料分類進一步進行資料預測，以及如何透過資料分群進行對資料進一步的深入探究。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "6.1. 機器學習初探",
    "uri": "/ch6/ch6.1/"
  },
  {
    "content": " Table Of Contents 章節目標 QGIS操作說明 範例一：空品測站分佈圖 資料匯入 產生 GeoJSON 檔案 篩選資料與改變資料點位顏色 將成果輸出成主題式地圖 範例二：避難所分佈圖 QGIS總結 Reference QGIS 是一套免費的地圖資料管理系統，除了可以將使用者蒐集到的資料以地理資續的方式呈現外，使用者也可透過 QGIS 來處理、分析及整合地理空間資料，並繪製主題地圖。在這個章節中，我們將利用 QGIS 來協助我們分析並呈現從民生公共物聯網上所得到的 PM2.5 資料，並在分析完成後將其結果輸出成主題式地圖以供判讀。我們同時也示範如何結合民生公共物聯網的災防資料，透過 QGIS 系統繪製防災避難所分佈圖，方便民眾自行查詢離家最近的防災避難所。\n註釋 本文所操作之 QGIS 版本為 3.16.8，惟本文所使用之功能皆為該軟體之基本功能，若使用其他版本軟體，應仍可正常操作。\n章節目標 如何將取得的資料導入 QGIS 在 QGIS 使用前面章節所介紹的地理空間分析功能 (Intersection、Buffer) 將結果輸出成一張主題地圖供別人觀賞 QGIS操作說明 在執行 QGIS 軟體後，可以看到下方的操作介面，除了中間區域的資料框外，上方為標準工具列，提供各式不同的基本操作工具與功能；左方有兩個子區域，分別為資料目錄視窗與圖層；右邊則為分析工具列，提供各式不同的分析工具。\n以下，我們將使用兩個簡單的範例，透過民生公共物聯網的空品資料與災防資料，介紹 QGIS 的基本操作。\n範例一：空品測站分佈圖 資料匯入 資料來源：民生公共物聯網歷史資料 https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器\n在這個部分會先介紹如何將資料成功導入到 QGIS 內。由於有些資料在儲存時，會把一張資料表拆開成多張不同的資料表，因此我們在進行分析時，必須要特別留意是否手邊的資料是否有這種情況，並將資料重新結合成原來的一張資料表，以下介紹資料結合的方法：\n資料導入\n首先，我們要先介紹如何將從民生公共物聯網歷史資料直接下載下來的 csv 檔導入到 QGIS。由於資料中有中文，導入時會顯示亂碼，所以採用導入方法為 Layer（位於最上方選單） \u003e Add Layer \u003e Add Delimited Text Layer，匯入介面如下圖： 資料結合 (Join)\n由於原始資料將 PM2.5 與測站經緯座標分成兩個檔案，故須先將 PM2.5 與經緯座標進行 Join。 Join方法如下，對要Join的檔案右鍵 \u003e Properties \u003e joins \u003e 點+號進入 Add Vector Join，如下圖 進入之後有四個主要的選項分別是：\nJoin Layer：想要 Join 的 Layer Join field：與 Target field 所對應，為 Join 時的參考 (類似 Primary key) Target field：與 Join field 所對應，為 Join 時的參考 (類似 Foreign key) Joined field：選擇想要 Join 的欄位 產生 GeoJSON 檔案 接著我們利用 QGIS 內建的功能來將原本的 csv 資料轉換成 GeoJSON 檔，操作程序為點選Processing \u003e Toolbox \u003e Create points from table。\n請注意在點選後選擇要輸入的 Table，在 X 選擇 lon、在 Y 輸入 lat，並且在 Target CRS 指定為 WGS 84（經緯度座標）然後輸入要輸出的檔案名稱，如下圖。\n接著選擇所要輸出的檔案格式後，點選「存檔」即可。\n篩選資料與改變資料點位顏色 接下來我們示範如何使用 QGIS 來篩選所需要的測站，並讓測站的呈現顏色隨著 PM 2.5 數值而改變。\n利用 Intersection 來根據縣市篩選所需要的測站\n在進行篩選前，需要先從政府資料開放平台下載直轄市、縣市界線的 shp 檔，之後將各縣市界線的 shp 檔匯入QGIS，如下圖： 註釋 由於民生公共物聯網計畫中，將彰化和南投縣市的校園空品微型感測器佈建劃分由暨南大學負責，因此本次所取用的資料中，並無彰化和南投的資料。\n接著我們點選 Select Feature 的圖示，並點選 Country 的 Layer 並選取想要的縣市，被選取的縣市將會呈現黃色，這邊以新北為例，如下圖： 在 Processing Toolbox 中找尋 Intersection 功能，點擊後會出現如下圖介面，其中有三個主要的輸入，分別是：\nInput Layer Overlay Layer Intersection (Output) 接著請在 Input Layer 中放入 PM2.5 的 layer，在 Overlay Layer 中放入縣市界線的 layer 並勾選Select features only，代表只篩選出和上一步驟中選取的新北市有交集的測站，接著在 Intersection 中輸入要輸出的檔案名稱，其中可支援輸出的檔案格式選項和之前可選擇的選項相同。\n執行過後將會得到下圖成果 根據 PM2.5 的數值來顯示不同的顏色\n接著我們在 PM2.5 的 layer 上點選右鍵 \u003e Properties \u003e Symbology 便可以看到圓點的顏色設定，有關各 PM2.5 測站的顏色設定步驟如下：\n在最上方原本的設定為 No Symbols，將其改為 Graduated 如下 Value 部分選 PM25 點擊下方 Classify 按鈕 在 Classes 設定顏色數目（註：建議設定類別不宜過多，以不超過7類為原則） 到 Color ramp 設定各數值之顏色 完成後點擊 OK 當一切都設定完成後，將可以得到下圖，其中圓點的顏色會隨著PM 2.5數值的不同而改變，而右邊的 Layer 則顯示不同顏色所代表的 PM 2.5 數值。 註釋 新版的QGIS (3.16 後)裡已經有 OpenStreetMap 的底圖，可以點擊下圖中的 XYZ Tiles → OpenStreetMap 來添加 OSM 底圖\n將成果輸出成主題式地圖 在完成了上述的設定後，接下來要將 QGIS 專案輸出成 JPG 圖片。我們點擊 Project \u003e New Print Layout 後會跳出 Layout 名稱設定，設定完成後會出現如下畫面：\n點擊左方的 Add map，並在繪圖區上框選範圍，來加入 PM 2.5 的地圖，如下圖\n接下來點選左邊的 Add Legend 並選取一範圍來匯入標籤，而在右方的 Item Properties 可以改變標籤中的字體大小、顏色等；最後再匯入標題、比例尺、指北針來完成主題圖。\n最後點選左上角的 Layout \u003e Export as Image 便可將主題圖輸出成圖片檔案。\n範例二：避難所分佈圖 資料來源：https://data.gov.tw/dataset/73242\n在政府的開放資料中，已將全台灣的緊急避難所整理成電子檔案，方便民眾下載使用，在這個範例中，我們將使用這些資料，介紹如何透過 QGIS 尋找離家裡最近的避難所。\n我們先去上面的網址取得避難所資料，接下來照前面所講的方法載入資料，如下圖：\n由於全台灣的避難所數量眾多，我們在本文中只分析台北市的避難所，其餘縣市的部分也可以用相同的方法分析，歡迎讀者自行嘗試。我們先用前面所述的 intersection 方法把台北市的避難所找出來。接下來，我們使用旁邊工具列的 Voronoi Polygons 來繪製Voronoi Diagram，如下圖：\n在 Voronoi Polygons 填入避難所的圖層，並按「執行」\n根據 Voronoi Diagram 的特性 (第 5.2 章節)，我們可以得知離自己家最接近的避難所位置，如下圖：\n在完成分析後，可以按照前面的方法將分析結果出成主題式地圖來供別人觀賞。\nQGIS總結 在本章節中我們介紹了如何將資料導入 QGIS，以及如何利用 QGIS 中的分析工具，來協助我們對資料進行分析。最後我們介紹了圖表匯出的方法，可以把分析好的資料製成主題圖供別人觀賞。當然，QGIS 中仍有許多功能是在這個章節中來不及介紹的，如果對 QGIS 還有更多的興趣，可以參考下方的一些其他資源。\nReference QGIS: A Free and Open Source Geographic Information System (https://qgis.org/) YouTube：GIS 網上小教室系列：第1集 — 從零開始學 QGIS QGIS 英文說明：QGIS Tutorials and Tips QGIS Documentation (https://www.qgis.org/en/docs/index.html) ",
    "description": "我們介紹使用 QGIS 系統進行的地理資料呈現，並且以Civil IoT Taiwan 的資料當作範例，利用點擊拖拉的方式，進行地理空間分析。同時我們也討論 QGIS 軟體的優缺點與使用時機。",
    "tags": [
      "空",
      "災"
    ],
    "title": "7.1. QGIS 應用",
    "uri": "/ch7/ch7.1/"
  },
  {
    "content": " Table Of Contents 程式語言 - Python 開發平台 – Google Colab 參考資料 程式語言 - Python 針對本套課程的主題著重在民生公共物聯網資料應用，本套課程將以目前資料科學界所常用的程式語言 Python 為主要程式語言，利用淺顯易懂的示範方式，帶領讀者透過做中學的方式，逐步進入各章節的主題內涵，並能獲得民生公共物聯網資料應用的第一手經驗，以及未來舉一反三應用於其他資料科學主題的能力。整體來說，Python 程式語言能在短時間內迅速成為資料科學界最熱門的程式語言，主要有下列三大優勢：\n學習的門檻較低：在文字式的程式語言來說，相較於其它語言（例如 C、Java），Python 程式碼較少特殊符號，字面上看起來比較接近日常生活中常見的英文文章，如果掌握了 Python 語法和程式執行的邏輯，再加上英文單字能力，就能夠很容易理解 Python 程式碼語意，因此學習 Python 的門檻較低！ 各式各樣的套件庫：Python 經過三十多年的發展，儼然壯大成一個生態系，除了基本語法外，還可以加裝各式各樣的套件，就能進階變形為各種的工具，適合用來解決五花八門的問題。透過本計畫量身訂制一個 Python 套件 (CIOT)，可以更快速地提高學習的天花板，從而了解如何更方便地應用民生公共物聯網資料。 閱讀理解程式碼較適合自學：隨著套件的使用，閱讀理解程式碼的能力比起程式碼寫作能力更為重要！如果要駕馭現有的套件，必須先看得懂使用手冊，然後再排列組合去拼湊成解決問題的程式碼。所以問題解決的模式，會是「在現有的程式高手基礎上去堆疊程式，而不是從零開始寫 Python 程式碼」；尤其是培養閱讀理解程式碼的能力，比較適合用自學模式來進行，當逐漸熟悉 Python 的邏輯後，學習閱讀程式碼就有機會變得跟閱讀故事書一樣有趣喔！ 也由於 Python 語言具備上述的各項優點，Python 語言目前已成為資料科學上最普遍被使用的程式語言，更成為許多程式設計學習者所使用的第一個程式語言。因此，除了坊間各種 Python 語言的教學書籍外，在網際網路上也可以找到多實用的學習資源，值得對 Python 語言有興趣的讀者，進一步主動的進行探索與學習。\n免費教學課程 用 Python 做商管程式設計（一），孔令傑，Coursera (https://zh-tw.coursera.org/learn/pbc1) Python 入門教學課程，彭彭的課程 (https://www.youtube.com/watch?v=wqRlKVRUV_k\u0026list=PL-g0fdC5RMboYEyt6QS2iLb_1m7QcgfHk) Python for Everybody Specialization, Coursera (https://www.coursera.org/specializations/python) Python for Data Science, AI \u0026 Development, Coursera (https://www.coursera.org/learn/python-for-applied-data-science-ai) Introduction to Python Programming, Udemy (https://www.udemy.com/course/pythonforbeginnersintro/) Learn Python for Total Beginners, Udemy (https://www.udemy.com/course/python-3-for-total-beginners/) Google’s Python Class (https://developers.google.com/edu/python) Introduction to Python, Microsoft (https://docs.microsoft.com/en-us/learn/modules/intro-to-python/) Learn Python 3 from Scratch, Educative (https://www.educative.io/courses/learn-python-3-from-scratch) 免費電子書資源 Python 教學 (https://docs.python.org/zh-tw/3/tutorial/index.html) Python 教學 (學習導讀)，STEAM 教育學習網 (https://steam.oxxostudio.tw/category/python/info/start.html) Python 程式設計，李明昌 (http://rwepa.blogspot.com/2020/02/pythonprogramminglee.html) Non-Programmer’s Tutorial for Python 3, Josh Cogliati (https://en.wikibooks.org/wiki/Non-Programmer’s_Tutorial_for_Python_3) Python 101, Michael Driscoll (https://python101.pythonlibrary.org/) The Python Coding Book, Stephen Gruppetta (https://thepythoncodingbook.com/) Python Data Science Handbook, Jake VanderPlas (https://jakevdp.github.io/PythonDataScienceHandbook/) Intro to Machine Learning with Python, Bernd Klein (https://python-course.eu/machine-learning/) Applied Data Science, Ian Langmore and Daniel Krasner (https://columbia-applied-data-science.github.io/) 開發平台 – Google Colab 有別於可以事先編譯成執行檔的C語言，Python 本身是屬於直譯式語言，就是在執行前才翻譯成機器語言來讓電腦執行，換句話說，就是邊執行邊直譯。以日常生活類似情境來理解，這種方式就好像有一個翻譯員在幫我們翻譯用外國人能接受的語言來溝通，我們說一句話，翻譯員就幫我們直譯一句話；相反的，外國人說一句話，翻譯員就幫我們把外國人的話翻譯回來讓我們理解。\n由於這種直譯的特性，所以除了 Python 官方提供的程式編輯器，也有許多其它不同功能的編輯器。基於 Python 程式碼與自然語言較雷同，就有人提出將 Python 編輯器做成類似筆記本的模式，可以把自然語言的文章與 Python 程式碼混在一頁並存的編輯器，其中以 Jupyter 最為人熟知。\n在 Jupyter 的基礎上，Google 公司將 Python 直譯程式搬到網路雲端，只要申請 Google 帳號，就可以在 Google 雲端硬碟中加裝免費的 Colab 應用程式，直接使用瀏覽器就可以享受 Python 程式編輯功能，也成為 Python 使用者最常使用的開發平台。\n整體而言，Google Colab 具有下列四大優點：\n預先安裝的套件：Google Colab 開發平台已預先安裝絕大多數的 Python 套件，讓使用者可以直接使用，避免必須自行安裝，甚至必須自行排除不同套件在安裝時產生的版本衝突問題，大幅降低使用 Python 開發程式的入門門檻。 雲端儲存：借助 Google 本身的雲端儲存空間，Google Colab 可將開發過程中的程式筆記儲存在雲端空間，只要透過網路存取，即使在不同的電腦上也可以無縫接軌，解決資料儲存、備份與攜帶的問題。 合作共創：Google Colab 提供網路分享與線上協作的功能，允許使用者將程式筆記透過雲端儲存空間分享給其他使用者，也允許使用者邀請其他使用者瀏覽、註解、甚至編輯自己的程式筆記，加速了團隊合作的便利性。 免費的 GPU 和 TPU 運算資源：借助 Google 本身豐沛的雲端運算資源，Google Colab 提供 GPU 與 TPU 處理器，讓使用者可以使用高階的處理器執行自己個人的程式筆記內容，嘉惠高運量的程式開發需求。 有關 Google Colab 的相關學習資源，可參考下列的連結：\nGoogle Colab 使用教學，MeDA School，國立台灣大學 (https://www.youtube.com/watch?v=OyS6K2XdgbQ) 使用 Google Colab，STEAM 教育學習網 (https://steam.oxxostudio.tw/category/python/info/online-editor.html) Getting Started With Google Colab, Anne Bonner (https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c) Use Google Colab Like A Pro, Wing Poon (https://pub.towardsai.net/use-google-colab-like-a-pro-39a97184358d) 參考資料 Python (https://www.python.org/) Google Colaboratory (https://colab.research.google.com/) Jupyter: Free software, open standards, and web services for interactive computing across all programming languages (https://jupyter.org/) 4 Reasons Why You Should Use Google Colab for Your Next Project, Orhan G. Yalçın (https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed) ",
    "description": "民生公共物聯網資料應用所使用到的程式語言 Python 和開發平台 Google Colab 簡介",
    "tags": [
      "概述"
    ],
    "title": "2.2. 課程軟體工具簡介",
    "uri": "/ch2/ch2.2/"
  },
  {
    "content": "\nTable Of Contents 獲取特定時間序列之資料 獲取特定區域之資料 實作：所在地有比附近的空氣糟嗎？ 獲取檢測站資料 去除無效資料 計算距離 Pandas 函式庫 顯示結果 參考資料 本章節將以時間、空間的角度存取民生公共物聯網的資料，並以空氣品質監測作為題目進行簡易實作。\n本章節會涵蓋到的技術：\ndatetime, math, numpy, pandas 等函式庫應用 json 資料格式處理 Pandas DataFrame 資料處理 獲取特定時間序列之資料 在 pyCIOT 中在執行 get_data() 時，能夠根據時間起始及結束時間獲取資料。格式以字典 (Dict) 傳入 time_range，分別為 start, end 及 num_of_data。\nstart 與 end 指資料搜集的開始及結束時間，格式為 ISO8601 或 Datetime。num_of_data 則是會控制獲取資料的筆數不會超過此數字。若在範圍內的資料超過 num_of_data 則會隔一段時間搜集，使資料與資料之間的時間間隔趨於平均。\n以空氣資料為例，獲取之資料最多能夠回溯一天。因此當將 end 變數設定為一天之前不會獲得任何資料，請留意。此外，因為民生物聯網中各個感測器的更新頻率不同，所以不同感測器每「天」的資料筆數會有不同，詳可參閱：https://ci.taiwan.gov.tw/dsp/dataset_air.aspx\nfrom datetime import datetime, timedelta end_date = datetime.now() # 獲取現在時間 isodate_end = end_date.isoformat().split(\".\")[0]+\"Z\" # 將格式轉換為 ISO8601 格式 start_date = datetime.now() + timedelta(days = -1) # 獲取前一天的時間 isodate_start = start_date.isoformat().split(\".\")[0]+\"Z\" # 將格式轉換為 ISO8601 格式 time = { \"start\": isodate_start, \"end\": isodate_end, \"num_of_data\": 15 } # 從「智慧城鄉空品微型感測器-11613429495」獲得距離現在一天、最多 15 筆資料 data = Air().get_data(\"OBS:EPA_IoT\", stationIds=[\"11613429495\"], time_range=time) data 資料會以 List 的格式儲存在 data 中，並依照不同種類的數值一起存放。溫度、相對濕度、PM2.5 的資料會分別存放在對應名字下的 ‘values’ list 下，並標上每筆資料紀錄的時間，以 ISO8601 顯示。\n[{'name': '智慧城鄉空品微型感測器-11613429495', 'description': '智慧城鄉空品微型感測器-11613429495', 'properties': {'city': '新竹市', 'areaType': '一般社區', 'isMobile': 'false', 'township': '香山區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '11613429495', 'locationId': 'HC0154', 'Description': 'AQ1001', 'areaDescription': '新竹市香山區'}, 'data': [{'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 30.6}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 30.7}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 30.7}]}, {'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T12:54:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:53:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 100}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 100}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T12:53:10.000Z', 'value': 11.9}, {'timestamp': '2022-08-27T12:52:09.000Z', 'value': 12.15}, {'timestamp': '2022-08-27T12:51:09.000Z', 'value': 12.2}, {'timestamp': '2022-08-27T12:50:09.000Z', 'value': 12.22}, {'timestamp': '2022-08-27T12:49:09.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:48:10.000Z', 'value': 12.54}, {'timestamp': '2022-08-27T12:47:10.000Z', 'value': 12.31}, {'timestamp': '2022-08-27T12:46:10.000Z', 'value': 12.19}, {'timestamp': '2022-08-27T12:45:10.000Z', 'value': 12.26}, {'timestamp': '2022-08-27T12:44:10.000Z', 'value': 12.17}, {'timestamp': '2022-08-27T12:43:09.000Z', 'value': 12.04}, {'timestamp': '2022-08-27T12:42:10.000Z', 'value': 11.7}, {'timestamp': '2022-08-27T12:41:09.000Z', 'value': 11.67}, {'timestamp': '2022-08-27T12:40:10.000Z', 'value': 11.56}, {'timestamp': '2022-08-27T12:39:10.000Z', 'value': 11.56}]}], 'location': {'latitude': 24.81796, 'longitude': 120.92664, 'address': None}}] 獲取特定區域之資料 在 pyCIOT 中也有根據地域獲取特定資料的方式。以特定地點的經度及緯度，以及以特定地點的經度及緯度為中心，並利用一個半徑距離構成搜尋範圍 (圓形)，以獲取特定空間內的測站 ID 及感測數值。\n特定區域的資料格式也是以字典 (Dict) 傳入，其中緯度、經度及半徑分別為 “latitude”、“longitude” 及 “distance”。特定區域及特定時間的篩選功能可以同時使用，搜尋特定區域時也可以將有要觀察的測站放進 “stationIds” 中，可以順便將區域外的測站去除。\nloc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # 半徑(km) } c = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) c[0] { 'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': { 'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [ { 'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 35.84}] },{ 'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 59.5}] },{ 'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-08-27T08:07:03.000Z', 'value': 11.09}] } ], 'location': { 'latitude': 24.998769, 'longitude': 121.512717, 'address': None } } 以上為在獲取 pyCIOT 測站資料時常用的，以時間和空間為篩選標準的方法之一，適用於所有包含 location及 timestamp型態的資料。為了示範，我們舉出一些簡單的例子，並使用這些 pyCIOT 函式庫實作。\n實作：所在地有比附近的空氣糟嗎？ 匯入資料：環保署智慧城鄉空品微型感測器（OBS:EPA_IoT） 測試地點：中和南勢角捷運站 1 號出口：(24.990550, 121.507532) 比較環境：新北市中和區 獲取檢測站資料 首先，需要獲得測試地點和比較環境的所有資料。我們可以利用「獲取特定區域之資料」的方法，將經度緯度設定在南勢角捷運站一號出口，將距離設定為三公里，即可簡單的將資料利用 Air().get_data()獲取：\n# 獲取檢測站的資料（溫度、濕度、 PM2.5） loc = { \"latitude\": 24.990550, # 緯度 \"longitude\": 121.507532, # 經度 \"distance\": 3.0 # (km) } EPA_IoT_zhonghe_data_raw = Air().get_data(src=\"OBS:EPA_IoT\", location = loc) print(\"len:\", len(EPA_IoT_zhonghe_data_raw)) # 印出測站個數 EPA_IoT_zhonghe_data_raw[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'data': [{'name': 'Relative humidity', 'description': '相對溼度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 94.84}]}, {'name': 'PM2.5', 'description': '細懸浮微粒 PM2.5', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 3.81}]}, {'name': 'Temperature', 'description': '溫度', 'values': [{'timestamp': '2022-09-11T09:58:21.000Z', 'value': 25.72}]}], 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}} 去除無效資料 在每個範圍內的測站中，不一定每個測站都還在順利運行。為了將這些測站去除，我們觀察無效測站的資料會有什麼特徵，發現三個資料（溫度、濕度、PM2.5 濃度）都會是 0。只要挑出並刪除這些資料便能夠進行下一步驟。\n# Data cleaning EPA_IoT_zhonghe_data = [] for datajson in EPA_IoT_zhonghe_data_raw: # 確認資料存在 if \"data\" not in datajson: continue; # 將格式轉換為 Temperature, Relative_Humidity 和 PM2_5 for rawdata_array in datajson['data']: if(rawdata_array['name'] == 'Temperature'): datajson['Temperature'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'Relative humidity'): datajson['Relative_Humidity'] = rawdata_array['values'][0]['value'] if(rawdata_array['name'] == 'PM2.5'): datajson['PM2_5'] = rawdata_array['values'][0]['value'] datajson.pop('data') # 確認所有資料皆為有效，同時去除無資料之檢測站 if \"Relative_Humidity\" not in datajson.keys(): continue if \"PM2_5\" not in datajson.keys(): continue if \"Temperature\" not in datajson.keys(): continue if(datajson['Relative_Humidity'] == 0 and datajson['PM2_5'] == 0 and datajson['Temperature'] == 0): continue EPA_IoT_zhonghe_data.append(datajson) print(\"len:\", len(EPA_IoT_zhonghe_data)) EPA_IoT_zhonghe_data[0] len: 70 {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24} 計算距離 假設每個測站的資料沒有誤差，那最接近目標地點的測站資料即為要比較的資料。為了找到最接近的測站，我們要算出將每個測站和目標地點的距離。\n我們可以利用點到點距離公式計算並排序找到最接近目標地點的測站，可以直接使用在 math 內的 pow() 函式，計算平方及平方根距離。但在這裡我們使用比較標準的 Haversine 公式計算地球上兩點間的球面距離，以下為在 WGS84 坐標系下的實作：\n# 增加與南勢角站距離欄位 import math def LLs2Dist(lat1, lon1, lat2, lon2): R = 6371 dLat = (lat2 - lat1) * math.pi / 180.0 dLon = (lon2 - lon1) * math.pi / 180.0 a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(lat1 * math.pi / 180.0) * math.cos(lat2 * math.pi / 180.0) * math.sin(dLon / 2) * math.sin(dLon / 2) c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) dist = R * c return dist for data in EPA_IoT_zhonghe_data: data['distance'] = LLs2Dist(data['location']['latitude'], data['location']['longitude'], 24.990550, 121.507532)# (24.990550, 121.507532) EPA_IoT_zhonghe_data[0] {'name': '智慧城鄉空品微型感測器-10382640142', 'description': '智慧城鄉空品微型感測器-10382640142', 'properties': {'city': '新北市', 'areaType': '交通', 'isMobile': 'false', 'township': '中和區', 'authority': '行政院環境保護署', 'isDisplay': 'true', 'isOutdoor': 'true', 'stationID': '10382640142', 'locationId': 'TW040203A0506917', 'Description': '廣域SAQ-210', 'areaDescription': '中和區'}, 'location': {'latitude': 24.998769, 'longitude': 121.512717, 'address': None}, 'PM2_5': 2.61, 'Relative_Humidity': 94.27, 'Temperature': 26.24, 'distance': 1.052754763080127} Pandas 函式庫 Pandas 是用於資料操縱和分析的函式庫，其中的 DataFrame 格式用來儲存雙維度或多欄位的資料格式，非常適合用來進行資料分析。我們將處理好的資料轉換成 DataFrame，並挑選出需要的欄位並根據先前計算的距離排序由小到大。\n# 轉換成 Pandas.DataFrame 格式 import pandas as pd df = pd.json_normalize(EPA_IoT_zhonghe_data) #Results contain the required data df EPA_IoT_zhonghe_data_raw = df[['distance', 'PM2_5', 'Temperature', 'Relative_Humidity', 'properties.stationID', 'location.latitude', 'location.longitude', 'properties.areaType']] EPA_IoT_zhonghe_data_raw = EPA_IoT_zhonghe_data_raw.sort_values(by=['distance', 'PM2_5'], ascending=True) EPA_IoT_zhonghe_data_raw 顯示結果 為了知道目標區域的空氣品質相較附近區域的好壞，可以大致上利用所有測站空氣品質的分佈得知。可以利用在 Python 中常利用的 numpy 資料科學處理常用函式庫等工具，或直接計算出平均及標準差，便可得到答案。\nimport numpy as np zhonghe_target = EPA_IoT_zhonghe_data_raw.iloc[0,1] zhonghe_ave = np.mean(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) zhonghe_std = np.std(EPA_IoT_zhonghe_data_raw.iloc[:,1].values) result = (zhonghe_target-zhonghe_ave)/zhonghe_std print('mean:', zhonghe_ave, 'std:', zhonghe_std) print('最近測站 PM2.5 濃度:', zhonghe_target) print('目標離平均', result, '個標準差\\n') if(result\u003e0): print('Result: 現在家裡附近的空氣比附近糟') else: print('Result: 現在家裡附近的空氣比附近好') mean: 6.71 std: 3.18 最近測站 PM2.5 濃度: 7.38 目標離平均 0.21 個標準差 Result: 現在家裡附近的空氣比附近糟 參考資料 Python pyCIOT package (https://pypi.org/project/pyCIOT/) pandas - Python Data Analysis Library (https://pandas.pydata.org/) 10 minutes to pandas — pandas documentation (https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) NumPy (https://numpy.org/) NumPy quickstart (https://numpy.org/doc/stable/user/quickstart.html) Haversine formula - Wikipedia (https://en.wikipedia.org/wiki/Haversine_formula) ",
    "description": "我們介紹如何在民生公共物聯網資料平台中獲取特定時間或時間段的資料，以及特定地理區域的資料，並透過簡單的案例演示其應用。",
    "tags": [
      "Python",
      "API",
      "空"
    ],
    "title": "3.2. 存取特定時空條件的資料",
    "uri": "/ch3/ch3.2/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引用 讀取資料 空品資料 水位資料 氣象資料 資料預處理 (Preprocess) 平穩性 (Stationary) 檢查 資料預測(Data forecast) ARIMA SARIMAX auto_arima Prophet LSTM Holt-Winter 綜合比較 參考資料 前一章節介紹了各種處理時序資料的方法，有視覺化呈現資料、時序資料分解……等，經過處理後的資料可以讓我們更進一步的運用，擁有過去的資料後，會想要預知未來，因此本章節將會判斷時序資料的特性以及使用多種預測模型找出資料的的模式，藉此預測未來。\n章節目標 時序資料特性的判斷：平穩性 學習各種預測模型並進行比較 使用時序資料進行訓練與預測 套件安裝與引用 在本章節中，我們將會使用到 pandas, matplotlib, numpy, statsmodels, warnings 等套件，這些套件由於在我們使用的開發平台 Colab 上皆已預先安裝好，因此不需要再另行安裝。然而，我們還會另外使用兩個 Colab 並未預先安裝好的套件：kats 和 pmdarima，需使用下列的方式自行安裝：\n!pip install --upgrade pip !pip install kats==0.1 ax-platform==0.2.3 statsmodels==0.12.2 !pip install pmdarima 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport warnings import numpy as np import pandas as pd import pmdarima as pm import statsmodels.api as sm import matplotlib.pyplot as plt import os, zipfile from dateutil import parser as datetime_parser from statsmodels.tsa.arima.model import ARIMA from statsmodels.tsa.statespace.sarimax import SARIMAX from statsmodels.tsa.stattools import adfuller, kpss from kats.consts import TimeSeriesData, TimeSeriesIterator from kats.detectors.outlier import OutlierDetector from kats.models.prophet import ProphetModel, ProphetParams from kats.models.lstm import LSTMModel, LSTMParams from kats.models.holtwinters import HoltWintersParams, HoltWintersModel 讀取資料 本章節的探討主題為時序資料的資料預測，因此我們將分別以民生公共物聯網資料平台上的空品、水位和氣象資料進行資料讀取的演示，接著再使用空品資料進行更進一步的探究。其中，每一類別的資料處理都將使用其中一個測站長期以來觀測到的資料作為資料集，而在 dataframe 的時間欄位名稱則為設為 timestamp，由於時間欄位的數值具有唯一性，因此我們也將使用此欄位作為 dataframe 的索引 (index)。\n空品資料 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的歷史資料，並存入 Air 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Air 資料夾中。\n!mkdir Air CSV_Air !wget -O Air/2018.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTguemlw\" !wget -O Air/2019.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMTkuemlw\" !wget -O Air/2020.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjAuemlw\" !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Air' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Air') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Air/{item}') 現在 CSV_Air 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 74DA38C7D2AC 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 air 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Air' extension_csv = '.csv' id = '74DA38C7D2AC' air = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'device_id==@id') air = pd.concat([air, filtered], ignore_index=True) air.dropna(subset=['timestamp'], inplace=True) for i, row in air.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) air.at[i, 'timestamp'] = naive air.set_index('timestamp', inplace=True) !rm -rf Air CSV_Air 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nair.drop(columns=['device_id', 'SiteName'], inplace=True) air.sort_values(by='timestamp', inplace=True) air.info() print(air.info()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 195305 entries, 2018-08-01 00:00:05 to 2021-12-31 23:54:46 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 PM25 195305 non-null object dtypes: object(1) memory usage: 3.0+ MB PM25 timestamp 2018-08-01 00:00:05 20.0 2018-08-01 00:30:18 17.0 2018-08-01 01:12:34 18.0 2018-08-01 01:18:36 21.0 2018-08-01 01:30:44 22.0 水位資料 和空品資料的範例一樣，由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「水利署地下水位站」的歷史資料，並存入 Water 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Water 資料夾中。\n!mkdir Water CSV_Water !wget -O Water/2018.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTguemlw\" !wget -O Water/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMTkuemlw\" !wget -O Water/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjAuemlw\" !wget -O Water/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2BawtOizh%2Ba6kC%2FmsLTliKnnvbJf5rKz5bed5rC05L2N56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Water' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip) and not it.endswith('QC.zip'): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Water') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Water/{item}') 現在 CSV_Water 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 338c9c1c-57d8-41d7-9af2-731fb86e632c 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 water 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Water' extension_csv = '.csv' id = '338c9c1c-57d8-41d7-9af2-731fb86e632c' water = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') water = pd.concat([water, filtered], ignore_index=True) water.dropna(subset=['timestamp'], inplace=True) for i, row in water.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) water.at[i, 'timestamp'] = naive water.set_index('timestamp', inplace=True) !rm -rf Water CSV_Water 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nwater.drop(columns=['station_id', 'ciOrgname', 'ciCategory', 'Organize_Name', 'CategoryInfos_Name', 'PQ_name', 'PQ_fullname', 'PQ_description', 'PQ_unit', 'PQ_id'], inplace=True) water.sort_values(by='timestamp', inplace=True) water.info() print(water.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 213466 entries, 2018-01-01 00:20:00 to 2021-12-07 11:00:00 Data columns (total 1 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 value 213465 non-null float64 dtypes: float64(1) memory usage: 3.3 MB value timestamp 2018-01-01 00:20:00 49.130000 2018-01-01 00:25:00 49.139999 2018-01-01 00:30:00 49.130001 2018-01-01 00:35:00 49.130001 2018-01-01 00:40:00 49.130001 氣象資料 我們從民生公共物聯網資料平台的歷史資料庫下載「中央氣象局自動氣象站」的歷史資料，並存入 Weather 資料夾中。\n同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先逐一將其解壓縮，產生每日資料的壓縮檔案，接著再將每日資料的壓縮檔案解壓縮，存入 CSV_Weather 資料夾中。\n!mkdir Weather CSV_Weather !wget -O Weather/2019.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMTkuemlw\" !wget -O Weather/2020.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjAuemlw\" !wget -O Weather/2021.zip \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bawo%2BixoS%2FkuK3lpK7msKPosaHlsYBf6Ieq5YuV5rCj6LGh56uZLzIwMjEuemlw\" #開始進行解壓縮 folder = 'Weather' extension_zip = '.zip' extension_csv = '.csv' for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if path.endswith(extension_zip): print(path) zip_ref = zipfile.ZipFile(path) zip_ref.extractall(folder) zip_ref.close() for subfolder in os.listdir(folder): path = f'{folder}/{subfolder}' if os.path.isdir(path): for item in os.listdir(path): if item.endswith(extension_zip): file_name = f'{path}/{item}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(path) zip_ref.close() for item in os.listdir(path): path2 = f'{path}/{item}' if os.path.isdir(path2): for it in os.listdir(path2): if it.endswith(extension_zip): file_name = f'{path2}/{it}' print(file_name) zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall('CSV_Weather') # decide path zip_ref.close() elif item.endswith(extension_csv): os.rename(path2, f'CSV_Weather/{item}') 現在 CSV_Weather 資料夾中即有每日所有感測器資料的 csv 格式檔案，為了將單一測站 (例如代碼為 C0U750 的測站) 的資料過濾出來，我們需要讀取每個 csv 檔案，並將檔案中該測站的資料存入名叫 weather 的 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nfolder = 'CSV_Weather' extension_csv = '.csv' id = 'C0U750' weather = pd.DataFrame() for item in os.listdir(folder): file_name = f'{folder}/{item}' df = pd.read_csv(file_name) if 'pm25' in list(df.columns): df.rename({'pm25':'PM25'}, axis=1, inplace=True) filtered = df.query(f'station_id==@id') weather = pd.concat([weather, filtered], ignore_index=True) weather.rename({'obsTime':'timestamp'}, axis=1, inplace=True) weather.dropna(subset=['timestamp'], inplace=True) for i, row in weather.iterrows(): aware = datetime_parser.parse(str(row['timestamp'])) naive = aware.replace(tzinfo=None) weather.at[i, 'timestamp'] = naive weather.set_index('timestamp', inplace=True) !rm -rf Weather CSV_Weather 最後，我們重新整理該測站的資料，將不需要用到的欄位資訊刪除，並且依照時間進行排序如下：\nweather.drop(columns=['station_id'], inplace=True) weather.sort_values(by='timestamp', inplace=True) weather.info() print(weather.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 27093 entries, 2019-01-01 00:00:00 to 2021-12-31 23:00:00 Data columns (total 15 columns): \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 ELEV 27093 non-null float64 1 WDIR 27089 non-null float64 2 WDSD 27089 non-null float64 3 TEMP 27093 non-null float64 4 HUMD 27089 non-null float64 5 PRES 27093 non-null float64 6 SUN 13714 non-null float64 7 H_24R 27089 non-null float64 8 H_FX 27089 non-null float64 9 H_XD 27089 non-null object 10 H_FXT 23364 non-null object 11 D_TX 27074 non-null object 12 D_TXT 7574 non-null object 13 D_TN 27074 non-null object 14 D_TNT 17 non-null object dtypes: float64(9), object(6) memory usage: 3.3+ MB ELEV WDIR WDSD TEMP HUMD PRES SUN H_24R H_FX \\ timestamp 2019-01-01 00:00:00 398.0 35.0 5.8 13.4 0.99 981.1 -99.0 18.5 -99.0 2019-01-01 01:00:00 398.0 31.0 5.7 14.1 0.99 981.0 -99.0 0.5 10.8 2019-01-01 02:00:00 398.0 35.0 5.3 13.9 0.99 980.7 -99.0 1.0 -99.0 2019-01-01 03:00:00 398.0 32.0 5.7 13.8 0.99 980.2 -99.0 1.5 -99.0 2019-01-01 04:00:00 398.0 37.0 6.9 13.8 0.99 980.0 -99.0 2.0 12.0 H_XD H_FXT D_TX D_TXT D_TN D_TNT timestamp 2019-01-01 00:00:00 -99.0 -99.0 14.5 NaN 13.4 NaN 2019-01-01 01:00:00 35.0 NaN 14.1 NaN 13.5 NaN 2019-01-01 02:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 03:00:00 -99.0 -99.0 14.1 NaN 13.5 NaN 2019-01-01 04:00:00 39.0 NaN 14.1 NaN 13.5 NaN 以上我們已經成功示範空品資料 (air)、水位資料 (water) 和氣象資料 (weather) 的讀取範例，在接下來的探討中，我們將以空品資料示範初步的時間序列資料處理，相同的方法也可以輕易改成使用水位資料或氣象資料而得到類似的結果，大家可以自行嘗試看看。\n資料預處理 (Preprocess) 我們首先依照章節 4.1 所介紹的方法對資料進行重新採樣，將資料分別取每小時平均 (air_hour)、每天平均 (air_day) 和每月平均 (air_month)。\nair_hour = air.resample('H').mean() air_day = air.resample('D').mean() air_month = air.resample('M').mean() 接著我們依照章節 4.1 所介紹的離群值偵測方法，將 air_hour 資料中的離群值移除，並將移除後的缺失資料，使用 Forward fill 方法填回。\nair_ts = TimeSeriesData(air_hour.reset_index(), time_col_name='timestamp') # 移除離群值 outlierDetection = OutlierDetector(air_ts, 'additive') outlierDetection.detector() outliers_removed = outlierDetection.remover(interpolate=False) air_hour_df = outliers_removed.to_dataframe() air_hour_df.rename(columns={'time': 'timestamp', 'y_0': 'PM25'}, inplace=True) air_hour_df.set_index('timestamp', inplace=True) air_hour = air_hour_df air_hour = air_hour.resample('H').mean() # 用 Forward 方法填回缺失值 air_hour.ffill(inplace=True) 平穩性 (Stationary) 檢查 在進行資料預測的探究前，我們先針對資料的平穩性 (stationary) 進行檢查。我們先挑選想要進行檢測的時間區段（例如 2020-06-10 ~ 2020-06-17），並將這個區段的資料存入 data 變數。\ndata = air_hour.loc['2020-06-10':'2020-06-17'] 接著我們計算這些資料的平均數 (mean) 與變異數 (var)，並且進行繪圖。\nnmp = data.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() 從圖中可以發現平均數的變化不大，但是變異數的變化卻很大。我們稱這樣的資料具備較差的平穩性；相反地，若是具備平穩性的資料，其平均數與變異數的變化不會與時間的推移有關。\n換句話說，如果資料分布隨著時間有一定的趨勢變化，那它就沒有平穩性；如果資料的分佈不會因為時間推移，平均數與變異數也維持固定，那它就有平穩性 (stationary)。平穩性的資料有利於尋找適合的的模型 (model) 並預測未來的數值。\n若要檢查資料是否具有平穩性，至少有下列兩種常見的方法：\nAugmented Dickey Fuller (ADF) test：使用 unit root test，如果 p-value \u003c 0.05，則資料具有平穩性。 Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test：與 ADF test 相反，如果 p-value \u003c 0.05，則資料不具有平穩性 (non-stationary)。 # ADF Test result = adfuller(data.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -2.7026194088541704 p-value: 0.07358609270498144 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.620177 p-value: 0.020802 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 若以我們使用的範例資料為例，經過 ADF 檢測得到的 p-value 為 0.073，因此該資料並沒有平穩性。為了達到平穩性，我們接下來將資料進行差分，也就是將第 i 筆資料減第 i-1 筆資料，並使用得到的結果再次進行檢測。\n在 dataframe 的資料格式上我們可以直接使用 data.diff() 來將資料差分 ，並將經過差分後的資料命名為 data_diff。\ndata_diff = data.diff() data_diff PM25 timestamp\t2020-06-10 00:00:00\tNaN 2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 我們可以看到第一筆資料為 Nan，這是因為第一筆資料無法減去前一筆資料，所以我們要將第一筆資料捨棄。\ndata_diff = data_diff[1:] data_diff PM25 timestamp\t2020-06-10 01:00:00\t-14.700000 2020-06-10 02:00:00\t-8.100000 2020-06-10 03:00:00\t0.200000 2020-06-10 04:00:00\t-1.900000 2020-06-10 05:00:00\t-1.300000 ...\t... 2020-06-17 19:00:00\t0.750000 2020-06-17 20:00:00\t4.875000 2020-06-17 21:00:00\t-3.375000 2020-06-17 22:00:00\t1.930556 2020-06-17 23:00:00\t3.944444 接著我們將資料繪圖來觀察經過差分後資料的平均數與變異數隨時間變化的關係。\nnmp = data_diff.PM25.to_numpy() size = np.size(nmp) nmp_mean = np.zeros(size) nmp_var = np.zeros(size) for i in range(size): nmp_mean[i] = nmp[:i+1].mean() nmp_var[i] = nmp[:i+1].var() y1 = nmp_mean[:] y2 = nmp_var[:] y3 = nmp x = np.arange(size) plt.plot(x, y1, label='mean') plt.plot(x, y2, label='var') plt.legend() plt.show() 從以上的結果我們發現平均數的變化依然不大，而變異數的變化則變小了。我們接著重複上述的平穩性檢測步驟：\n# PM25 # ADF Test result = adfuller(data_diff.PM25.values, autolag='AIC') print(f'ADF Statistic: {result[0]}') print(f'p-value: {result[1]}') for key, value in result[4].items(): print('Critial Values:') print(f' {key}, {value}') # KPSS Test result = kpss(data_diff.PM25.values, regression='c') print('\\nKPSS Statistic: %f' % result[0]) print('p-value: %f' % result[1]) for key, value in result[3].items(): print('Critial Values:') print(f' {key}, {value}') ADF Statistic: -13.350457196046884 p-value: 5.682260865619701e-25 Critial Values: 1%, -3.4654311561944873 Critial Values: 5%, -2.8769570530458792 Critial Values: 10%, -2.574988319755886 KPSS Statistic: 0.114105 p-value: 0.100000 Critial Values: 10%, 0.347 Critial Values: 5%, 0.463 Critial Values: 2.5%, 0.574 Critial Values: 1%, 0.739 經過檢測後得到 ADF test 的 p-value 為 5.68e-25，由此可知經過一次差分後的資料便具有平穩性，這個結果會在本章節後續的預測模型使用到。\n資料預測(Data forecast) 經過前一部分的預處理後，這邊我們將會示範使用不同的預測模型來對時序資料進行預測，我們將依次使用 ARIMA、SARIMAX、auto_arima、Prophet、LSTM 與 Holt-Winter 模型。\nARIMA ARIMA 模型其實是 ARMA 模型的擴展版本，因此我們先介紹先介紹 ARMA 模型，並將 ARMA 模型拆成兩部分，分別是:\n自迴歸模型 (AR, autogressive model)：使用一個參數 p，並以前 p 個歷史值做線性組合來預測當下的數值。 移動平均模型 (MA, moving average model)：使用一個參數 q，並以前 q 個使用 AR 模型的預測誤差進行線性組合，以預測當下的數值。 而 ARIMA 模型比 ARMA 模型還多使用一個參數 d，還記得前面的平穩性檢查嗎？如果資料不具有平穩性就要做差分，而參數 d 就代表需要做差分的次數。\n以下我們使用空品資料進行演練。首先，我們將資料製圖以選擇要使用的資料片段：\nair_hour.loc['2020-06-01':'2020-06-30']['PM25'].plot(figsize=(12, 8)) 我們選擇一段想要使用的資料，並將資料分為兩部分：\n訓練資料 (train data)：用來訓練模型，找出最適合模型的參數。 測試資料 (test data)：當得到訓練模型後，可用於評估該模型在資料預測上的準確度。 在我們接下來的範例中，我們設定測試資料的長度為 48 小時 (train_len=-48)，訓練資料則為扣除最後 48 小時的全部資料。\ndata_arima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_arima.iloc[:train_len] test = data_arima.iloc[train_len:] 我們首先判斷這段資料是具有平穩性，並以差分的次數決定 d 參數的數值\n# Run Dicky-Fuller test result = adfuller(train) # Print test statistic print('The test stastics:', result[0]) # Print p-value print(\"The p-value:\",result[1]) The test stastics: -3.1129543556288826 The p-value: 0.025609243615341074 由於 p-value 已經比 0.05 小，因此我們不需要進行差分（亦即 d=0）即可繼續探討 ARIMA模型中的參數 p 和參數 q，而比較簡單的方法就是將可能的 p、q 組合分別帶入模型後，再從中判斷模型的好壞。\n我們可以使用 AIC 或 BIC 方法，來判斷模型跟訓練資料是否擬合，一般來說，其判斷出來的數值越小代表模型的效果越好。例如，我們先將 p 和 q 的範圍限制在 0~2 之間，這樣總共有 9 種可能的組合，再分別查看其 AIC 與 BIC 的數值，並以數值最小的 p 和 q 組合，作為這兩個參數的決定值。\nwarnings.filterwarnings('ignore') order_aic_bic =[] # Loop over p values from 0-2 for p in range(3): # Loop over q values from 0-2 for q in range(3): try: # create and fit ARMA(p,q) model model = sm.tsa.statespace.SARIMAX(train['PM25'], order=(p, 0, q)) results = model.fit() # Print order and results order_aic_bic.append((p, q, results.aic, results.bic)) except: print(p, q, None, None) # Make DataFrame of model order and AIC/BIC scores order_df = pd.DataFrame(order_aic_bic, columns=['p', 'q', 'aic','bic']) # lets sort them by AIC and BIC # Sort by AIC print(\"Sorted by AIC \") # print(\"\\n\") print(order_df.sort_values('aic').reset_index(drop=True)) # Sort by BIC print(\"Sorted by BIC \") # print(\"\\n\") print(order_df.sort_values('bic').reset_index(drop=True)) Sorted by AIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 Sorted by BIC p q aic bic 0 1 0 349.493661 354.046993 1 1 1 351.245734 358.075732 2 2 0 351.299268 358.129267 3 1 2 352.357930 361.464594 4 2 1 353.015921 362.122586 5 2 2 353.063243 364.446574 6 0 2 402.213407 409.043405 7 0 1 427.433962 431.987294 8 0 0 493.148188 495.424854 我們可以發現當 (p,q) = (1,0) 時，AIC 和 BIC 的值最小，代表這是最好的模型組態，因此我們決定 p、d、q 這三個參數分別設為 1, 0, 0 後就可以正式開始訓練模型。\n# Instantiate model object model = ARIMA(train, order=(1,0,0)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ============================================================================== Dep. Variable: PM25 No. Observations: 72 Model: ARIMA(1, 0, 0) Log Likelihood -168.853 Date: Fri, 26 Aug 2022 AIC 343.706 Time: 05:01:13 BIC 350.536 Sample: 06-17-2020 HQIC 346.425 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ const 6.3774 1.959 3.255 0.001 2.537 10.218 ar.L1 0.7792 0.047 16.584 0.000 0.687 0.871 sigma2 6.2934 0.746 8.438 0.000 4.832 7.755 =================================================================================== Ljung-Box (L1) (Q): 0.08 Jarque-Bera (JB): 76.49 Prob(Q): 0.77 Prob(JB): 0.00 Heteroskedasticity (H): 2.30 Skew: 1.44 Prob(H) (two-sided): 0.05 Kurtosis: 7.15 =================================================================================== 接著我們使用測試資料進行預測，並評估其預測的效果，從圖形化的資料呈現中，我們可以發現資料預測結果的曲線太過平滑，和實際上的數值差異很大。事實上，若觀察整體資料的變化趨勢，會發現資料本身存在有規律的起伏，而 ARIMA 只能預測出資料的趨勢，若要準確的預測資料的數值，其結果仍有極大的差距。\ndata_arima['forecast'] = results.predict(start=24*5-48, end=24*5) data_arima[['PM25', 'forecast']].plot(figsize=(12, 8)) SARIMAX data_sarimax = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_sarimax.iloc[:train_len] test = data_sarimax.iloc[train_len:] 我們接著介紹 SARIMAX 模型。SARIMAX 模型共有七個參數，分別是 p, d, q, P, D, Q, s；這些參數可以分為兩組：第一組為 order=(p, d, q) 這三個參數跟 ARIMA 模型的參數一樣；另一組是 seasonal_order=(P, D, Q, s)，也就是週期性的 AR 模型參數、週期性的差分次數、週期性的MA模型參數，最後再加上一個週期性長度的參數。\n參數 說明 p AR 模型參數 d 達到平穩性所需要的差分次數 q MA 模型參數 P 週期性的 AR 模型參數 D 週期上達到平穩性所需要的差分次數 Q 週期性的 MA 模型參數 s 週期長度 由於從先前的觀察可以發現，這些資料大致上約 24 個小時會有一個週期性的變化，因此我們讓 s=24，並用下列的指令進行模型建立。\n# Instantiate model object model = SARIMAX(train, order=(1,0,0), seasonal_order=(0, 1, 0, 24)) # Fit model results = model.fit() print(results.summary()) results.plot_diagnostics(figsize=(10, 10)) SARIMAX Results ========================================================================================== Dep. Variable: PM25 No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:26 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== 接下來我們使用測試資料進行資料預測，並將預測結果用視覺化方式呈現，可以發現相較於 ARIMA 模型＃，SARIMA 模型的預測結果雖然仍有待加強，但已比 ARIMA 模型進步許多。\n# 總共有五天的資料，要預測最後兩天的部分，因此預測的起始小時為 24*5-48，結束小時為 24*5 data_sarimax['forecast'] = results.predict(start=24*5-48, end=24*5) data_sarimax[['PM25', 'forecast']].plot(figsize=(12, 8)) auto_arima 我們使用 pmdarima 這個 Python 套件，這個套件類似 R 語言中的 auto.arima 模型，可以自動尋找最合適的 ARIMA 模型參數，增加使用者在使用 ARIMA 模型時的方便性。目前 pmdarima 套件中的 pmdarima.ARIMA 物件，其實便同時包含了 ARMA, ARIMA 和 SARIMAX 這三種模型，而使用 pmdarima.auto_arima 方法時，只要提供參數 p, q, P, Q 的範圍，便能在指定的範圍內尋找出最適合的參數組合。\n我們接下來實作 pmdarima.auto_arima 的使用方法，先把資料集切分為訓練資料與預測資料：\ndata_autoarima = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_autoarima.iloc[:train_len] test = data_autoarima.iloc[train_len:] 針對 p, q, P, Q 這四個參數，我們分別用 start 和 max 來指定對應的範圍，同時設定週期性參數 seasonal 為 True，並且設定週期變數 m 為 24 小時。接著我們便可以直接執行得到最佳的模型參數組合和模型擬合結果。\nresults = pm.auto_arima(train,start_p=0, d=0, start_q=0, max_p=5, max_d=5, max_q=5, start_P=0, D=1, start_Q=0, max_P=5, max_D=5, max_Q=5, m=24, seasonal=True, error_action='warn', trace = True, supress_warnings=True, stepwise = True, random_state=20, n_fits = 20) print(results.summary()) Performing stepwise search to minimize aic ARIMA(0,0,0)(0,1,0)[24] intercept : AIC=268.023, Time=0.04 sec ARIMA(1,0,0)(1,1,0)[24] intercept : AIC=247.639, Time=0.85 sec ARIMA(0,0,1)(0,1,1)[24] intercept : AIC=250.711, Time=0.79 sec ARIMA(0,0,0)(0,1,0)[24] : AIC=271.305, Time=0.04 sec ARIMA(1,0,0)(0,1,0)[24] intercept : AIC=247.106, Time=0.09 sec ARIMA(1,0,0)(0,1,1)[24] intercept : AIC=247.668, Time=0.45 sec ARIMA(1,0,0)(1,1,1)[24] intercept : AIC=inf, Time=2.63 sec ARIMA(2,0,0)(0,1,0)[24] intercept : AIC=249.013, Time=0.15 sec ARIMA(1,0,1)(0,1,0)[24] intercept : AIC=248.924, Time=0.21 sec ARIMA(0,0,1)(0,1,0)[24] intercept : AIC=250.901, Time=0.11 sec ARIMA(2,0,1)(0,1,0)[24] intercept : AIC=250.579, Time=0.30 sec ARIMA(1,0,0)(0,1,0)[24] : AIC=246.926, Time=0.06 sec ARIMA(1,0,0)(1,1,0)[24] : AIC=247.866, Time=0.28 sec ARIMA(1,0,0)(0,1,1)[24] : AIC=247.933, Time=0.31 sec ARIMA(1,0,0)(1,1,1)[24] : AIC=inf, Time=2.35 sec ARIMA(2,0,0)(0,1,0)[24] : AIC=248.910, Time=0.08 sec ARIMA(1,0,1)(0,1,0)[24] : AIC=248.893, Time=0.09 sec ARIMA(0,0,1)(0,1,0)[24] : AIC=252.779, Time=0.08 sec ARIMA(2,0,1)(0,1,0)[24] : AIC=250.561, Time=0.17 sec Best model: ARIMA(1,0,0)(0,1,0)[24] Total fit time: 9.122 seconds SARIMAX Results ========================================================================================== Dep. Variable: y No. Observations: 72 Model: SARIMAX(1, 0, 0)x(0, 1, 0, 24) Log Likelihood -121.463 Date: Fri, 26 Aug 2022 AIC 246.926 Time: 05:01:37 BIC 250.669 Sample: 06-17-2020 HQIC 248.341 - 06-19-2020 Covariance Type: opg ============================================================================== coef std err z P\u003e|z| [0.025 0.975] ------------------------------------------------------------------------------ ar.L1 0.6683 0.069 9.698 0.000 0.533 0.803 sigma2 9.1224 1.426 6.399 0.000 6.328 11.917 =================================================================================== Ljung-Box (L1) (Q): 0.11 Jarque-Bera (JB): 5.70 Prob(Q): 0.75 Prob(JB): 0.06 Heteroskedasticity (H): 2.03 Skew: 0.42 Prob(H) (two-sided): 0.17 Kurtosis: 4.46 =================================================================================== 最後我們使用尋找到的最佳模型進行資料預測，並且將預測結果與測試資料用疊圖的方式繪製在同一張圖上，由於本次所找到最佳模型即為剛剛介紹 SARIMAX 時的最佳模型參數組合，因此兩者的預測結果也大致相同。\nresults.predict(n_periods=10) 2020-06-20 00:00:00 10.371336 2020-06-20 01:00:00 13.142043 2020-06-20 02:00:00 13.505843 2020-06-20 03:00:00 9.506395 2020-06-20 04:00:00 7.450378 2020-06-20 05:00:00 7.782850 2020-06-20 06:00:00 7.633757 2020-06-20 07:00:00 5.200781 2020-06-20 08:00:00 3.634188 2020-06-20 09:00:00 3.946824 Freq: H, dtype: float64 data_autoarima['forecast']= pd.DataFrame(results.predict(n_periods=48), index=test.index) data_autoarima[['PM25', 'forecast']].plot(figsize=(12, 8)) Prophet 我們接下來使用 kats 套件中提供的 Prophet 模型進行資料預測，這個模型是由 Facebook 的資料科學團隊提出，擅長針對週期性強的時間序列資料進行預測，並且可以容忍缺失資料 (missing data)、資料偏移 (shift) 以及偏離值 (outlier)。\n我們首先把資料集切分為訓練資料與預測資料，並用折線圖的方式呈現訓練資料的變化狀況。\ndata_prophet = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_prophet.iloc[:train_len] test = data_prophet.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) 我們接著使用 ProphetParams 設定 Prophet 模型的參數，並將訓練資料與參數用於初始化設定 ProphetModel，接著我們使用 fit 方法建立模型，並用 predict 方法進行資料預測，便能得到最後預測的結果。\n# Specify parameters params = ProphetParams(seasonality_mode=\"multiplicative\") # Create a model instance m = ProphetModel(trainData, params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq=\"H\") data_prophet['forecast'] = fcst[['time','fcst']].set_index('time') fcst time\tfcst\tfcst_lower fcst_upper 0\t2020-06-20 00:00:00\t14.705192\t12.268361\t17.042476 1\t2020-06-20 01:00:00\t15.089580\t12.625568\t17.573396 2\t2020-06-20 02:00:00\t14.921077\t12.459802\t17.411335 3\t2020-06-20 03:00:00\t13.846131\t11.444988\t16.200284 4\t2020-06-20 04:00:00\t12.278140\t9.863531\t14.858334 5\t2020-06-20 05:00:00\t10.934739\t8.372450\t13.501025 6\t2020-06-20 06:00:00\t10.126712\t7.654647\t12.658054 7\t2020-06-20 07:00:00\t9.535067\t7.034313\t11.762639 8\t2020-06-20 08:00:00\t8.661877\t6.255147\t11.132732 9\t2020-06-20 09:00:00\t7.424133\t5.055052\t9.770750 10\t2020-06-20 10:00:00\t6.229786\t3.640543\t8.625856 11\t2020-06-20 11:00:00\t5.464764\t3.039011\t7.939283 12\t2020-06-20 12:00:00\t4.998005\t2.692023\t7.550191 13\t2020-06-20 13:00:00\t4.334771\t1.961382\t6.506875 14\t2020-06-20 14:00:00\t3.349172\t1.059836\t5.768178 15\t2020-06-20 15:00:00\t2.819902\t0.399350\t5.226658 16\t2020-06-20 16:00:00\t4.060070\t1.556264\t6.322976 17\t2020-06-20 17:00:00\t7.792830\t5.331987\t10.237182 18\t2020-06-20 18:00:00\t13.257767\t10.873149\t15.542380 19\t2020-06-20 19:00:00\t18.466805\t15.895210\t20.874602 20\t2020-06-20 20:00:00\t21.535994\t19.150397\t23.960260 21\t2020-06-20 21:00:00\t22.005943\t19.509141\t24.691836 22\t2020-06-20 22:00:00\t21.014449\t18.610361\t23.661906 23\t2020-06-20 23:00:00\t20.191905\t17.600568\t22.868388 24\t2020-06-21 00:00:00\t20.286952\t17.734177\t22.905280 25\t2020-06-21 01:00:00\t20.728067\t18.235829\t23.235212 26\t2020-06-21 02:00:00\t20.411124\t17.755181\t22.777073 27\t2020-06-21 03:00:00\t18.863739\t16.261775\t21.315573 28\t2020-06-21 04:00:00\t16.661351\t13.905466\t19.374726 29\t2020-06-21 05:00:00\t14.781150\t12.401465\t17.499478 30\t2020-06-21 06:00:00\t13.637436\t11.206142\t16.239831 31\t2020-06-21 07:00:00\t12.793609\t9.940829\t15.319559 32\t2020-06-21 08:00:00\t11.580455\t9.059603\t14.261605 33\t2020-06-21 09:00:00\t9.891025\t7.230943\t12.471543 34\t2020-06-21 10:00:00\t8.271552\t5.840853\t10.677227 35\t2020-06-21 11:00:00\t7.231671\t4.829449\t9.733231 36\t2020-06-21 12:00:00\t6.592515\t4.108251\t9.107216 37\t2020-06-21 13:00:00\t5.699548\t3.288052\t8.019402 38\t2020-06-21 14:00:00\t4.389985\t1.848621\t6.825121 39\t2020-06-21 15:00:00\t3.685033\t1.196467\t6.150064 40\t2020-06-21 16:00:00\t5.289956\t2.907623\t8.012851 41\t2020-06-21 17:00:00\t10.124029\t7.397842\t12.676256 42\t2020-06-21 18:00:00\t17.174959\t14.670539\t19.856592 43\t2020-06-21 19:00:00\t23.856724\t21.102924\t26.712359 44\t2020-06-21 20:00:00\t27.746195\t24.636118\t30.673178 45\t2020-06-21 21:00:00\t28.276321\t25.175013\t31.543197 46\t2020-06-21 22:00:00\t26.932054\t23.690073\t29.882014 47\t2020-06-21 23:00:00\t25.811943\t22.960132\t28.912079 data_prophet timestamp\tPM25\tforecast 2020-06-17 00:00:00\t6.300000\tNaN 2020-06-17 01:00:00\t11.444444\tNaN 2020-06-17 02:00:00\t6.777778\tNaN 2020-06-17 03:00:00\t4.875000\tNaN 2020-06-17 04:00:00\t5.444444\tNaN ...\t...\t... 2020-06-21 19:00:00\t18.777778\t23.856724 2020-06-21 20:00:00\t21.400000\t27.746195 2020-06-21 21:00:00\t11.222222\t28.276321 2020-06-21 22:00:00\t9.800000\t26.932054 2020-06-21 23:00:00\t8.100000\t25.811943 我們接著使用 ProphetModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\nm.plot() 為了更容易觀察預測結果的正確性，我們使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，藍色曲線與同時間的黑色曲線在變化趨勢與數值區間皆十分相似，整體來說預測結果已可算是差強人意。\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() LSTM 接下來，我們介紹使用 LSTM (long short-term memory) 模型進行資料預測。LSTM 模型是一種適合使用在連續資料的預測模型，因為它會對不同時間的資料產生不同的長短期記憶，並藉此預測出最後的結果。目前在 kats 套件中就有提供 LSTM 模型，因此我們可以直接用類似使用 Prophet 模型的語法進行操作。\n我們首先把資料集切分為訓練資料與預測資料，並用繪圖的方式查看訓練資料的變化狀況。\ndata_lstm = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_lstm.iloc[:train_len] test = data_lstm.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) 接著我們依序選擇 LSTM 模型的各項參數，分別是訓練次數 (num_epochs)、一次讀入的資料時間長度 (time_window)、還有跟長短期記憶比較相關的神經網路層數 (hidden_size)，然後便可以直接進行模型訓練與資料預測。\nparams = LSTMParams( hidden_size=10, # number of hidden layers time_window=24, num_epochs=30 ) m = LSTMModel(trainData, params) m.fit() fcst = m.predict(steps=48, freq=\"H\") data_lstm['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst\tfcst_lower\tfcst_upper 0\t2020-06-20 00:00:00\t11.905971\t11.310672\t12.501269 1\t2020-06-20 01:00:00\t10.804338\t10.264121\t11.344554 2\t2020-06-20 02:00:00\t9.740741\t9.253704\t10.227778 3\t2020-06-20 03:00:00\t8.696406\t8.261586\t9.131226 4\t2020-06-20 04:00:00\t7.656923\t7.274077\t8.039769 5\t2020-06-20 05:00:00\t6.608442\t6.278019\t6.938864 6\t2020-06-20 06:00:00\t5.543790\t5.266600\t5.820979 7\t2020-06-20 07:00:00\t4.469023\t4.245572\t4.692474 8\t2020-06-20 08:00:00\t3.408312\t3.237897\t3.578728 9\t2020-06-20 09:00:00\t2.411980\t2.291381\t2.532578 10\t2020-06-20 10:00:00\t1.564808\t1.486567\t1.643048 11\t2020-06-20 11:00:00\t0.982147\t0.933040\t1.031255 12\t2020-06-20 12:00:00\t0.792612\t0.752981\t0.832242 13\t2020-06-20 13:00:00\t1.105420\t1.050149\t1.160691 14\t2020-06-20 14:00:00\t1.979013\t1.880062\t2.077964 15\t2020-06-20 15:00:00\t3.408440\t3.238018\t3.578862 16\t2020-06-20 16:00:00\t5.337892\t5.070997\t5.604786 17\t2020-06-20 17:00:00\t7.659332\t7.276365\t8.042299 18\t2020-06-20 18:00:00\t10.104147\t9.598940\t10.609355 19\t2020-06-20 19:00:00\t12.047168\t11.444809\t12.649526 20\t2020-06-20 20:00:00\t12.880240\t12.236228\t13.524252 21\t2020-06-20 21:00:00\t12.748750\t12.111312\t13.386187 22\t2020-06-20 22:00:00\t12.128366\t11.521947\t12.734784 23\t2020-06-20 23:00:00\t11.311866\t10.746273\t11.877459 24\t2020-06-21 00:00:00\t10.419082\t9.898128\t10.940036 25\t2020-06-21 01:00:00\t9.494399\t9.019679\t9.969119 26\t2020-06-21 02:00:00\t8.551890\t8.124296\t8.979485 27\t2020-06-21 03:00:00\t7.592260\t7.212647\t7.971873 28\t2020-06-21 04:00:00\t6.613075\t6.282421\t6.943729 29\t2020-06-21 05:00:00\t5.614669\t5.333936\t5.895402 30\t2020-06-21 06:00:00\t4.605963\t4.375664\t4.836261 31\t2020-06-21 07:00:00\t3.611552\t3.430974\t3.792129 32\t2020-06-21 08:00:00\t2.679572\t2.545593\t2.813550 33\t2020-06-21 09:00:00\t1.887442\t1.793070\t1.981814 34\t2020-06-21 10:00:00\t1.340268\t1.273255\t1.407282 35\t2020-06-21 11:00:00\t1.156494\t1.098669\t1.214318 36\t2020-06-21 12:00:00\t1.440240\t1.368228\t1.512252 37\t2020-06-21 13:00:00\t2.251431\t2.138859\t2.364002 38\t2020-06-21 14:00:00\t3.592712\t3.413076\t3.772347 39\t2020-06-21 15:00:00\t5.415959\t5.145161\t5.686757 40\t2020-06-21 16:00:00\t7.613187\t7.232528\t7.993847 41\t2020-06-21 17:00:00\t9.918564\t9.422636\t10.414493 42\t2020-06-21 18:00:00\t11.755348\t11.167580\t12.343115 43\t2020-06-21 19:00:00\t12.576593\t11.947764\t13.205423 44\t2020-06-21 20:00:00\t12.489052\t11.864599\t13.113504 45\t2020-06-21 21:00:00\t11.915885\t11.320090\t12.511679 46\t2020-06-21 22:00:00\t11.133274\t10.576610\t11.689938 47\t2020-06-21 23:00:00\t10.264495\t9.751270\t10.777719 我們一樣使用 LSTMModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\nm.plot() 為了觀察預測結果的正確性，我們也使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，從圖中可以觀察到藍色曲線與同時間的黑色曲線在變化趨勢上大致一致，但整體來說資料預測的結果 (藍線) 則比實際測試資料 (黑線) 的數值略低一些。\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() Holt-Winter 我們也使用 kats 套件提供的 Holt-Winter 模型，這是一種利用移動平均的概念，分配歷史資料的權重，以進行資料預測的方法。我們一樣先把資料集切分為訓練資料與預測資料，並用繪圖的方式查看訓練資料的變化狀況。\ndata_hw = air_hour.loc['2020-06-17':'2020-06-21'] train_len = -48 train = data_hw.iloc[:train_len] test = data_hw.iloc[train_len:] trainData = TimeSeriesData(train.reset_index(), time_col_name='timestamp') trainData.plot(cols=[\"PM25\"]) 接著我們需要設定 Holt-Winter 模型的參數，分別是設定使用加法或乘法來分解時序資料 (以下範例使用乘法 mul)，以及週期性的長度 (以下範例設為 24 小時)，然後便可以進行模型訓練與資料預測。\nwarnings.simplefilter(action='ignore') # Specify parameters params = HoltWintersParams( trend=\"mul\", seasonal=\"mul\", seasonal_periods=24, ) # Create a model instance m = HoltWintersModel( data=trainData, params=params) # Fit mode m.fit() # Forecast fcst = m.predict(steps=48, freq='H') data_hw['forecast'] = fcst[['time', 'fcst']].set_index('time') fcst time\tfcst 72\t2020-06-20 00:00:00\t14.140232 73\t2020-06-20 01:00:00\t14.571588 74\t2020-06-20 02:00:00\t12.797056 75\t2020-06-20 03:00:00\t10.061594 76\t2020-06-20 04:00:00\t9.927476 77\t2020-06-20 05:00:00\t8.732691 78\t2020-06-20 06:00:00\t10.257460 79\t2020-06-20 07:00:00\t8.169070 80\t2020-06-20 08:00:00\t6.005400 81\t2020-06-20 09:00:00\t5.038056 82\t2020-06-20 10:00:00\t6.391835 83\t2020-06-20 11:00:00\t5.435677 84\t2020-06-20 12:00:00\t3.536135 85\t2020-06-20 13:00:00\t2.725477 86\t2020-06-20 14:00:00\t2.588198 87\t2020-06-20 15:00:00\t2.967987 88\t2020-06-20 16:00:00\t3.329448 89\t2020-06-20 17:00:00\t4.409821 90\t2020-06-20 18:00:00\t10.295263 91\t2020-06-20 19:00:00\t10.587033 92\t2020-06-20 20:00:00\t14.061718 93\t2020-06-20 21:00:00\t18.597275 94\t2020-06-20 22:00:00\t12.040684 95\t2020-06-20 23:00:00\t12.124081 96\t2020-06-21 00:00:00\t13.522973 97\t2020-06-21 01:00:00\t13.935499 98\t2020-06-21 02:00:00\t12.238431 99\t2020-06-21 03:00:00\t9.622379 100\t2020-06-21 04:00:00\t9.494116 101\t2020-06-21 05:00:00\t8.351486 102\t2020-06-21 06:00:00\t9.809694 103\t2020-06-21 07:00:00\t7.812468 104\t2020-06-21 08:00:00\t5.743248 105\t2020-06-21 09:00:00\t4.818132 106\t2020-06-21 10:00:00\t6.112815 107\t2020-06-21 11:00:00\t5.198396 108\t2020-06-21 12:00:00\t3.381773 109\t2020-06-21 13:00:00\t2.606503 110\t2020-06-21 14:00:00\t2.475216 111\t2020-06-21 15:00:00\t2.838426 112\t2020-06-21 16:00:00\t3.184109 113\t2020-06-21 17:00:00\t4.217320 114\t2020-06-21 18:00:00\t9.845847 115\t2020-06-21 19:00:00\t10.124881 116\t2020-06-21 20:00:00\t13.447887 117\t2020-06-21 21:00:00\t17.785455 118\t2020-06-21 22:00:00\t11.515076 119\t2020-06-21 23:00:00\t11.594832 我們一樣使用 HoltWintersModel 內建的繪圖方法，將訓練資料 (黑線) 與預測結果 (藍線) 繪製出來。\nm.plot() 為了觀察預測結果的正確性，我們也使用另一種繪圖的方式，將訓練資料 (黑線)、測試資料 (黑線) 與預測結果 (藍線) 同時繪製出來，從圖中可以觀察到藍色曲線與同時間的黑色曲線在變化趨勢與數值區間皆大致一致，但整體來說資料預測的結果 (藍線) 對於上升坡段的反應略慢於測試資料 (黑線)。\nfig, ax = plt.subplots(figsize=(12, 7)) train.plot(ax=ax, label='train', color='black') test.plot(ax=ax, color='black') fcst.plot(x='time', y='fcst', ax=ax, color='blue') # ax.fill_between(test.index, fcst['fcst_lower'], fcst['fcst_upper'], alpha=0.1) ax.get_legend().remove() 綜合比較 最後，為了方便觀察與比較起見，我們將剛剛介紹的六種預測模型的預測結果，同時繪製在下方的圖中（註：要先跑過上面所有預測模型的程式碼才看得到六張圖），可以清楚地觀察與比較六種模型在不同時間區間與曲線變化特性下的預測準確度，方便使用者決定最終的模型選擇，以及未來的可能應用。\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 8)) data_arima[['PM25', 'forecast']].plot(ax=axes[0, 0], title='ARIMA') data_sarimax[['PM25', 'forecast']].plot(ax=axes[1, 0], title='SARIMAX') data_autoarima[['PM25', 'forecast']].plot(ax=axes[2, 0], title='auto_arima') data_prophet[['PM25', 'forecast']].plot(ax=axes[0, 1], title='Prophet') data_lstm[['PM25', 'forecast']].plot(ax=axes[1, 1], title='LSTM') data_hw[['PM25', 'forecast']].plot(ax=axes[2, 1], title='Holt-Winter') fig.tight_layout(pad=1, w_pad=2, h_pad=5) 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Rob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice, 3rd edition (https://otexts.com/fpp3/) Stationarity, NIST Engineering Statistics Handbook (https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc442.htm) Unit root test - Wikipedia (https://en.wikipedia.org/wiki/Unit_root_test) Akaike information criterion (AIC) - Wikipedia (https://en.wikipedia.org/wiki/Akaike_information_criterion) Bayesian information criterion (BIC) - Wikipedia (https://en.wikipedia.org/wiki/Bayesian_information_criterion) ARIMA models (https://otexts.com/fpp2/arima.html) SARIMAX: Introduction (https://www.statsmodels.org/stable/examples/notebooks/generated/statespace_sarimax_stata.html) Prophet: Forecasting at scale (https://facebook.github.io/prophet/) Long short-term memory (LSTM) – Wikipedia (https://en.wikipedia.org/wiki/Long_short-term_memory) Time Series Forecasting with ARIMA Models In Python [Part 1] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-1-c2940a7dbc48?gi=264dc7630363) Time Series Forecasting with ARIMA Models In Python [Part 2] | by Youssef Hosni | May, 2022 | Towards AI (https://pub.towardsai.net/time-series-forecasting-with-arima-models-in-python-part-2-91a30d10efb0) Kats: a Generalizable Framework to Analyze Time Series Data in Python | by Khuyen Tran | Towards Data Science (https://towardsdatascience.com/kats-a-generalizable-framework-to-analyze-time-series-data-in-python-3c8d21efe057) Kats - Time Series Forecasting By Facebook | by Himanshu Sharma | MLearning.ai | Medium (https://medium.com/mlearning-ai/kats-time-series-forecasting-by-facebook-a2741794d814) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，套用現有的 Python 資料科學套件 (例如 scikit-learn、Kats 等)，用動手實作的方式，比較各種套件所內建的不同資料預測模型的使用方法與預測結果，用製圖的方式進行資料呈現，並且探討不同的資料集與不同時間解析度的資料預測，在真實場域所代表的意義，以及可能衍生的應用。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "4.2. 時間序列資料預測",
    "uri": "/ch4/ch4.2/"
  },
  {
    "content": "\n{{ \u003c toc \u003e }}\n廣布在生活環境中的微型測站，協助我們掌握細緻的環境變化，並可據以決策跟行動。所以，清楚地掌握測站間的分布和數據特性，也是我們在分析測站數據時的重要基礎。這些測站除了本身的位置可能會形成某種幾何結構或空間群聚。同時，我們也可以依照測站位置與數值的差異，去推估沒有測站的區域的數值，從而獲得一個更為全面的數值分佈狀況，並從中探索感測數值與環境因子間的相關性。在這一個段落中，我們會利用水利署在不同縣市的淹水感測器與地下水位站資料，來進行一些簡單的空間分析。\n勢力分佈圖 (Voronoi diagram) 首先，我們可能需要釐清個別測站的服務/防守範圍，並以此範圍中的測站數據來代表該區的現況。這個時候，我們可以利用沃羅諾伊圖（voronoi diagram）的方法去找尋這個範圍。沃羅諾伊圖的原理是在兩個相鄰測站間建立一條垂直平分線段，並藉由整合這些線段以構成一個多邊形；每個多邊形範圍的中心點就是測站，而該測站的數值則約可代表這個範圍內的數值。在這個範例中，我們嘗試利用嘉義縣、嘉義市的淹水感測器資料，去練習建立沃羅諾伊圖，這樣我們就可以初略知道這些淹水感測器的勢力分佈範圍。\nimport matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import urllib.request import ssl import json #install geopython libraries !apt install gdal-bin python-gdal python3-gdal #install python3-rtree - Geopandas requirement !apt install python3-rtree #install geopandas !pip install geopandas #install pykrige !pip install pykrige #install elevation !pip install elevation #install affine rasterio !pip install affine rasterio #install descartes - Geopandas requirement !pip install descartes import geopandas as gpd !pip install pyCIOT import pyCIOT.data as CIoT # 前往政府開放資料庫下載 縣市界線(TWD97經緯度) 的資料，並解壓縮到名為 shp 的資料夾 !wget -O \"shp.zip\" -q \"https://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=72874C55-884D-4CEA-B7D6-F60B0BE85AB0\" !unzip shp.zip -d shp # 以水利署淹水感測器資料為例，資料集 gpd 為感測器數值與位置資料、basemap為台灣縣市地理邊界 shp file # 以pyCIOT取得資料 wa = CIoT.Water().get_data(src=\"FLOODING:WRA\") wa2 = CIoT.Water().get_data(src=\"FLOODING:WRA2\") flood_list = wa + wa2 county = gpd.read_file('/content/shp/COUNTY_MOI_1090820.shp') basemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] flood_df = pd.DataFrame([],columns = ['name', 'Observations','lon', 'lat']) for i in flood_list: #print(i['data'][0]) if len(i['data'])\u003e0: df = pd.DataFrame([[i['properties']['stationName'],i['data'][0]['values'][0]['value'],i['location']['longitude'],i['location']['latitude']]],columns = ['name', 'Observations','lon', 'lat']) else : df = pd.DataFrame([[i['properties']['stationName'],-999,-999,-999]],columns = ['name', 'Observations','lon', 'lat']) flood_df = pd.concat([flood_df,df]) #print(df) result_df = flood_df.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") station=result_df.sort_values(by=['lon', 'lat']) station = station[station.lon!=-999] station.reset_index(inplace=True, drop=True) gdf_flood = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_flood, basemap, how='intersection') from scipy.spatial import Voronoi, voronoi_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) inputp = intersected_data[['lon','lat']] basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); vor = Voronoi(inputp) voronoi_plot_2d(vor,ax = ax,show_vertices=False,) plt.show() 此外，我們也可以利用德勞內三角分割（Delaunay triangulation）去描述測站的服務/防守範圍；它的原理是以單一測站為中心，並尋找最鄰近的兩個點，以連接成一個三角形範圍。若我們將三角形的範圍視為一個均質的平面，而這個範圍內的感測數值則可用三個節點測站值的平均來替代。\n整體而言，這兩個演算法都可以協助我們以圖形的方式，理解感測器在空間上的分佈，及其所建立的空間結構。\nfrom scipy.spatial import Delaunay, delaunay_plot_2d import numpy as np fig, ax = plt.subplots(figsize=(6, 10)) #input should be array inputp = np.array(inputp) tri = Delaunay(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); delaunay_plot_2d(tri,ax=ax) plt.show() 最小範圍多邊形/凸包 (Convex hull) 最小範圍多邊形的演算法，是從一群測站中，選出位於最邊緣的若干個測站構成一個能含納所有的點位、且邊長最小的多邊形，這樣我們就可以在一堆測站中，找到一個群聚的範圍，並用這個範圍來發展一些計算。最小範圍多邊形的演算法，主要是依照測站的x座標排序測站位置，而當 X 座標相同則以 Y 座標大小排序，從而找到最外圍的端點並連接成為多邊形 (當然類似的概念還有許多方法)，透過最小範圍多邊形的演算，我們可以評估測站的有效監測範圍。所以，我們也可以利用嘉義縣、嘉義市的淹水感測器分佈，已瞭解這些淹水感測器的覆蓋範圍。\nfrom scipy.spatial import ConvexHull, convex_hull_plot_2d fig, ax = plt.subplots(figsize=(6, 10)) hull = ConvexHull(inputp) basemap.plot(ax=ax, facecolor='none', edgecolor='purple'); convex_hull_plot_2d(hull,ax=ax) plt.tight_layout() 空間群聚 (Clustering) 正如前文所說，越鄰近的測站，其周邊環境中的干擾因子可能也越相似，所以我們可以利用 Kmeans 這種分類演算法，將測站進行分群，以進一步探勘其感測數據與環境因子間的關係。Kmeans主要是根據我們預先設定的分群的數量 n，並隨機尋找 n 個點做為中心，去尋找周邊的鄰居，經由量測樣本點與中心點的直線距離，去把樣本點分群並計算各群的平均值，最後，重複前述的程序直到所有樣本點與中心點的距離平均值最短，即可完成分群。\n此外，測站不單只有空間位置，也會有其量測數值，如果我們同時參考其空間位置與量測數值，將量測數值較為相似且地理位置相近的測站聚集起來，這些聚集的測站所遭受的環境干擾因子亦較為接近，而透過測站的空間位置，亦正可以反映出不同環境干擾因子在地理空間上的影像範圍。因此，空間群聚也是地理空間資料分析的重要一環。在這個案例中，我們也嘗試以雲林地區的地下水位站為案例，去描述這些測站的空間群聚狀況。\n# 獲取地下水位站資料 count = 733 num = 0 water_level = pd.DataFrame([]) while(num\u003c=count): url_level = \"https://sta.ci.taiwan.gov.tw/STA_WaterResource_v2/v1.0/Datastreams?$skip=\"+str(num)+\"\u0026$filter=%28%28Thing%2Fproperties%2Fauthority_type+eq+%27%E6%B0%B4%E5%88%A9%E7%BD%B2%27%29+and+substringof%28%27Datastream_Category_type%3D%E5%9C%B0%E4%B8%8B%E6%B0%B4%E4%BD%8D%E7%AB%99%27%2CDatastreams%2Fdescription%29%29\u0026$expand=Thing,Thing%28%24expand%3DLocations%29,Observations%28%24top%3D1%3B%24orderby%3DphenomenonTime+desc%3B%24top%3D1%29\u0026$count=true\" ssl._create_default_https_context = ssl._create_unverified_context r_l = urllib.request.urlopen(url_level) string_l = r_l.read().decode('utf-8') jf_level = json.loads(string_l) station = pd.DataFrame(jf_level['value']).filter(items=['Thing','observedArea','Observations']) station['lat']=station['observedArea'] for i in range(len(station)): station['Thing'][i] = station['Thing'][i]['properties']['stationName'] if pd.isnull(station['observedArea'][i]): station['lat'][i]=-1 station['observedArea'][i]=-1 else: station['lat'][i]=station['lat'][i]['coordinates'][1] station['observedArea'][i]=station['observedArea'][i]['coordinates'][0] if len(station['Observations'][i])!=0: station['Observations'][i] = station['Observations'][i][0]['result'] else: station['Observations'][i] = -1 station = station.rename(columns={\"Thing\": \"name\", 'observedArea': 'lon'}) if num ==0 : water_level = station else: water_level = pd.concat([water_level, station]) num+=100 result_df = water_level.drop_duplicates(subset=['name'], keep='first') station=result_df.sort_values(by=['lon', 'lat']) station.reset_index(inplace=True, drop=True) station = station[station.lon!=-1] gdf_level = gpd.GeoDataFrame( station, geometry=gpd.points_from_xy(station.lon, station.lat),crs=\"EPSG:4326\") # 與雲林縣範圍 intersect basemap = county.loc[county['COUNTYNAME'].isin([\"雲林縣\"])] basemap = basemap.set_crs(4326,allow_override=True) intersected_data = gpd.overlay(gdf_level, basemap, how='intersection') from sklearn.cluster import KMeans from scipy.spatial import ConvexHull import folium clusterp = intersected_data[[\"name\",\"lon\", 'lat', 'Observations']] # 1. 利用 kmeans 對資料進行分群 #1.1 資料前處理 X = clusterp.iloc[:, 1:3].values # 利用 elbow method 看應該分成幾群 wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1, 11), wcss) plt.title('The Elbow Method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show() # 1.2 根據分群的數量訓練 K-Means model kmeans = KMeans(n_clusters = 3, init = 'k-means++', random_state = 42) y_kmeans = kmeans.fit_predict(X) # 1.3 map data back to df clusterp['cluster'] = y_kmeans+1 # to step up to group 1 to 4 # 2.將資料畫在地圖上 # 創建底圖(m) m = folium.Map(location=[clusterp['lat'].mean(), clusterp['lon'].mean()], tiles='CartoDB positron', zoom_start=7) # 根據分群數量建立圖層 layer1 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup1\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer1) layer2 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup2\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer2) layer3 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup3\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer3) layer4 = folium.FeatureGroup(name= '\u003cu\u003e\u003cb\u003egroup4\u003c/b\u003e\u003c/u\u003e',show= True) m.add_child(layer4) # 建立圖標(利用 html 的 CSS 語法) my_symbol_css_class= \"\"\" \u003cstyle\u003e .fa-g1:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4:before { font-family: Arial; font-weight: bold; font-size: 12px; color: black; background-color:white; border-radius: 10px; white-space: pre; content: ' g4 '; } .fa-g1bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g1 '; } .fa-g2bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g2 '; } .fa-g3bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g3 '; } .fa-g4bad:before { font-family: Arial; font-weight: bold; font-size: 12px; color: white; background-color:red; border-radius: 10px; white-space: pre; content: ' g4 '; } \u003c/style\u003e \"\"\" # 將語法加進地圖中 m.get_root().html.add_child(folium.Element(my_symbol_css_class)) # 創建每個圖標的 icon for index, row in clusterp.iterrows(): if row['cluster'] == 1: color='black' fa_symbol = 'fa-g1' lay = layer1 elif row['cluster'] == 2: color='purple' fa_symbol = 'fa-g2' lay = layer2 elif row['cluster'] == 3: color='orange' fa_symbol = 'fa-g3' lay = layer3 elif row['cluster'] == 4: color='blue' fa_symbol = 'fa-g4' lay = layer4 folium.Marker( location=[row['lat'], row['lon']], title = row['name']+ 'group:{}'.format(str(row[\"cluster\"])), popup = row['name']+ 'group:{},value:{}'.format(str(row[\"cluster\"]),str(row['Observations'])), icon= folium.Icon(color=color, icon=fa_symbol, prefix='fa')).add_to(lay) # 準備將資料畫上地圖 layer_list = [layer1,layer2,layer3,layer4] color_list = ['black','purple','orange','blue'] for g in clusterp['cluster'].unique(): # 利用 ConvexHull 找到每個群的邊界 # 首先，取出每群的 lat lon latlon_cut =clusterp[clusterp['cluster']==g].iloc[:, 1:3] # 再來，使用 scipy 中的 ConvexHull 函式 # 將剛剛的資料 (lat lon) 放進該函式 hull = ConvexHull(latlon_cut.values) # 取得每群的 lat lon 邊界 Lat = latlon_cut.values[hull.vertices,0] Long = latlon_cut.values[hull.vertices,1] # 將 boundary 存成 dataframe ，並轉成 lat lon 的 list # 在 folium 中畫出邊界的 polygon cluster = pd.DataFrame({'lat':Lat,'lon':Long }) area = list(zip(cluster['lat'],cluster['lon'])) list_index = g-1 lay_cluster = layer_list[list_index ] folium.Polygon(locations=area, color=color_list[list_index], weight=2, fill=True, fill_opacity=0.1, opacity=0.8).add_to(lay_cluster) # 加入可以選擇 layer 的控制區 folium.LayerControl(collapsed=False,position= 'bottomright').add_to(m) # 存成 html 檔 print(m) # 地圖會存在與程式碼相同路徑的資料夾中 m.save('River_clustering.html') 密度 (Kernel density) 密度是我們常用以描述事件聚集強度的觀念，而傳統一般的密度都是以：個數/面積的公式去計算，但是，這樣的計算方法卻容易受到面積的影響，同樣的測站數/案例數在面積不同的鄉鎮，就會出現不同的密度，反而讓我們無法正確地探勘事件的聚集強度。\n因此，為了避免面積造成計算的差異，所以我們可以用核密度 (kernel density) 的方式去描述事件聚集的強度；核密度的概念就是：以樣本點為中心，並利用固定半徑的移動視窗 (moving windows) 去框選其他樣本點，最後以所有樣本點的數值加總來取代樣本點的舊有數值。這樣的方式可以標準化密度公式中的「面積」，並取得一個全面性的密度分佈圖，以協助我們瞭解整體事件的分佈強度 (圖1)。\n在這個案例中，我們也來看看是否能用核密度的方法去描述地下水位站的分佈強弱。\nbasemap = county.loc[county['COUNTYNAME'].isin([\"嘉義縣\",\"嘉義市\"])] basemap = basemap.set_crs(4326,allow_override=True) gdf = gpd.overlay(gdf_level, basemap, how='intersection') # selecting the polygon's geometry field to filter out points that # are not overlaid # 利用plotly套件 畫出河川水位核密度地圖，gdf為河川水位資料點位與數值 import plotly.express as px fig = px.density_mapbox(gdf, lat='lat', lon='lon', z='Observations', radius=25, center=dict(lat=23.5, lon=120.5), zoom=8, mapbox_style=\"stamen-terrain\") fig.show() 空間內插 (Spatial interpolation) 微型感測站的設置就像是我們在空間上進行數值採樣，透過這些採樣的結果，我們可以利用一些統計方法還原母體的全貌。因為我們無法在每一吋土地上佈滿測站，所以一定會面臨有些地區有資料，有些地區卻沒有資料的狀況，而空間內插 (Spatial interpolation) 就是以統計的方法，協助我們推估沒有資料的區域，進而了解母體全貌的方法。\n要利用空間內插的方法，首先，我們必須要先瞭解：確定性模型（Deterministic model）與機率性模型 (Stochastic model) 這兩個概念。所謂確定性模型就是我們再掌握某空間現象的分佈規則下，舊可以利用某個相關性參數，去推估未知區域的數值，例如台灣的門牌號碼是將單數與雙數非別排列，所以假若某間房子的前一個門牌為6號，而後一個門牌為10號，那我們可以推估中間這個房子的門牌為8號。而機率性則是假設真實環境非常複雜，然而我們只能透過機率以及變異數的變化去建立適當的推估模型，並接受其差值以及不確定性 (uncertainty)。\n所以，接下來我們就利用雲林的地下水位站來練習幾種常見的空間內插方法。\n反距離加權法 (Inverse Distance Weighting) 在反距離加權法的模式中，我們會利用已知樣本點間的數值差與間距建立推估模型。一般而言，若兩點的差值為10單位、而間距為100公尺，則理論上每10公尺的差值應該為1，但是考量到差值的分佈不應該是線性關係，所以反距離加權法利用地理學第一定律：越鄰近的事物越相近，去以距離作為評估兩點之間數值差異的依據，其策略就是把差值X距離次方的倒數，以獲得該位置的推估值。所以，距離越大權數越小，反之距離越近，權數愈大。\nimport numpy as np import matplotlib.pyplot as plt from scipy.interpolate import Rbf def distance_matrix(x0, y0, x1, y1): obs = np.vstack((x0, y0)).T interp = np.vstack((x1, y1)).T # 在指定範圍中建立 distance matrix # 參考資料: \u003chttp://stackoverflow.com/questions/1871536\u003e d0 = np.subtract.outer(obs[:,0], interp[:,0]) d1 = np.subtract.outer(obs[:,1], interp[:,1]) print(d0.dtype,d1.dtype) return np.hypot(d0, d1) def simple_idw(x, y, z, xi, yi, pows): dist = distance_matrix(x,y, xi,yi) # IDW 權重為 1 / distance weights = 1.0 / dist # 使權重加總為 1 weights /= weights.sum(axis=0) # 在每個要插值的點上，將權重乘以 Z 值 zi = np.dot(weights.T, z) return zi fig, ax = plt.subplots(figsize=(6, 4)) ax.set_aspect('equal') pows = 2 nx, ny = 100, 100 xmin, xmax = 119.8, 121.2 ymin, ymax = 23, 24 interpolatep = gdf[[\"lon\", 'lat', 'Observations']] x = interpolatep['lon'] y = interpolatep['lat'] z = interpolatep['Observations'] x = x.astype(\"float64\") y = y.astype(\"float64\") z = z.astype(\"float64\") xi = np.linspace(xmin,xmax, nx) yi = np.linspace(ymin, ymax, ny) xi, yi = np.meshgrid(xi, yi) xi, yi = xi.flatten(), yi.flatten() # 計算 IDW grid = simple_idw(x,y,z,xi,yi,pows) grid = grid.reshape((ny, nx)) grid = grid.astype(\"float64\") plt.imshow(grid, extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='lightgray'); ax.scatter(x, y, marker=\".\", color='orange', s=z,label=\"input point\") plt.colorbar() plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.title('IDW') plt.show() 克力金法 (Kriging) 克利金法的原理則是利用已知點的位置與數值，去建立一個半變異圖 (semi-variogram)，並依據這個圖進行樣本點的分組，以得到數個區域性變量 (regionalized variable)，並以其為基礎進行數值推估。克利金法與前述反距離加權法相似，都是利用已知點的數值與距離去推估鄰近地區的未知點數值，比較大的差異是克力金法會將樣本的依照距離作分組，從而依照距離調整其不同的推估公式。\n# 建立網格範圍與解析度 import numpy as np resolution = 0.1 # cell size in meters gridx = np.arange(119.8, 121.2, resolution) gridy = np.arange(23, 24, resolution) # 定義函式 raster to polygon import itertools from shapely.geometry import Polygon def pixel2poly(x, y, z, resolution): \"\"\" x: x coords of cell y: y coords of cell z: matrix of values for each (x,y) resolution: spatial resolution of each cell \"\"\" polygons = [] values = [] half_res = resolution / 2 for i, j in itertools.product(range(len(x)), range(len(y))): minx, maxx = x[i] - half_res, x[i] + half_res miny, maxy = y[j] - half_res, y[j] + half_res polygons.append(Polygon([(minx, miny), (minx, maxy), (maxx, maxy), (maxx, miny)])) if isinstance(z, (int, float)): values.append(z) else: values.append(z[j, i]) return polygons, values # 利用pykrige 內插計算網格數值 from pykrige.ok import OrdinaryKriging krig = OrdinaryKriging(x=gdf[\"lon\"], y=gdf[\"lat\"], z=gdf['Observations'], variogram_model=\"spherical\", pseudo_inv=True) z, ss = krig.execute(\"grid\", gridx, gridy) plt.imshow(z); # 利用plotly 將網格與地圖疊合呈現 import plotly.express as px polygons, values = pixel2poly(gridx, gridy, z, resolution) water_model = (gpd.GeoDataFrame({\"water_modelled\": values}, geometry=polygons, crs=\"EPSG:4326\") .to_crs(\"EPSG:4326\") ) fig = px.choropleth_mapbox(water_model, geojson=water_model.geometry, locations=water_model.index, color=\"water_modelled\", color_continuous_scale=\"RdYlGn_r\", opacity=0.5, center={\"lat\": 24, \"lon\": 121}, zoom=6, mapbox_style=\"carto-positron\") fig.update_layout(margin=dict(l=0, r=0, t=30, b=10)) fig.update_traces(marker_line_width=0) 最近鄰居插值 (Nearest neighbor Interpolation) 最近鄰居法的方法其實很簡單，若我們想得知空間上某一個位置的數值，只需要找到最鄰近且有數值的測站，就可以當作是這個位置的數值。這個方法基本上也是依循越鄰近越相似的原理去設計，且常被應用在影像處理及放大的案例上。\nfrom scipy.interpolate import NearestNDInterpolator import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(6, 4)) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") X = np.linspace(min(xd), max(xd)) Y = np.linspace(min(yd), max(yd)) X, Y = np.meshgrid(X, Y) # 2D grid for interpolation interp = NearestNDInterpolator(list(zip(xd, yd)), zd) Z = interp(X, Y) im = ax.pcolormesh(X, Y, Z, shading='auto') basemap.plot(ax=ax, facecolor='none', edgecolor='gray'); sns.scatterplot(x='lon', y='lat', data=interpolatep,label=\"input point\") plt.legend() plt.colorbar(im) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.show() 空間內插後的資料處理 擷取等值線資料 (contour) 一般來說，我們將測站依照其位置與數值進行空間內插後，就會得到一個全面性的網格資料，然而，我們可以如何解析這些網格資料呢？首先，最容易的方式就是依照網格的數值與位置，去將數值相近的點連成一條線，其概念類似於在起伏不定的地形上，劃設等高線，而我們這樣的話法可以將其視為等值線。\nfrom osgeo import gdal import numpy as np import matplotlib import matplotlib.pyplot as plt import elevation # 利用 Matplotlib 中的 'contourf' 函式來畫 fig, ax = plt.subplots(figsize=(6, 10)) X = np.linspace(xmin, xmax) Y = np.linspace(ymin, ymax) krig = OrdinaryKriging(x=interpolatep['lon'], y=interpolatep['lat'], z=interpolatep['Observations'], variogram_model=\"spherical\") z, ss = krig.execute(\"grid\", X, Y) im = ax.contourf(z, cmap = \"viridis\", levels = list(range(-30, 30, 10)),extent=(xmin, xmax, ymin, ymax)) basemap.plot(ax=ax, facecolor='none', edgecolor='black'); plt.title(\"Elevation Contours Taiwan\") plt.show() 擷取橫切面的資料 (Profile) 劃設等值線可以讓我們獲知數值的分佈梯度與範圍，而另一個協助我們瞭解數值分佈的方式則是剖面線，其原理就是在兩點之間劃設一條直線，並依照直線的位置去擷取相對應的推估數值。這樣的方法可以協助我們知道兩點之間的數值變化起伏，在某些空氣品質的研究中，科學家就會利用剖面線的方式評估道路兩側的PM2.5變化。\ndef export_kde_raster(Z, XX, YY, min_x, max_x, min_y, max_y, proj, filename): '''Export and save a kernel density raster.''' # 取得解析度 xres = (max_x - min_x) / len(XX) yres = (max_y - min_y) / len(YY) # 取得 bound 等資訊 transform = Affine.translation(min_x - xres / 2, min_y - yres / 2) * Affine.scale(xres, yres) # 輸出為 raster with rasterio.open( filename, mode = \"w\", driver = \"GTiff\", height = Z.shape[0], width = Z.shape[1], count = 1, dtype = Z.dtype, crs = proj, transform = transform, ) as new_dataset: new_dataset.write(Z, 1) from pykrige.ok import OrdinaryKriging from affine import Affine import rasterio import math start_cor = [119.9,23.2] end_cor = [120.1,23.9] npoints=100 X = np.linspace(xmin, xmax, npoints) Y = np.linspace(ymin, ymax, npoints) interpolatep = gdf[[\"lon\", 'lat', 'Observations']] xd = interpolatep['lon'] yd = interpolatep['lat'] zd = interpolatep['Observations'] xd = xd.astype(\"float64\") yd = yd.astype(\"float64\") zd = zd.astype(\"float64\") krig = OrdinaryKriging(x=xd, y=yd, z=zd, variogram_model=\"spherical\") zr, ss = krig.execute(\"grid\", X, Y) # 輸出 raster export_kde_raster(Z = zr, XX = X, YY = Y, min_x = xmin, max_x = xmax, min_y = ymin, max_y = ymax, proj = 4326, filename = \"kriging_result.tif\") kriging = rasterio.open(\"kriging_result.tif\",mode='r') dist = math.sqrt((end_cor[0]-start_cor[0])**2+(end_cor[1]-start_cor[1])**2)*111 npoints=500 lat = np.linspace(start_cor[1], end_cor[1],npoints) lon = np.linspace(start_cor[0], end_cor[0],npoints) distarray = np.linspace(0, dist,npoints) np.append(distarray, dist) df = pd.DataFrame({'Latitude': lat, 'Longtitude':lon,'h_distance':distarray}) df['Observations']=0 gdf_pcs = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.Longtitude, df.Latitude)) gdf_pcs.crs = {'init':'epsg:4326'} for index, row in gdf_pcs.iterrows(): rows, cols = kriging.index(row['geometry'].x,row['geometry'].y) kri_data = kriging.read(1) df['Observations'].loc[index] = kri_data[rows, cols] profile = df[['h_distance','Observations']] profile.plot(x='h_distance',y='Observations') kriging.close() 參考資源 Geopandas初探, Chimin. https://ithelp.ithome.com.tw/articles/10202336 scipy.spatial 空間處理模組說明 (https://docs.scipy.org/doc/scipy/reference/spatial.html) scipy.interpolate 空間內插模組說明 (https://docs.scipy.org/doc/scipy/reference/interpolate.html) pykrige 克力金內插模組說明 (https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/api.html#krigging-algorithms) ",
    "description": "我們使用民生公共物聯網資料平台的感測資料，介紹較為進階的地理空間分析，利用測站資訊中的 GPS 位置座標，首先利用尋找最大凸多邊形 (Convex Hull) 的套件，框定感測器所涵蓋的地理區域；接著套用 Voronoi Diagram 的套件，將地圖上的區域依照感測器的分布狀況，切割出每個感測器的勢力範圍。針對感測器與感測器之間的區域，我們利用空間內插的方式，套用不同的空間內插演算法，根據感測器的數值，進行空間地圖上的填值，並產製相對應的圖片輸出。",
    "tags": [
      "Python",
      "水",
      "空"
    ],
    "title": "5.2. 地理空間分析",
    "uri": "/ch5/ch5.2/"
  },
  {
    "content": "\nTable Of Contents 異常檢測框架 異常事件種類 異常事件可能原因 實際案例演練 套件安裝與引用 讀取資料與環境設定 尋找鄰近的感測器 每五分鐘進行感測資料的時間切片 以時間切片為單位計算鄰居感測器的平均感測值 判斷異常事件的門檻值 非正常使用機器偵測模組 (MD) 實作 即時污染偵測模組 (RED) 實作 感測器可靠度評估模組 (DR) 實作 參考資料 異常檢測框架 目前已有多個大規模微型空品監測系統成功部署於不同的國家與城市之中，然而這些微型感測器的主要挑戰之一為如何確保數據品質，並且能即時偵測出可能的異常現象。在台灣的中央研究院資訊科學研究所網路實驗室研究團隊，於2018年提出了一種可用於實際環境中的異常檢測框架，稱之為 Anomaly Detection Framework (ADF)。\n此異常檢測框架由四個模組所組成：\n時間片斷異常偵測 (Time-Sliced Anomaly Detection, TSAD)：可即時偵測感測器於空間或時間上的異常數據，並將結果輸出給其他模組進行進一步分析。 即時污染偵測模組 (Real-time Emission Detection, RED)：可透過 TSAD 的偵測結果，即時檢測潛在的區域性污染事件。 感測器可靠度評估模組 (Device Ranking, DR)：可累積 TSAD 的偵測結果，並據以評估每個微型感測器設備的可靠度 非正常使用機器偵測模組 (Malfunction Detection, MD)：可累積 TSAD 的偵測結果，透過數據分析判別可能為非正常使用的微型感測器，例如安裝在室內的機器、安置在持續性污染源旁邊的機器等。 異常事件種類 在 ADF 框架中，TSAD 模組在微型感測器每次收到新的感測資料後，便會進行時間類或空間類的異常事件判斷，我們已微型空品感測器為例，進行說明：\n時間類異常事件：我們假設空氣的擴散是均勻緩慢的，因此同一台微型空品感測器在短時間內的數值變化應極為平緩，如果有某台微型空品感測器的感測數值在短時間內出現劇烈的變化，代表在時間維度上可能出現異常事件。 空間類異常事件：我們可以假設戶外的空氣在地理空間上是會均勻擴散的，因此微型空品感測器的感測數值，理應與周圍鄰近的感測器相似，如果有某台微型空品感測器的感測數值，與同時間鄰近區域的微型空品感測器的感測數值出現極大的差異，代表該感測器所處的空間可能出現異常事件。 異常事件可能原因 以上所述的異常事件有許多可能的原因，常見的原因有：\n安裝環境異常：感測器被安裝於特定環境，因此無法呈現整體環境現象，例如安裝於廟宇旁、燒烤店內或其他室內不通風的地點。 機器故障或安裝錯誤：例如感測器安裝時將取風口的方向弄錯，或者感測器的風扇積垢導致運轉不順暢。 出現臨時污染源：例如感測器旁邊剛好有人在抽菸、發生火災或排放污染物質。 實際案例演練 在這篇文章中，我們將以民生公共物聯網中的空品資料為例，使用部分佈建於高雄市的校園微型空品感測器來進行分析，並且介紹如何使用 ADF 檢測框架來找出其中可能為室內機器或位於污染源附近的機器，藉此過濾出可信度相對較低的機器們，進而提高整體空品感測結果的可信度。\n套件安裝與引用 在這個案例中，我們將會使用到 pandas, numpy, plotly 和 geopy 等套件，這些套件在我們使用的開發平台 Google Colab 上已有預先提供，因此我們不需要另行安裝，可以直接用下列的方法引用，以備之後資料處理與分析使用。\nimport pandas as pd import numpy as np import plotly.express as px from geopy.distance import geodesic 讀取資料與環境設定 在這個案例中，我們將使用部分民生公共物聯網佈建於高雄的校園微型空品感測器來進行分析，我們所設定的時間和空間範圍如下：\n地理區域：緯度: 22.631231 - 22.584989, 經度: 120.263422 - 120.346764 時間區間：2022.10.15 - 2022.10.28 註：校園微型空品感測器的原始資料請至 民生公共物聯網-資料服務平台下載，為了方便讀者可以重現這個範例的內容和結果，我們先把所有使用到的資料整理成 allLoc.csv 檔案，做為接下來資料分析的依據。\n我們首先載入資料檔案，並預覽資料的內容：\nDF = pd.read_csv(\"https://LearnCIOT.github.io/data/allLoc.csv\") DF.head() 接著我們擷取資料檔中每個感測器的 GPS 地理位置座標，由於這些感測器的 GPS 座標都不會改變，我們使用比較特別的方式，將資料檔中每個感測器的經度和緯度資料各自取平均值，做為該感測器的地理位置座標。\ndfId = DF[[\"device_id\",\"lon\",\"lat\"]].groupby(\"device_id\").mean().reset_index() print(dfId.head()) device_id lon lat 0 74DA38F207DE 120.340 22.603 1 74DA38F20A10 120.311 22.631 2 74DA38F20B20 120.304 22.626 3 74DA38F20B80 120.350 22.599 4 74DA38F20BB6 120.324 22.600 為了大致了解資料檔內感測器的地理位置分布，我們將感測器的位置繪製在地圖上。\nfig_map = px.scatter_mapbox(dfId, lat=\"lat\", lon=\"lon\", color_continuous_scale=px.colors.cyclical.IceFire, zoom=9, mapbox_style=\"carto-positron\") fig_map.show() 尋找鄰近的感測器 由於我們所使用的校園微型空氣品質感測器都是固定安裝在校園內，其 GPS 地理位置座標並不會改變，為了節省之後資料分析的運算時間，我們先統一將每個感測器的「鄰居」清單計算出來。在我們的案例中，我們定義如果兩個微型感測器的相互距離小於等於 3 公里，那麼這兩個感測器變互為鄰居關係。\n我們首先撰寫一個小函式 countDis，可以針對輸入的兩個 GPS 地理座標位置，計算兩者之間的實體公里距離。\ndef countDis(deviceA, deviceB): return geodesic((deviceA[\"lat\"], deviceB['lon']), (deviceB[\"lat\"], deviceB['lon'])).km 接著我們將原有資料的感測器列表從 DataFrame 資料型態，轉化成 Dictionary 資料型態，並且計算所有任意兩個感測器間的距離，只要兩者間的距離小於 3km，便將彼此存入對方的鄰居感測器列表 dicNeighbor 中。\n# set the maximum distance of two neighbors DISTENCE = 3 ## convert Dataframe -\u003e Dictionary # {iD: {'lon': 0.0, 'lat': 0.0}, ...} dictId = dfId.set_index(\"device_id\").to_dict(\"index\") ## obtain the list of sensor device_id listId = dfId[\"device_id\"].to_list() ## initialize dicNeighbor # {iD: []} dicNeighbor = {iD: [] for iD in listId} ## use countDis to calculate distance of every two sensors # The two sensors are deem to be neighbors of each other if their distance is less than 3km # Time complexity: N! for x in range(len(listId)): for y in range(x+1, len(listId)): if ( countDis( dictId[listId[x]], dictId[listId[y]]) \u003c DISTENCE ): dicNeighbor[listId[x]].append( listId[y] ) dicNeighbor[listId[y]].append( listId[x] ) 每五分鐘進行感測資料的時間切片 由於原始資料中每個感測器的感測資料在時間上並未同步，在 ADF 框架中提出將感測資料以每單位時間為間隔，獲取整體感測結果的時間切片 (time slice)。我們首先將原始資料中的 date 和 time 兩個欄位進行整併，並以 Python 語言中的 datetime 時間資料型態，儲存形成 datetime 新欄位，接著再將原有的 date、time、RH、temperature、lat、lon 等不需要的欄位刪除。\n# combine the 'date' and 'time' columns to a new column 'datetime' DF[\"datetime\"] = DF[\"date\"] + \" \" + DF[\"time\"] # remove some non-necessary columns DF.drop(columns=[\"date\",\"time\", \"RH\",\"temperature\",\"lat\",\"lon\"], inplace=True) # convert the new 'datetime' column to datetime data type DF['datetime'] = pd.to_datetime(DF.datetime) 由於我們所處理的校園微型空氣品質感測器的資料頻率約為 5 分鐘，因此我們將時間切片的單位時間 FREQ 也設為 5 分鐘，並計算每五分鐘內，每個感測器所回傳的感測值的平均值。為了確保資料正確性，我們也多做了一道檢查，將 PM2.5 感測值為負數的資料予以刪除。\nFREQ = '5min' dfMean = DF.groupby(['device_id', pd.Grouper(key = 'datetime', freq = FREQ)]).agg('mean') # remove invalid records (i.e., PM2.5 \u003c 0) dfMean = dfMean[ dfMean['PM2.5'] \u003e= 0 ] print(dfMean.head()) PM2.5 device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 2022-10-15 00:05:00 37.0 2022-10-15 00:10:00 37.0 2022-10-15 00:20:00 36.0 2022-10-15 00:25:00 36.0 以時間切片為單位計算鄰居感測器的平均感測值 為了計算在特定時間切片上，特定感測器的鄰居感測器的平均感測值，我們撰寫 cal_mean 函式，可以根據輸入的特定感測器代碼 iD 與時間戳記 dt，回傳該感測器的鄰居感測器數量與平均感測值。\ndef cal_mean(iD, dt): neighborPM25 = dfMean[ (dfMean.index.get_level_values('datetime') == dt) \u0026 (dfMean.index.get_level_values(0).isin(dicNeighbor[iD])) ][\"PM2.5\"] avg = neighborPM25.mean() neighbor_num = neighborPM25.count() return avg, neighbor_num 接著我們針對 dfMean 中每台感測器的每個時間戳記，計算其鄰居感測器的數量與平均感測值，並分別存入 avg 與 neighbor_num 兩個新欄位中。比較特別的是，這邊我們使用 zip 和 apply 的語法，可以將 DataFrame 的數值帶入函式當中進行運算，其中：\n我們使用 zip 的語法，用於打包 apply_func 所回傳的兩個參數值。 我們使用 apply 的語法，以配合 DataFrame 的規則，接收 apply_func 的回傳值。 def apply_func(x): return cal_mean(x.name[0], x.name[1]) dfMean['avg'], dfMean['neighbor_num'] = zip(*dfMean.apply(apply_func, axis=1)) print(dfMean.head()) PM2.5 avg neighbor_num device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 2022-10-15 00:05:00 37.0 19.888889 9 2022-10-15 00:10:00 37.0 16.500000 12 2022-10-15 00:20:00 36.0 16.750000 8 2022-10-15 00:25:00 36.0 17.000000 11 判斷異常事件的門檻值 我們透過觀察發現，所謂的「異常事件」指的是感測器的感測值，與我們心中認定的合理值有過大的差距，這個合理值有可能是鄰近的感測器感測值（空間類異常），也有可能是同一個感測器的前一筆感測值（時間類異常），或者是根據其他資訊來源的合理推估值。同時，我們也發現所謂的「過大的差距」，其實是一個很模糊的說法，其具體數值隨感測值的大小，亦有極大的不同。\n因此，我們首先根據現有 dfMean[’avg'] 的分布狀況，切分為 9 個區間，並且針對每一個區間的 PM2.5 感測值，計算其標準差，並據以做為判斷感測值是否異常的門檻值標準，如下表所示。舉例來說，當原始數值為 10 (ug/m3) 時，倘若周圍感測器平均高於 10+6.6 ，或低於 10-6.6 ，我們便會認定該感測器的該筆感測值為異常事件。\n原始數值 (ug/m3) 門檻值標準 0-11 6.6 12-23 6.6 24-35 9.35 36-41 13.5 42-47 17.0 48-58 23.0 59-64 27.5 65-70 33.5 71+ 91.5 根據這個對照表，我們撰寫下列的函示 THRESHOLD，根據輸入的感測數值，回傳對應的門檻值，並將此門檻值存入 dfMEAN 的新欄位 PM_thr 中。\ndef THRESHOLD(value): if value\u003c12: return 6.6 elif value\u003c24: return 6.6 elif value\u003c36: return 9.35 elif value\u003c42: return 13.5 elif value\u003c48: return 17.0 elif value\u003c54: return 23.0 elif value\u003c59: return 27.5 elif value\u003c65: return 33.5 elif value\u003c71: return 40.5 else: return 91.5 dfMean['PM_thr'] = dfMean['PM2.5'].apply(THRESHOLD) 由於原始資料的最後一筆紀錄的時間是 2022-10-28 23:45，因此我們將資料判斷的時間設為 2022-10-29 日，並將原始資料中每一筆紀錄與資料判斷時間的差距，存入 dfMEAN 的新欄位 days 中。接著我們先預覽一下目前 dfMEAN 資料表的狀況。\nTARGET_DATE = \"2022-10-29\" dfMean = dfMean.assign(days = lambda x: ( (pd.to_datetime(TARGET_DATE + \" 23:59:59\") - x.index.get_level_values('datetime')).days ) ) print(dfMean.head()) PM2.5 avg neighbor_num PM_thr days device_id datetime 74DA38F207DE 2022-10-15 00:00:00 38.0 13.400000 10 13.5 14 2022-10-15 00:05:00 37.0 19.888889 9 13.5 14 2022-10-15 00:10:00 37.0 16.500000 12 13.5 14 2022-10-15 00:20:00 36.0 16.750000 8 13.5 14 2022-10-15 00:25:00 36.0 17.000000 11 13.5 14 非正常使用機器偵測模組 (MD) 實作 在接下來的範例中，我們實作 ADF 中的非正常使用機器偵測模組 (Malfunction Detection, MD)，其核心觀念為，倘若我們將某一個微型感測器的 PM2.5 數值與其周圍 3 公里內的其他感測器進行比較，如果其感測數值低於鄰近感測器的平均值 (avg) 減掉可接受的門檻值 (PM_thr)，則將該機器 ˋ視為裝設在室內的機器（標記為indoor）；如果其感測數值高於鄰近感測器的平均值 (avg) 加上可接受的門檻值 (PM_thr)，則將該機器視為裝設在污染源旁的機器（標記為emission）。為了避免因為參考的鄰近感測器數量不夠導致誤判，我們只採計鄰近區域存有多於 2 個其他感測器的案例。\nMINIMUM_NEIGHBORS = 2 dfMean[\"indoor\"] = ((dfMean['avg'] - dfMean['PM2.5']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean[\"emission\"] = ((dfMean['PM2.5'] - dfMean['avg']) \u003e dfMean['PM_thr']) \u0026 (dfMean['neighbor_num'] \u003e= MINIMUM_NEIGHBORS) dfMean 由於每日的空氣品質狀況不同，對於 indoor 和 emission 的判斷結果也會產生不同程度的影響，為了獲得更有說服力的判斷結果，我們考慮過去 1 天、7 天、 14 天等三個時間長度，分別計算在不同時間長度下，同一個感測器被判斷為 indoor 或 emission 的比例。同時，為了避免外在環境導致的誤判影響最後的判斷結果，我們強制修改計算出來的結果，將比例小於 1/3 的數值，強制改為 0。\n# initialize dictIndoor = {iD: [] for iD in listId} dictEmission = {iD: [] for iD in listId} for iD in listId: dfId = dfMean.loc[iD] for day in [1, 7, 14]: indoor = (dfId[ dfId['days'] \u003c= day]['indoor'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictIndoor[iD].append( indoor if indoor \u003e 0.333 else 0 ) emission = (dfId[ dfId['days'] \u003c= day]['emission'].sum() / len(dfId[ dfId['days'] \u003c= day])).round(3) dictEmission[iD].append( emission if emission \u003e 0.333 else 0 ) 我們接著分別列印出 dictIndoor 和 dictEmission 的內容，觀察判斷出來的結果。\ndictIndoor {'74DA38F207DE': [0, 0, 0], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0.995, 0.86, 0.816], '74DA38F20B80': [0.995, 0.872, 0.867], '74DA38F20BB6': [0, 0, 0], '74DA38F20C16': [0.989, 0.535, 0], '74DA38F20D7C': [0, 0, 0], '74DA38F20D8A': [0, 0, 0], '74DA38F20DCE': [0, 0, 0], '74DA38F20DD0': [0.984, 0.871, 0.865], '74DA38F20DD8': [0, 0.368, 0.369], '74DA38F20DDC': [0, 0, 0], '74DA38F20DE0': [0, 0, 0], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0, 0, 0], '74DA38F20E42': [0.99, 0.866, 0.87], '74DA38F20E44': [0, 0, 0], '74DA38F20F0C': [0.979, 0.872, 0.876], '74DA38F20F2C': [0, 0, 0], '74DA38F210FE': [0.99, 0.847, 0.864]} dictEmission {'74DA38F207DE': [0.92, 0.737, 0.735], '74DA38F20A10': [0, 0, 0], '74DA38F20B20': [0, 0, 0], '74DA38F20B80': [0, 0, 0], '74DA38F20BB6': [0.492, 0.339, 0], '74DA38F20C16': [0, 0, 0], '74DA38F20D7C': [0.553, 0.342, 0.337], '74DA38F20D8A': [0.672, 0.457, 0.388], '74DA38F20DCE': [0.786, 0.556, 0.516], '74DA38F20DD0': [0, 0, 0], '74DA38F20DD8': [0, 0, 0], '74DA38F20DDC': [0.345, 0, 0], '74DA38F20DE0': [0.601, 0.503, 0.492], '74DA38F20DE2': [0, 0, 0], '74DA38F20E0E': [0.938, 0.75, 0.75], '74DA38F20E42': [0, 0, 0], '74DA38F20E44': [0.938, 0.69, 0.6], '74DA38F20F0C': [0, 0, 0], '74DA38F20F2C': [0.744, 0.575, 0.544], '74DA38F210FE': [0, 0, 0]} 由上述的結果可以發現，對於 indoor 和 emission 的判斷結果，隨著參考的過往時間長短，存在不小的差異，由於不同過往時間的長度代表不同的參考意義，因此我們使用權重的方式，給予 1 天參考時間 A 的權重，給予 7 天參考時間 B 的權重，最後給予 14 天參考時間 1-A-B 的權重。\n同時，我們考量在一天參考時間內的工作時間約為 24 小時中的 8 小時，七天參考時間內的工作時間約為 168 小時中的 40 小時，14 天參考時間內的工作時間約為 336 小時中的 80 小時，因此若一個感測器被判斷為 indoor 或 emission 類型，其在 MD 中所佔的加權比重應大於等於 MD_thresh，且\nMD_thresh = (8.0/24.0)*A+(40.0/168.0)B+(80.0/336.0)(1-A-B)\n在我們的範例中，我們假定 A=0.2，B=0.3，因此我們可以透過下列的程式獲得加權判斷後的 indoor 和 emission 類別感測器清單。\nA=0.2 B=0.3 MD_thresh=(8.0/24.0)*A+(40.0/168.0)*B+(80.0/336.0)*(1-A-B) listIndoorDevice = [] listEmissionDevice = [] for iD in listId: rate1 = A*dictIndoor[iD][0] + B*dictIndoor[iD][1] + (1-A-B)*dictIndoor[iD][2] if rate1 \u003e MD_thresh: listIndoorDevice.append( (iD, rate1) ) rate2 = A*dictEmission[iD][0] + B*dictEmission[iD][1] + (1-A-B)*dictEmission[iD][2] if rate2 \u003e MD_thresh: listEmissionDevice.append( (iD, rate2) ) 我們接著分別列印出 listIndoorDevice 和 listEmissionDevice 的內容，觀察透過加權判斷出來的結果。\nlistIndoorDevice [('74DA38F20B20', 0.865), ('74DA38F20B80', 0.8941), ('74DA38F20C16', 0.3583), ('74DA38F20DD0', 0.8906), ('74DA38F20DD8', 0.2949), ('74DA38F20E42', 0.8928), ('74DA38F20F0C', 0.8954), ('74DA38F210FE', 0.8841)] listEmissionDevice [('74DA38F207DE', 0.7726), ('74DA38F20D7C', 0.38170000000000004), ('74DA38F20D8A', 0.4655), ('74DA38F20DCE', 0.5820000000000001), ('74DA38F20DE0', 0.5171), ('74DA38F20E0E', 0.7876), ('74DA38F20E44', 0.6945999999999999), ('74DA38F20F2C', 0.5933)] 即時污染偵測模組 (RED) 實作 針對即時污染偵測 (Real-time Emission Detection, RED)，我們假設某個微型感測器在連續的取樣中，如果最新的感測數值比前一次的感測數值多出 1/5 以上，代表其周遭環境出現劇烈的變化，極有可能是因為周遭出現空汙的排放現象。由於當感測數值較小時，1/5 的變化量其實在 PM2.5 的真實濃度變化上仍十分輕微，因此我們也排除感測數值小於 20 的狀況，以避免因為感測器本身的誤差導致即時污染偵測的誤判。\ndfMean['red'] = False for iD in listId: dfId = dfMean.loc[iD] p_index = '' p_row = [] for index, row in dfId.iterrows(): red = False if p_index: diff = row['PM2.5'] - p_row['PM2.5'] if p_row['PM2.5']\u003e20 and diff\u003ep_row['PM2.5']/5: red = True dfMean.loc[pd.IndexSlice[iD, index.strftime('%Y-%m-%d %H:%M:%S')], pd.IndexSlice['red']] = red p_index = index p_row = row dfMean 感測器可靠度評估模組 (DR) 實作 最後，我們總結 RED 與 MD 兩個模組的判斷結果，進行微型感測器的可靠度評估 (Device Ranking, DR)，其主要概念為：\n如果感測器的感測資料常常被判定為時間類異常或空間類異常，代表該感測器的硬體，或者其所屬的環境可能有潛在的問題，需要進一步釐清。 如果感測器的感測資料較少被判定為異常，代表該感測器的感測數值與周遭感測器的數值具備很高的一致性，因此可信度高。 我們以日為單位，根據每個感測器每日的所有感測資料，統計曾經被判斷為時間類異常 (red=True) 或空間類異常 (indoor=True 或 emission=True) 的總次數，並計算其占一日所有資料筆數中的比例，以此做為感測器的資訊是否可靠的依據。\ndevice_rank = pd.DataFrame() for iD in listId: dfId = dfMean.loc[iD] abnormal = {} num = {} for index, row in dfId.iterrows(): d = index.strftime('%Y-%m-%d') if d not in abnormal: abnormal[d] = 0 if d not in num: num[d] = 0 num[d] = num[d] + 1 if row['indoor'] or row['emission'] or row['red']: abnormal[d] = abnormal[d] + 1 for d in num: device_rank = device_rank.append(pd.DataFrame({'device_id': [iD], 'date': [d], 'rank': [1 - abnormal[d]/num[d]]})) device_rank.set_index(['device_id','date']) 參考資料 Ling-Jyh Chen, Yao-Hua Ho, Hsin-Hung Hsieh, Shih-Ting Huang, Hu-Cheng Lee, and Sachit Mahajan. ADF: an Anomaly Detection Framework for Large-scale PM2.5 Sensing Systems. IEEE Internet of Things Journal, volume 5, issue 2, pp. 559-570, April, 2018. (https://dx.doi.org/10.1109/JIOT.2017.2766085) ",
    "description": "我們使用空品類別資料，示範台灣微型空品感測資料上常用的感測器異常偵測演算法，以做中學的方式，一步步從資料準備，特徵擷取，到資料分析、統計與歸納，重現異常偵測演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析方法，逐步達成進階且實用的資料應用服務。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "6.2. 異常資料偵測",
    "uri": "/ch6/ch6.2/"
  },
  {
    "content": " Table Of Contents 章節目標 資料來源 Tableau 基本操作說明 資料導入 工作表 (Worksheet) 簡介 Tableau 應用範例ㄧ：空品資料時空分佈圖 繪製空間分布圖 (地理資訊地圖) 繪製時間分布圖(折線圖) Tableau 應用範例二：災情通報資料儀表板 (Dashboard) 災情資料格式轉換 儀錶板大小設定 將工作表加入儀錶板 加入互動式按鈕 讓同一個互動按鈕作用於多個工作表 加入其他資訊 文本 (Story) 總結 參考資料 在資料分析中，將資料視覺化往往可以協助我們對資料有更進一步的理解。而在將資料圖像化時，不同的資料類型在進行資料圖像化時所使用的圖表類型也有所不同，例如在呈現有經緯度座標的資料時，多半會使用地圖類的圖表，而在呈現時間序列資料時，則可以使用折線圖、直方圖等。然而，許多的資料常常具備不只一種特性，如民生公共物聯網的感測器資料就同時具備了經緯座標及時間序列，因此在呈現時，往往需要用到不只一種圖表才能呈現資料的意涵。而本章節要來介紹一款十分方便的軟體 Tableau，來幫助我們呈現上述複雜的資料型態。\nTableau 是一個提供使用者容易上手的視覺化分析平台，除了可以用其來快速製作各式圖表外，其 Dashboard 更是可以讓使用者更加方便地呈現不同的圖表資料；此外，Dashboard 還可以讓讀者與圖表進行互動，除了可以幫助讀者更加輕鬆快速地理解資料外，同時也讓資料分析變得更加地容易。Tableau 原本是一款付費的商業軟體，不過它也有提供免費版的 Tableau Public，來供大家使用。然而要注意的是，因為是 “Public” 所以利用它所製作出來的成果均會被公開。\n註釋 本文所操作之 Tableau Public 版本為 Tableau Desktop Public Edition (2022.2.2 (20222.22.0916.1526) 64 位)，惟本文所使用之功能皆為該軟體之基本功能，若使用其他版本軟體，應仍可正常操作。\n章節目標 知道如何將資料導入 Tableau 進行圖像化呈現 利用工作表 (Worksheet) 來繪製出分析圖表 利用儀錶板 (Dashboard) 及文本 (Story) 來設計出互動式圖表/簡報 資料來源 民生公共物聯網歷史資料 - 空品資料 - 中研院_校園空品微型感測 (https://history.colife.org.tw/#/?cd=%2F空氣品質%2F中研院_校園空品微型感測器) 民生公共物聯網歷史資料 - 災害示警與災情通報 - 消防署_災情通報 (https://history.colife.org.tw/#/?cd=%2F災害示警與災情通報%2F消防署_災情通報) Tableau 基本操作說明 資料導入 Tableau 可以讀取文字檔 (csv) 及空間資訊檔 (shp、geojson)，要匯入資料時點選 資料 \u003e 新增資料來源，來選擇要匯入的資料檔案。\n根據輸入的檔案類型不同，下面提供不同的操作範例：\n空間檔案 (shp、geojson) 匯入範例\n首先點擊左下角的資料來源，可以看到現在匯入的資料欄位等。 ![Tableau screenshot](figures/7-2-3-2.png) 接著點擊欄位名稱的上方符號可以更改該欄位資料的屬性。\n最後將經緯度座標賦予地理角色，以便後續的繪圖工作\n文字檔案 (CSV) 匯入範例\n將 PM 2.5 的紀錄值及測站位置匯入 Tableau 後，先建立起資料表之間的關係：\n接下來建立 PM 2.5 測站與測站座標之間的連結，點擊兩下測站資料進入聯結畫布，將另一個資料表拖到聯結畫布上。\n最後點選中間的連結圖示，設定連結形式以及運算邏輯。\n當設定完成後，可以看到右下角的 Station id 是從 Location 的資料表來的，這樣一來就把 PM 2.5資料與測站位置資料連結在一起了。\n工作表 (Worksheet) 簡介 工作表是 Tableau 可以用來將資料視覺化的地方，除了可以將資料繪製成圓餅圖、長條圖、折線圖等傳統圖形外，也可以用來繪製地理資訊的地圖。接下來將來展示如何繪製不同的圖表。\n在處理完資料後，點選左下的新增工作表來進行繪圖，進入工作表後可以看到下圖介面。\n在工作表的右上方有一個顯示的按鈕，點擊下去後便會看到不同類型的圖表 (系統會自動判斷可以繪製的圖表，無法繪製的會以反白的方式來呈現) ，接著可以點擊想要的圖表來更改圖表類型。\nTableau 應用範例ㄧ：空品資料時空分佈圖 繪製空間分布圖 (地理資訊地圖) 在開始繪製之前，我們需要先將經緯度從度量改成維度，有關度量及維度的詳細解說可見參考資料，詳細操作過程如以下動畫：\n接著我們將經緯度拉到欄及列的位置，並將PM 2.5值拉到標記位置並將其標記改為色彩，即可產生全台PM 2.5的分布圖，如以下動畫。\n我們可以點選色彩來改變圓點的顏色，而在上方的篩選條件則可以篩選特定條件的PM 2.5值，並且點選 PM 2.5 \u003e度量，則可以選擇 PM 2.5 的平均值、總和、中位數等。下面將示範如何顯示新北市的 PM 2.5值。\n將Site Name拉到篩選條件 點選萬用字元，選擇包含後輸入新北市 點選確定 繪製時間分布圖(折線圖) 我們將 Timestamp 拉到欄，並將 Site Name 及 PM 2.5 值拉到列。接著點選 Timestamp 便可以設定時間的間隔，例如下圖中我們將時間間隔設定為 1 小時。注意：跟繪製空間分布圖時一樣，這裡也可以用篩選功能對要展示的測站進行篩選。而下方的動畫中，我們示範了如何顯示位於高雄的PM2.5一整天的監測數值。\nTableau 應用範例二：災情通報資料儀表板 (Dashboard) 儀錶板可以將不同的工作表 (圖表) 組合在一起，以此來呈現更加豐富的資訊，並讓使用者可以動態的檢視資料，在接下來的範例中，我們將介紹如何建立簡單的儀表板。由於在前面的文章中我們已經用過空品資料做為範例，因此我們在這邊將改使用暴雨災情資料來作為後續示範用途。\n災情資料格式轉換 我們使用 2018/08/23 所發生的 823 水災為例，由於原始檔案格式為 xml 檔，我們首先使用下方網址將原始檔案轉為csv檔：https://www.convertcsv.com/xml-to-csv.htm (如下動畫)\n儀錶板大小設定 在 Tableau 選單中點選儀錶板後，在左方的工具列表中可以設定儀表板的大小。\n將工作表加入儀錶板 接著在左方的工具列表中可以看到之前創建的工作表，將其拖拉至儀表板的空白處，即可在儀表板中加入工作表。\nTableau 在接收到工作表的資料後，便會自動讀取內容的資訊，並自動產生對應的初始圖表。\n由於剛拖入的工作表是固定大小且無法隨意改變的，這時在工作表的右上方有一個往下的箭頭，點下後選擇「漂浮」後，就可以改變圖形的大小。\n加入互動式按鈕 接著我們展示如何提供互動的介面，讓使用者可以自行挑選災情資料中，想要觀察的時間點。\n我們首先點選工作表右上方往下的箭頭，點下後選擇「篩選條件」，並選擇「日 (Case Time)」。\n點選後便會看到工作表彈出一個可以勾選日期的選擇欄位，這時使用者只要勾選想要觀察的日期，即可看到該日的災情情形。\n若我們想要將日期的選擇方式，也可以點選選擇欄位右上方的往下鍵號，便可以改變選擇欄位的樣式。\n例如我們可以將選擇的方式，由原先的清單方式改成滑桿方式如下。\n讓同一個互動按鈕作用於多個工作表 上述的互動按鈕只能用於一個工作表，若想讓儀錶板上的多個工作表同時共用同一個互動按鈕，可參考下列作法：\n照上面的方法先建立一個互動按鈕欄位；\n點選互動按鈕欄位右上方的往下鍵號，選擇套用於工作表並點擊選取的工作表；\n選擇要套用的工作表，即可完成設定。\n加入其他資訊 在儀表板上若需要加入其他的資訊，可以在工具列的左下方找到一個物件欄位，裡面有加入文字、圖片等物件，只要將所需的物件拖拉到儀錶板上方即可在儀表板中加入文字、圖片等物件。\n文本 (Story) 文本是可以將多個儀表板或工作表組合，以此來做成類似投影片的顯示效果。將文本加入儀表板或工作表的方法，跟從儀錶板中加入工作表一樣，將右方已創建的工作表或儀表板拖入文本的空白處即可完成。\n若有新增文本頁面的需求，可以點選上方的新增文本點，即可新增新的空白頁面，或是複製現有頁面成為新的頁面。\n最後，為了讓文本資訊更貼切其表達的內容，我們點擊文本頁面上方的方框兩次，即可修改頁面的標題。\n總結 在本章節簡單的介紹了 Tableau 的一些操作方法，以及如何利用 Tableau 來設計出與讀者互動的簡報/圖表，然而 Tableau 的功能遠遠不止這些，而網路上也有許多利用 Tableau 所創作出來的互動式簡報範例，因此鼓勵大家多多去其他網站上學習。\n參考資料 Tableau Public (https://public.tableau.com/) 使用 Tableau 設計自選式視覺化圖表—初階講義 (https://www.beclass.com/share/202007/7145995555358rap_2.pdf) 維度和度量（藍色和綠色）(https://help.tableau.com/current/pro/desktop/zh-tw/datafields_typesandroles.htm) VISUALIZING HEALTH DATA 看見健康數據 (https://visualizinghealthdata.idv.tw/?route=article/faq\u0026faq_id=138) Get Started with Tableau (https://help.tableau.com/current/guides/get-started-tutorial/en-us/get-started-tutorial-home.htm) Tableau Tutorial — Learn Data Visualization Using Tableau (https://medium.com/edureka/tableau-tutorial-71ef4c122e55) YouTube: Tableau教程 (https://www.youtube.com/watch?v=1q8rlXS4gEc\u0026list=PLAKvINjDAWNImaqKWJIErQtaNi19q84yF) YouTube: 用 Tableau 10.3 做大數據分析初階 (https://www.youtube.com/watch?v=oS5cM3v9ILc) YouTube: 用Tableau 做大數據分析進階 (https://www.youtube.com/watch?v=HuBQ-4khWm8) YouTube: Tableau For Data Science 2022 (https://www.youtube.com/watch?v=Wh4sCCZjOwo) YouTube: Tableau Online Training (https://www.youtube.com/watch?v=ttCDqyfrcEc) ",
    "description": "我們介紹使用 Tableau 工具呈現民生公共物聯網的開放資料，並使用空品資料和災害通報資料進行兩個範例演示。我們介紹如何使用工作表、儀表板和文本來建立互動式的資料視覺化系統，方便使用者近一步深入探索。我們同時提供豐富的參考資料供使用者學習參考。",
    "tags": [
      "空",
      "災"
    ],
    "title": "7.2. Tableau 應用",
    "uri": "/ch7/ch7.2/"
  },
  {
    "content": "\nTable Of Contents 章節目標 套件安裝與引用 讀取資料與資料預處理 (Preprocessing) 資料分群 / 集群分析 (Clustering) 快速傅立葉轉換 (Fast Fourier Transform) 小波轉換 (Wavelet Transform) 參考資料 集群分析 (Cluster Analysis) 是資料科學中常見的資料處理方法，其主要目的是被用來找出資料中相似的群聚，透過集群分析後，將性質相近的資料群聚在一起，以方便使用者可以針對特徵相似的資料進行更深入的分析與處理。在民生公共物聯網的開放資料中，由於每一種感測器的資料都是時序性的資料，為了將為數眾多的感測器進行適當的分群，以利更深入的資料分析，我們在這個單元將介紹時序資料在分群時常使用的特徵擷取方法，以及在集群分析時常使用到的分群方法。\n章節目標 學習使用快速傅立葉轉換 (FFT) 與小波轉換 (Wavelet) 擷取時序資料的特徵 採用非監督式學習的方法，針對時序資料進行分群 套件安裝與引用 在本章節中，我們將會使用到 pandas, numpy, matplotlib, pywt 等 Colab 開發平台已預先安裝好的套件，以及另外一個 Colab 並未預先安裝的套件 tslearn ，需使用下列的方式自行安裝：\n!pip install --upgrade pip !pip install tslearn 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport numpy as np import pandas as pd import pywt import os, zipfile import matplotlib.pyplot as plt from datetime import datetime, timedelta from numpy.fft import fft, ifft from pywt import cwt from tslearn.clustering import TimeSeriesKMeans 讀取資料與資料預處理 (Preprocessing) 由於我們這次要使用的是長時間的歷史資料，因此我們不直接使用 pyCIOT 套件的讀取資料功能，而直接從民生公共物聯網資料平台的歷史資料庫下載「中研院校園空品微型感測器」的 2021 年歷史資料，並存入 Air 資料夾中。\n!mkdir Air CSV_Air !wget -O Air/2021.zip -q \"https://history.colife.org.tw/?r=/download\u0026path=L%2Bepuuawo%2BWTgeizqi%2FkuK3noJTpmaJf5qCh5ZyS56m65ZOB5b6u5Z6L5oSf5ris5ZmoLzIwMjEuemlw\" !unzip Air/2021.zip -d Air 同時，由於所下載的資料是 zip 壓縮檔案的格式，我們需要先將其解壓縮，產生每日資料的壓縮檔案。由於我們在接下來的範例中只會使用到 2021/08 的資料，因此我們將 202108 子資料夾中的檔案進行解壓縮，讀取當中 csv 檔的資料，再存入 air_month 這個 dataframe 中。\nfolder = 'Air/2021/202108' extension_zip = 'zip' extension_csv = 'csv' for item in os.listdir(folder): if item.endswith(extension_zip): file_name = f'{folder}/{item}' zip_ref = zipfile.ZipFile(file_name) zip_ref.extractall(folder) zip_ref.close() air_month = pd.DataFrame() for item in os.listdir(folder): if item.endswith(extension_csv): file_name = f'{folder}/{item}' df = pd.read_csv(file_name, parse_dates=['timestamp']) air_month = air_month.append(df) air_month.set_index('timestamp', inplace=True) air_month.sort_values(by='timestamp', inplace=True) 目前 air_month 的格式還不符合我們的需求，需要先將其整理成以站點資料為欄，以時間資料為列的資料格式。我們先找出資料中共有多少站點，並將這些站點資訊存在一個序列中。\nid_list = air_month['device_id'].to_numpy() id_uniques = np.unique(id_list) id_uniques array(['08BEAC07D3E2', '08BEAC09FF12', '08BEAC09FF22', ..., '74DA38F7C648', '74DA38F7C64A', '74DA38F7C64C'], dtype=object) 接著我們將每個站點的資料存成一個欄，並放入 air 這個 dataframe 中。最後我們將所有下載的資料與解壓縮後產生的資料移除，以節省雲端的儲存空間。\nair = pd.DataFrame() for i in range(len(id_uniques)): # print('device_id==\"' + id_uniques[i] + '\"') query = air_month.query('device_id==\"' + id_uniques[i] + '\"') query.sort_values(by='timestamp', inplace=True) query_mean = query.resample('H').mean() query_mean.rename(columns={'PM25': id_uniques[i]}, inplace=True) air = pd.concat([air, query_mean], axis=1) !rm -rf Air 我們可以用下列的語法快速查看 air 的內容。\nair.info() print(air.head()) \u003cclass 'pandas.core.frame.DataFrame'\u003e DatetimeIndex: 745 entries, 2021-08-01 00:00:00 to 2021-09-01 00:00:00 Freq: H Columns: 1142 entries, 08BEAC07D3E2 to 74DA38F7C64C dtypes: float64(1142) memory usage: 6.5 MB 08BEAC07D3E2 08BEAC09FF12 08BEAC09FF22 08BEAC09FF2A \\ timestamp 2021-08-01 00:00:00 2.272727 1.250000 16.818182 12.100000 2021-08-01 01:00:00 1.909091 1.285714 13.181818 14.545455 2021-08-01 02:00:00 2.000000 1.125000 12.727273 16.600000 2021-08-01 03:00:00 2.083333 3.000000 11.800000 12.090909 2021-08-01 04:00:00 2.000000 2.600000 10.090909 8.545455 08BEAC09FF34 08BEAC09FF38 08BEAC09FF42 08BEAC09FF44 \\ timestamp 2021-08-01 00:00:00 2.727273 0.000000 1.181818 NaN 2021-08-01 01:00:00 2.000000 0.545455 0.909091 NaN 2021-08-01 02:00:00 4.090909 1.583333 0.636364 NaN 2021-08-01 03:00:00 0.545455 1.454545 1.181818 NaN 2021-08-01 04:00:00 1.363636 1.363636 2.454545 NaN 08BEAC09FF46 08BEAC09FF48 ... 74DA38F7C62A \\ timestamp ... 2021-08-01 00:00:00 2.636364 2.545455 ... 6.777778 2021-08-01 01:00:00 1.636364 2.272727 ... 7.800000 2021-08-01 02:00:00 1.400000 3.100000 ... 7.300000 2021-08-01 03:00:00 2.181818 4.000000 ... 12.000000 2021-08-01 04:00:00 1.909091 2.100000 ... 9.000000 74DA38F7C630 74DA38F7C632 74DA38F7C634 74DA38F7C63C \\ timestamp 2021-08-01 00:00:00 9.800000 13.200000 5.0 6.200000 2021-08-01 01:00:00 13.000000 15.700000 5.2 6.800000 2021-08-01 02:00:00 12.800000 19.300000 5.0 7.300000 2021-08-01 03:00:00 8.444444 15.200000 5.1 6.777778 2021-08-01 04:00:00 6.500000 10.222222 4.9 6.100000 74DA38F7C63E 74DA38F7C640 74DA38F7C648 74DA38F7C64A \\ timestamp 2021-08-01 00:00:00 NaN 7.500000 5.000000 9.000000 2021-08-01 01:00:00 NaN 10.200000 4.900000 6.600000 2021-08-01 02:00:00 NaN 10.500000 3.666667 7.600000 2021-08-01 03:00:00 NaN 8.500000 8.400000 8.000000 2021-08-01 04:00:00 NaN 5.571429 6.200000 5.666667 74DA38F7C64C timestamp 2021-08-01 00:00:00 7.600000 2021-08-01 01:00:00 7.700000 2021-08-01 02:00:00 7.888889 2021-08-01 03:00:00 6.400000 2021-08-01 04:00:00 5.000000 接下來，我們首先將資料中有資料缺漏的部分 (數值為 Nan) 予以刪除，並且先將資料繪製成圖形，觀察資料的分布狀況。\nair = air[:-1] air_clean = air.dropna(1, how='any') # 將具有Nan值的欄位捨棄 air_clean.plot(figsize=(20, 15), legend=None) 由於原始資料中感測器的瞬間數值容易因為環境變化而有瞬間劇烈的變動，因此我們使用移動平均的方式，將每十次的感測值進行移動平均，讓處理過的資料可以較為平順並反映感測器周遭的整體趨勢，以方便進入接下來的集群分析。\nair_clean = air_clean.rolling(window=10, min_periods=1).mean() # 使用移動平均使曲線變化更為平順 air_clean.plot(figsize=(20, 15), legend=None) 資料分群 / 集群分析 (Clustering) 由於目前 dataframe 內的資料儲存是將每一個測站的資料分別放在獨立的一欄 (column) 中，而將每一列 (row) 存放特定時間點所有站點的感測器數值，此種資料格式較適合作以時間為主軸的時序資料處理與分析，但是對於我們接下來要進行的資料分群，反而需要改成以測站為主軸進行資料處理，因此我們在開始之前，需要先將現有資料進行轉製 (transpose)，也就是將資料的欄與列交換，然後才能進入資料分群。\n資料分群已是一項十分普遍的資料科學方法，在許多常見的資料分群方法中，我們選用 tslearn 套件所提供的 KMeans 分群方法 (TimeSeriesKMeans)，在機器學習的領域中，把這類的方法歸類為一種「非監督式學習」的方法，因為在分群的過程中並沒有一定的標準將資料歸為某一群，而是只利用資料彼此間的相似度決定分群，因此後續若要找出離群值或進行預測也更為方便。\nKMeans 分群法的內部運作大致分為下列幾個步驟：\n先決定 k 的值，亦即最後要分成幾個群； 隨機 k 筆資料，當為起始的 k 個群的中心點 (又稱「群心」)； 根據距離公式計算每筆資料到每個群心的距離，並選擇最近的群心，將該筆資料歸屬到該群； 針對每個群，重新計算其新的群心，並重複上述步驟，直到 k 的群的群心不再有變動為止。 我們先設定 k=10，並使用 TimeSeriesKMeans，將資料分為 10 群 (0~9) 如下：\nair_transpose = air_clean.transpose() model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) # n_cluster:分群數量, max_iter: 分群的步驟最多重複幾次 pre = model.fit(air_transpose) pre.labels_ array([3, 3, 6, 3, 3, 3, 3, 6, 9, 6, 3, 6, 3, 3, 3, 3, 3, 1, 3, 3, 6, 3, 3, 3, 3, 6, 6, 6, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 9, 3, 3, 3, 3, 3, 6, 3, 2, 6, 3, 2, 3, 3, 3, 6, 9, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 6, 3, 3, 3, 3, 3, 6, 3, 6, 3, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 6, 9, 6, 7, 3, 2, 3, 6, 3, 3, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 6, 6, 3, 6, 3, 3, 3, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 6, 3, 3, 3, 0, 6, 0, 6, 3, 0, 3, 3, 3, 6, 6, 6, 6, 3, 3, 6, 3, 3, 9, 6, 3, 6, 3, 6, 6, 3, 2, 3, 3, 3, 6, 9, 6, 6, 3, 3, 6, 3, 0, 3, 3, 6, 6, 6, 3, 0, 0, 6, 3, 6, 6, 6, 3, 6, 6, 3, 0, 3, 3, 0, 3, 3, 6, 3, 6, 6, 6, 3, 0, 3, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0, 0, 9, 9, 0, 0, 0, 4, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0, 0, 9, 0, 9, 0, 0, 0, 9, 9, 0, 9, 0, 0, 0, 5, 0, 0, 0, 0, 9, 9, 0, 1, 0, 9, 2, 9, 9, 0, 0, 5, 0, 9, 0, 9, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 2, 3, 3, 0, 3, 0, 0, 0, 6, 3, 0, 8, 9, 3, 9, 9, 0, 5, 0, 3, 0, 9, 9, 5, 5, 5, 3, 5, 9, 3, 0, 0, 3, 0, 0, 0, 3, 5, 5, 3, 0, 5, 6, 5, 5, 5, 0, 9, 0, 6, 6, 6, 3, 0, 3, 5, 5, 9, 6, 3, 0, 0, 0, 0, 3, 5, 0, 5, 5, 0, 3, 3, 9, 5, 5, 9, 5, 9, 9, 9, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5]) 由於分群的結果有可能因為少數感測器擁有特殊的資料特徵而自成一個群集，而這些感測器其實可以被視為偏離測站，並沒有進一步分析的價值，因此我們下一步便要先統計一下每一個分離出來的群集，其內部感測器的個數是否只有一個，如果是的話，便將該群集捨棄掉。例如在這個範例中，我們便會捨棄掉其中只有一個感測器的群集如下：\n# build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_clean.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() clusters_dropped [7, 4, 8] 最後，我們將剩下的群集分別用繪圖的方式展示，可以發現每一個群集內的資料皆呈現極高的相似度，而不同群集的資料，彼此則呈現明顯可分辨的差異。為了量化群集內資料的相似度，我們定義一個新的變數 qualiuty，且這個變數的值等於內部每筆資料與其他資料的相關性 (correlation) 的平均值，且 quality 的值越低代表這個群集內部的資料越相似。\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_clean[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_clean[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() 到目前為止，我們已經展示如何使用空品感測的資料，針對資料內的特徵資訊進行資料分群，然而由於原始資料中夾雜許多的雜訊干擾，因此在分群上的表現仍有所不足。為了減少原始資料中的雜訊干擾，我們以下介紹兩種常用的進階方法：傅立葉轉換 (Fourier transform)、小波轉換 (wavelet transform)，分別提取時序資料的進階特徵後，再用來進行資料分群，可以更有效提升感測器分群的成效。\n快速傅立葉轉換 (Fast Fourier Transform) 傅立葉轉換 (Fourier Transform) 是一種常用的信號分析方法，能將原始資料由時域 (time domain) 轉換至頻域 (frequency domain) ，以方便更進一步的特徵擷取與資料分析，常見於工程與數學領域，以及聲音與時序資料的分析。由於標準的傅立葉轉換牽涉到複雜的數學運算，在實作上較為複雜費時，因此後來發展出將原始資料離散化的快速傅立葉轉換方法，可以大幅減少傅立葉轉換的運算複雜度，特別適用於大量數據資料的處理；因此在我們接下來的主題中，將使用快速傅立葉轉換的方法，來協助資料分群所需要的資料特徵的擷取。\n我們首先針對單一測站 (‘08BEAC07D3E2’) 透過繪圖的方式，觀察其在時域下的變化圖。\nair_clean['08BEAC07D3E2'].plot() # 畫出id=08BEAC07D3E2的測站資料 接著我們使用 numpy 套件中的快速傅立葉轉換工具 fft，將測站的資料輸入進行轉換，並用繪圖的方式觀察轉換到頻域後的資料分佈狀況。\nX = fft(air_clean['08BEAC07D3E2']) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] plt.figure(figsize = (12, 6)) plt.plot(f_oneside, np.abs(X[:n_oneside]), 'b') plt.xlabel('Freq (Hz)') plt.ylabel('FFT Amplitude |X(freq)|') plt.show() 然後我們將相同的轉換步驟，擴及到所有的感測器，並且將所有感測器完成快速傅立葉轉換後的結果，儲存在 air_fft 變數中。\nair_fft = pd.DataFrame() col_names = list(air_clean.columns) for name in col_names: X = fft(air_clean[name]) N = len(X) n = np.arange(N) # get the sampling rate sr = 1 / (60*60) T = N/sr freq = n/T # Get the one-sided specturm n_oneside = N//2 # get the one side frequency f_oneside = freq[:n_oneside] air_fft[name] = np.abs(X[:n_oneside]) print(air_fft.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 12019.941563 11255.762431 13241.408211 12111.740447 11798.584351 1 3275.369377 2640.372699 1501.672853 3096.493565 3103.006928 2 1109.613670 1201.362571 257.659947 1085.384353 1128.373457 3 2415.899146 1631.128345 888.838822 2281.597031 2301.936400 4 1130.973327 446.032078 411.940722 1206.512460 1042.054041 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 12441.845754 11874.390693 12259.096742 3553.255916 13531.649701 ... 1 3262.357287 2999.042917 1459.720167 1109.764942 646.846038 ... 2 1075.877632 1005.445596 478.569869 368.572815 1163.425916 ... 3 2448.646527 2318.870954 956.029693 272.486798 553.409732 ... 4 1087.461354 1172.755489 437.920193 471.824176 703.557830 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 11502.841221 10589.689400 11220.068048 10120.220198 11117.146124 1 2064.762890 1407.105290 1938.647888 1126.088084 2422.787262 2 2163.528535 1669.014077 2054.586664 1759.326882 1782.523300 3 1564.407983 1157.759192 1253.849261 1244.799149 1519.477057 4 1484.232397 1177.909914 1318.704021 1106.349846 1373.167639 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 11243.094213 26.858333 9408.414826 11228.246949 8931.871618 1 2097.343959 15.020106 1667.485473 1687.251179 1395.239491 2 1806.524987 10.659603 1585.987276 1851.628868 1527.925427 3 1521.392873 6.021244 1217.547879 1240.173667 1022.239794 4 1393.469185 3.361938 1161.975844 1350.756178 1051.434944 [5 rows x 398 columns] 我們接著使用和前面相同的方法，使用 TimeSeriesKMeans 針對已轉換到頻域的感測器資料分成 10 個群集，並將分群後只有單一感測器的群集予以刪除，在這個範例中，最後會剩下 9 個群集，我們把每個感測器所屬的群集代碼列印出來。\nfft_transpose = air_fft.transpose() # 將資料行列交換以符合分群模型輸入的需求 model = TimeSeriesKMeans(n_clusters=10, metric=\"dtw\", max_iter=5) pre = model.fit(fft_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_fft.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 0 3 08BEAC09FF42 7 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 0 8 08BEAC09FF8C 2 9 08BEAC09FF9C 8 最後，我們把九個群集的感測器資料分別繪製出來，可以發現每個群集內的感測器在頻域上的資料皆有極度的相似性，同時不同群集的感測器在頻域上的資料彼此相異程度，亦明顯比同群集內的感測器資料差異度為大。\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) for idx, cluster_number in enumerate(clusters_final): x_corr = air_fft[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_fft[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() 小波轉換 (Wavelet Transform) 除了傅立葉轉換外，小波轉換也是另一種將原始資料從時域轉換到頻域的常見手法，相較於傅立葉轉換，小波轉換可以提供更多頻域資料的觀察角度，因此近期無論在視訊、音訊、時序等相關的資料分析，已被廣泛地使用，並得到很好的效果。\n我們使用 pywt 套件進行小波轉換的資料處理，由於小波轉換會使用到母小波來擷取時序資料中的特徵，因此我們先用下列的語法查看可使用的母小波名稱。\nwavlist = pywt.wavelist(kind=\"continuous\") print(wavlist) ['cgau1', 'cgau2', 'cgau3', 'cgau4', 'cgau5', 'cgau6', 'cgau7', 'cgau8', 'cmor', 'fbsp', 'gaus1', 'gaus2', 'gaus3', 'gaus4', 'gaus5', 'gaus6', 'gaus7', 'gaus8', 'mexh', 'morl', 'shan'] 不同於傅立葉轉換只能看到頻率上的變化，小波轉換可以將有限長的母小波進行縮放並在一段資料上擷取特徵，而在挑選母小波時可以先將其作圖觀察後再做決定，若以 morl 母小波為例，我們可以使用下列的語法進行作圖。\nwav = pywt.ContinuousWavelet(\"morl\") scale = 1 int_psi, x = pywt.integrate_wavelet(wav, precision=10) int_psi /= np.abs(int_psi).max() wav_filter = int_psi[::-1] nt = len(wav_filter) t = np.linspace(-nt // 2, nt // 2, nt) plt.plot(t, wav_filter.real) plt.ylim([-1, 1]) plt.xlabel(\"time (samples)\") 我們首先進行小波轉換的基本參數設定，並且使用 morl 母小波，針對所選定的感測器，將小波的縮放範圍設為 1~31 倍，進行小波轉換並作圖。\n# 參數設定 F = 1 #Samples per hour hours = 744 nos = np.int(F*hours) #No of samples in 31 days x = air_clean['08BEAC09FF2A'] scales = np.arange(1, 31, 1) coef, freqs = cwt(x, scales, 'morl') # scalogram plt.figure(figsize=(15, 10)) plt.imshow(abs(coef), extent=[0, 744, 30, 1], interpolation='bilinear', cmap='viridis', aspect='auto', vmax=abs(coef).max(), vmin=abs(coef).min()) plt.gca().invert_yaxis() plt.yticks(np.arange(1, 31, 1)) plt.xticks(np.arange(0, nos/F, nos/(20*F))) plt.ylabel(\"scales\") plt.xlabel(\"hour\") plt.colorbar() plt.show() 整體而言，我們可以發現在圖中 scales 較大時的顏色較偏向黃綠色，代表擷取出來的特徵與母小波較為相近，反之則較偏向藍色，這些比對結果的特徵數值將會被用於接下來的資料分群。\n不過，由於經過小波轉換後，每個測站的資料已經轉為二維的特徵數值，在開始進行資料分群前，我們需要先將原本二維資料的每一個欄位串接起來，變成一維的資料格式，並存入 air_cwt 變數中。\nair_cwt = pd.DataFrame() scales = np.arange(28, 31, 2) col_names = list(air_clean.columns) for name in col_names: coef, freqs = cwt(air_clean[name], scales, 'morl') air_cwt[name] = np.abs(coef.reshape(coef.shape[0]*coef.shape[1])) print(air_cwt.head()) 08BEAC07D3E2 08BEAC09FF22 08BEAC09FF2A 08BEAC09FF42 08BEAC09FF48 \\ 0 0.778745 6.336664 2.137342 1.849035 0.778745 1 0.929951 8.348031 2.977576 1.829540 0.958915 2 1.190476 11.476048 3.223773 1.832544 1.292037 3 1.227021 12.890554 4.488244 1.634717 1.338655 4 1.252126 14.103178 5.715656 1.430744 1.376562 08BEAC09FF66 08BEAC09FF80 08BEAC09FF82 08BEAC09FF8C 08BEAC09FF9C ... \\ 0 0.555941 0.334225 8.110581 2.287378 0.409913 ... 1 0.596532 0.201897 8.774271 1.706411 0.311242 ... 2 0.771641 0.294242 8.327808 1.018296 2.754100 ... 3 0.713931 0.065011 8.669036 0.249029 3.278048 ... 4 0.670639 0.193083 8.795376 0.624988 3.628597 ... 74DA38F7C504 74DA38F7C514 74DA38F7C524 74DA38F7C554 74DA38F7C5BA \\ 0 1.875873 1.090635 0.284901 1.111999 1.049112 1 0.643478 1.446061 0.420701 0.974641 0.478931 2 1.101801 2.404254 1.389941 1.190141 0.511454 3 2.370436 2.510728 1.927692 0.703424 1.054164 4 3.563873 2.387982 2.463458 0.098485 1.448078 74DA38F7C5BC 74DA38F7C5E0 74DA38F7C60C 74DA38F7C62A 74DA38F7C648 0 1.107551 0.000918 0.080841 0.859855 0.415574 1 0.245802 0.005389 1.053260 1.762642 0.190458 2 1.162331 0.009754 2.667145 3.210326 0.516525 3 1.993138 0.014496 3.669712 3.889207 0.666492 4 2.672666 0.020433 4.482730 4.311888 0.677992 [5 rows x 398 columns] 由於經過二維資料轉換成一維資料的程序，每筆資料的特徵數值數量因而增加，將造成後續資料運算的複雜度大幅提升，因此我們先只取前 100 個特徵數值做為每個感測器的代表性特徵，並以此套用 KMeans 方法進行資料分群。\n由於小波轉換可以得到更細微的資料特徵，因此我們在資料分群的過程中，預設分為 20 個群集（讀者可自行測試不同的群集數目設定，觀察結果會產生哪些變化）；此外，我們對於分群結果也一樣先檢查是否存在只有單一感測器的小群集，並將其剔除，以避免少數特殊狀況的感測器，影響整體資料分群的結果。\nair_cwt_less = air_cwt.iloc[:, 0:101] cwt_transpose = air_cwt_less.transpose() # 將資料行列交換以符合分群模型輸入的需求 model = TimeSeriesKMeans(n_clusters=20, metric=\"dtw\", max_iter=10, verbose=1, n_jobs=-1) pre = model.fit(cwt_transpose) # build helper df to map metrics to their cluster labels df_cluster = pd.DataFrame(list(zip(air_cwt.columns, pre.labels_)), columns=['metric', 'cluster']) # make some helper dictionaries and lists cluster_metrics_dict = df_cluster.groupby(['cluster'])['metric'].apply(lambda x: [x for x in x]).to_dict() cluster_len_dict = df_cluster['cluster'].value_counts().to_dict() clusters_dropped = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]==1] clusters_final = [cluster for cluster in cluster_len_dict if cluster_len_dict[cluster]\u003e1] clusters_final.sort() print(df_cluster.head(10)) metric cluster 0 08BEAC07D3E2 7 1 08BEAC09FF22 3 2 08BEAC09FF2A 6 3 08BEAC09FF42 10 4 08BEAC09FF48 7 5 08BEAC09FF66 7 6 08BEAC09FF80 7 7 08BEAC09FF82 6 8 08BEAC09FF8C 15 9 08BEAC09FF9C 19 在我們的範例中，經過上述的程序後，最後留下 14 個群集，我們將每個群集內的感測器原始資料一一繪出，我們可以發現相同群集內的感測器資料一致性更加明顯，同時不同群集間的差異也更加細緻明確，比之前使用原始資料或傅立葉轉換時的成效更為明顯。\nfig, axes = plt.subplots(nrows=len(clusters_final), ncols=1, figsize=(20, 15), dpi=500) # legend = axes.legend(loc=\"upper left\", bbox_to_anchor=(1.02, 0, 0.07, 1)) for idx, cluster_number in enumerate(clusters_final): x_corr = air_cwt[cluster_metrics_dict[cluster_number]].corr().abs().values x_corr_mean = round(x_corr[np.triu_indices(x_corr.shape[0],1)].mean(),2) plot_title = f'cluster {cluster_number} (quality={x_corr_mean}, n={cluster_len_dict[cluster_number]})' air_cwt[cluster_metrics_dict[cluster_number]].plot(ax=axes[idx], title=plot_title) axes[idx].get_legend().remove() fig.tight_layout() plt.show() 參考資料 民生公共物聯網歷史資料 (https://history.colife.org.tw/) Time Series Clustering — Deriving Trends and Archetypes from Sequential Data | by Denyse | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) How to Apply K-means Clustering to Time Series Data | by Alexandra Amidon | Towards Data Science (https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) Understanding K-means Clustering: Hands-On with SciKit-Learn | by Carla Martins | May, 2022 | Towards AI (https://pub.towardsai.net/understanding-k-means-clustering-hands-on-with-scikit-learn-b522c0698c81) Fast Fourier Transform. How to implement the Fast Fourier… | by Cory Maklin | Towards Data Science (https://towardsdatascience.com/fast-fourier-transform-937926e591cb) PyWavelets/pywt: PyWavelets - Wavelet Transforms in Python (https://github.com/PyWavelets/pywt) ",
    "description": "我們介紹較為進階的資料分群分析。我們首先介紹兩種時間序列的特徵擷取方法，分別是傅立葉轉換 (Fourier Transform) 和小波轉換 (Wavelet Transform)，並且以簡要的方式說明兩種轉換方式之異同。我們介紹兩種不同的時間序列比較方法，分別是幾何距離 (Euclidean Distance) 與動態時間規整 (Dynamic Time Warping, DTW) 距離，並根據所使用的距離函數，套用既有的分群演算法套件，並且探討不同的資料集與不同時間解析度的資料分群，在真實場域所代表的意義，以及可能衍生的應用。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "4.3. 時間序列屬性分群",
    "uri": "/ch4/ch4.3/"
  },
  {
    "content": "\nTable Of Contents 套件安裝與引用 讀取資料與環境設定 資料前處理 校正模型訓練與驗證 輸出本日最佳校正模型 校正成果簡介 參考資料 在這篇文章中，我們將以民生公共物聯網中的空品資料為例，介紹如何讓兩個不同等級的空品感測資料，可以透過資料科學的方法，進行系統性的動態校正，以達到系統資料融合的目的，讓不同佈建專案的成果，可以合力打造更為全面性的空品感測結果。我們使用下列兩種空品感測系統：\n環保署空品感測器：在傳統的空品監測方式中，以極為專業、大型、昂貴的監測站為主，該專業的監測站由於部署及維護成本較高，通常會是由當地的環境保護機構（EPA）來負責運營。也因此不會在每個社區都有部署。依據台灣環保署網站公告，截至目前 2022 年 7 月為止，台灣的中央監測站數量為 81 座。 微型空品感測器：與傳統的大型專業測站相比，微型空品感測器利用低成本的感測器，透過網路的資料串連，以物聯網的方式建構更密集的空品感測網，這項技術不僅架設的成本低，連帶的提供了更靈活的安裝條件，擴大了可覆蓋的範圍。同時，這項技術具有易於安裝和維護的特點，滿足了大規模即時空氣品質監測系統的條件，而且能夠做到每分鐘上傳一次數據的資料頻率，也使用戶對於突然的污染事件得以立即的反應，進一步的降低傷害。 當然我們不能期待成本較低的感測器會擁有專業儀器的高準確度，如何提高其精準度成為成了另一項需要被解決的問題。因此，在以下的內容中，我們將演示如何利用資料科學的方法，調校微型空品感測器的空品感測結果，讓其感測資料的準確度能達到與環保署空品感測器相比擬的方法，以促進系統整合與更近一步的資料應用。\n套件安裝與引用 在本章節中，我們將會使用到 pandas, numpy, datetime, sklearn, scipy, 及 joblib 等套件，這些套件在我們使用的開發平台 Colab 上已有預先提供，因此我們不需要另行安裝，可以直接用下列的方法引用，以備之後資料處理與分析使用。\nimport pandas as pd import numpy as np from datetime import datetime, timedelta from sklearn import linear_model, svm, tree from sklearn import metrics as sk_metrics from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split, cross_validate from sklearn.feature_selection import f_regression import scipy.stats as stats import joblib 讀取資料與環境設定 在這個範例中，我們將使用環保署空品測站萬華站 (wanhua)，以及中研院校園空品微型感測器中，兩台與環保署萬華測站放置在同一地點的空氣盒子 (機器代號分別為 08BEAC028A52 和 08BEAC028690) 作為案例，並且使用線性迴歸 (Linear Regression)、隨機森林迴歸 (Random Foreset Regression) 以及支持向量迴歸 (Support Vector Regression, SVR) 三個訓練模型，同時考量感測資料中的 PM2.5 濃度、溫度、相對濕度和時間戳記（小時值）共計四個欄位的資料，搭配三個不同時間長度的歷史資料（3 天、5 天、8 天），進行一系列的探究。\n為了方便起見，我們先依據這些想定的內容，進行下列的程式初始設定。\nSITE= \"wanhua\" # 環保署萬華測站在輸入資料中的代碼 EPA= \"EPA-Wanhua\" # 兩台與環保署萬華測站放置在相同位置的空氣盒子機器代號 AIRBOXS= ['08BEAC028A52', '08BEAC028690'] # 不同歷史資料的長度設定 DAYS= [8, 5, 3] # 本範例所使用的迴歸方法 METHODS= ['LinearRegression', 'RandomForestRegressor', 'SVR'] # 本範例所使用的迴歸方法的簡寫 METHOD_SW= { 'LinearRegression':'LinearR', 'RandomForestRegressor':'RFR', 'SVR':'SVR' } # 本範例所使用的迴歸方法的函數名稱 METHOD_FUNTION= {'LinearRegression':linear_model.LinearRegression(), 'RandomForestRegressor': RandomForestRegressor(n_estimators= 300, random_state= 36), 'SVR': svm.SVR(C=20) } # 本範例中所使用的各項欄位資料對照 FIELD_SW= {'s_d0':'PM25', 'pm25':'PM25', 'PM2_5':\"PM25\", 'pm2_5':\"PM25\", 's_h0':\"HUM\", 's_t0':'TEM'} # 本範例中所探究的各種不同欄位組合方式 FEATURES_METHOD= {'PHTR':[\"PM25\", \"HR\", \"TEM\", \"HUM\"], 'PH':['PM25','HR'], 'PT':['PM25','TEM'], 'PR':['PM25', 'HUM'], 'P':['PM25'], 'PHR':[\"PM25\", \"HR\", \"HUM\"], 'PTR':[\"PM25\", \"TEM\", \"HUM\"], 'PHT':[\"PM25\", \"HR\", \"TEM\"] } 接下來我們考慮需要下載多少資料供系統校正與資料融合使用。如下圖所示，假設我們所要取得的是第 N 天的校正模型，那麽第 N - 1 天的資料便會做為測試資料使用，以評估未來校正模型的準確度；因此，倘若我們所設定的訓練資料為 X 天，那就代表第 N - 2 到 N - (2+X) 天的歷史資料將被作為訓練資料使用。在我們的想定中，我們將 N 設定為目前時間（今天），而最大的可能 X 值為 8，因此我們將需要預先準備共計十天的歷史資料以供接下來的操作使用。\n為了方面日後的使用，我們先用下面的程式碼，明定校正模型的日期 (第 N 天: TODAY)、測試資料的日期 (第 N-1 天: TESTDATE)、以及訓練資料的結束日期 (第 N-2 天: ENDDATE)。\nTODAY = datetime.today() TESTDATE = (TODAY - timedelta(days=1)).date() ENDDATE = (TODAY - timedelta(days=2)).date() 我們使用中研院校園空品微型感測器所提供的資料下載網址，根據指定的日期 與機器代碼 ，可以使用 CSV 檔案格式下載對應日期與機器的感測資料。注意，由於下載平台的限制，下載時所指定的日期 只能是下載當天算起，過去 30 天內的日期。\nhttps://pm25.lass-net.org/data/history-date.php?device_id=\u003cID\u003e\u0026date=\u003cYYY-MM-DD\u003e\u0026format=CSV 例如，假設我們要下載環保署萬華測站在 2022 年 9 月 21 日的感測資料，我們可以用下列的方式進行下載：\nhttps://pm25.lass-net.org/data/history-date.php?device_id=EPA-Wanhua\u0026date=2022-09-21\u0026format=CSV 利用這個資料下載的方式，我們撰寫一個 Python 的小函式 getDF，可以針對輸入的機器代碼，下載過去 10 天的感測資料，並將資料彙整成單一一個 DataFrame 物件後回傳。\ndef getDF(id): temp_list = [] for i in range(1,11): date = (TODAY - timedelta(days=i)).strftime(\"%Y-%m-%d\") URL = \"https://pm25.lass-net.org/data/history-date.php?device_id=\" + id + \"\u0026date=\" + date + \"\u0026format=CSV\" temp_DF = pd.read_csv( URL, index_col=0 ) temp_list.append( temp_DF ) print(\"ID: {id}, Date: {date}, Shape: {shape}\".format(id=id, date=date, shape=temp_DF.shape)) All_DF = pd.concat( temp_list ) return All_DF 接著，我們可以下載安裝在環保署萬華測站的第一台空氣盒子感測資料，並存放在 AirBox1_DF 物件中：\n# First AirBox device AirBox1_DF = getDF(AIRBOXS[0]) AirBox1_DF.head() ID: 08BEAC028A52, Date: 2022-09-29, Shape: (208, 19) ID: 08BEAC028A52, Date: 2022-09-28, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-27, Shape: (225, 19) ID: 08BEAC028A52, Date: 2022-09-26, Shape: (230, 19) ID: 08BEAC028A52, Date: 2022-09-25, Shape: (231, 19) ID: 08BEAC028A52, Date: 2022-09-24, Shape: (232, 19) ID: 08BEAC028A52, Date: 2022-09-23, Shape: (223, 19) ID: 08BEAC028A52, Date: 2022-09-22, Shape: (220, 19) ID: 08BEAC028A52, Date: 2022-09-21, Shape: (222, 19) ID: 08BEAC028A52, Date: 2022-09-20, Shape: (215, 19) 利用同樣的方法，我們依次下載在環保署萬華測站的第二台空氣盒子感測資料，以及環保署萬華測站的感測資料，並分別存放在 AirBox2_DF 和 EPA_DF 物件中。\n# Second AirBox device AirBox2_DF = getDF(AIRBOXS[1]) # EPA station EPA_DF = getDF(EPA) 資料前處理 由於我們目前所下載的資料中，含有許多不需要用到的欄位，為了避免佔用太多的記憶體空間，我們先精簡所使用的資料，只留下需要的內容。\nCol_need = [\"timestamp\", \"s_d0\", \"s_t0\", \"s_h0\"] AirBox1_DF_need = AirBox1_DF[Col_need] print(AirBox1_DF_need.head()) AirBox2_DF_need = AirBox2_DF[Col_need] print(AirBox2_DF_need.head()) Col_need = [\"time\", \"date\", \"pm2_5\"] EPA_DF_need = EPA_DF[Col_need] print(EPA_DF_need.head()) del AirBox1_DF del AirBox2_DF del EPA_DF timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:03:28Z 9.0 29.75 71.0 1 2022-09-30T00:33:46Z 11.0 31.36 67.0 2 2022-09-30T00:39:51Z 10.0 31.50 67.0 3 2022-09-30T00:45:58Z 12.0 31.50 66.0 4 2022-09-30T00:52:05Z 12.0 31.86 66.0 timestamp s_d0 s_t0 s_h0 index 0 2022-09-30T00:00:31Z 9.0 29.36 -53.0 1 2022-09-30T00:07:17Z 9.0 29.50 -52.0 2 2022-09-30T00:23:47Z 10.0 30.25 -45.0 3 2022-09-30T00:34:24Z 10.0 31.11 -36.0 4 2022-09-30T00:40:31Z 11.0 31.25 -35.0 time date pm2_5 index 0 00:00:00 2022-09-30 9.0 1 01:00:00 2022-09-30 10.0 2 02:00:00 2022-09-30 16.0 3 03:00:00 2022-09-30 19.0 4 04:00:00 2022-09-30 20.0 接著為了統一資料欄位，我們將環保署測站原有的 date 與 time 欄位進行整併，並產生一個新的 timestamp 欄位。\nEPA_DF_need['timestamp'] = pd.to_datetime( EPA_DF_need[\"date\"] + \"T\" + EPA_DF_need[\"time\"], utc=True ) print(EPA_DF_need.head()) time date pm2_5 timestamp index 0 00:00:00 2022-09-30 9.0 2022-09-30 00:00:00+00:00 1 01:00:00 2022-09-30 10.0 2022-09-30 01:00:00+00:00 2 02:00:00 2022-09-30 16.0 2022-09-30 02:00:00+00:00 3 03:00:00 2022-09-30 19.0 2022-09-30 03:00:00+00:00 4 04:00:00 2022-09-30 20.0 2022-09-30 04:00:00+00:00 由於空氣盒子與環保署測站的資料時間解析度不同，為了能拉齊兩邊的資料，我們將空氣盒子的資料由原有每五分鐘一筆資料，用每小時取平均值的方式，改為每小時一筆資料。\n# 小時平均 def getHourly(DF): DF = DF.set_index( pd.DatetimeIndex(DF[\"timestamp\"]) ) DF_Hourly = DF.resample('H').mean() DF_Hourly.reset_index(inplace=True) return DF_Hourly AirBox1_DF_need_Hourly = getHourly( AirBox1_DF_need) AirBox2_DF_need_Hourly = getHourly( AirBox2_DF_need) EPA_DF_need_Hourly = getHourly( EPA_DF_need) # 可省略，原始數據已經是小時平均 del AirBox1_DF_need del AirBox2_DF_need del EPA_DF_need print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp s_d0 s_t0 s_h0 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp pm2_5 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 為了方面接下來的運算與識別，我們先把兩個來源的資料欄位，換成容易識別的欄位名稱。\n# 轉換欄位名稱: s_d0 = PM25, s_h0 = 相對濕度(HUM), s_t0 = 溫度(TEMP) Col_rename = {\"s_d0\":\"PM25\", \"s_h0\":\"HUM\", \"s_t0\":\"TEM\"} AirBox1_DF_need_Hourly.rename(columns=Col_rename, inplace=True) AirBox2_DF_need_Hourly.rename(columns=Col_rename, inplace=True) # 轉換欄位名稱: pm2_5 = PM25, Col_rename = {\"pm2_5\":\"EPA_PM25\"} EPA_DF_need_Hourly.rename(columns=Col_rename, inplace=True) print(AirBox1_DF_need_Hourly.head()) print(EPA_DF_need_Hourly.head()) timestamp PM25 TEM HUM 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 1 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 2 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 3 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 4 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 timestamp EPA_PM25 0 2022-09-21 00:00:00+00:00 6.0 1 2022-09-21 01:00:00+00:00 14.0 2 2022-09-21 02:00:00+00:00 NaN 3 2022-09-21 03:00:00+00:00 11.0 4 2022-09-21 04:00:00+00:00 10.0 由於兩台空氣盒子的硬體相同且擺放位置相同，可以視為同一個資料來源，因此我們將兩台空氣盒子的資料進行合併，產生 AirBoxs_DF 物件，並且將這個新的物件與環保署空品測站的資料用「交集」的方式，以時間欄位為基準進行交集合併，並產生 All_DF 物件。\n# 將兩台 AirBox 資料合併 AirBoxs_DF = pd.concat([AirBox1_DF_need_Hourly, AirBox2_DF_need_Hourly]).reset_index(drop=True) # 以時間欄位為基準，用只保留交集部分的方法 (inner)，合併 EPA 與 Airbox All_DF = pd.merge( AirBoxs_DF, EPA_DF_need_Hourly, on=[\"timestamp\"], how=\"inner\" ) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 02:00:00+00:00 11.100000 29.919000 61.100000 NaN 5 2022-09-21 02:00:00+00:00 9.000000 30.418889 -59.222222 NaN 6 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 7 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 8 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 9 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 我們先將資料中存在空值 (NaN) 的狀況予以排除。\n# 去除空值 All_DF.dropna(how=\"any\", inplace=True) All_DF.reset_index(inplace=True, drop=True) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 最後，由於在進行模型建構時，會使用到每日小時值的資料，因此我們替 All_DF 增加一個 HR 欄位，內容為 timestamp 中的小時值。\ndef return_HR(row): row['HR'] = int(row[ \"timestamp\" ].hour) return row All_DF = All_DF.apply(return_HR , axis=1) print(All_DF.head(10)) timestamp PM25 TEM HUM EPA_PM25 HR 0 2022-09-21 00:00:00+00:00 9.555556 27.024444 62.111111 6.0 0 1 2022-09-21 00:00:00+00:00 8.100000 27.083000 -65.300000 6.0 0 2 2022-09-21 01:00:00+00:00 10.166667 30.036667 59.500000 14.0 1 3 2022-09-21 01:00:00+00:00 8.100000 30.194000 -57.100000 14.0 1 4 2022-09-21 03:00:00+00:00 8.400000 29.980000 61.900000 11.0 3 5 2022-09-21 03:00:00+00:00 7.100000 29.408000 -64.400000 11.0 3 6 2022-09-21 04:00:00+00:00 5.250000 31.308750 56.000000 10.0 4 7 2022-09-21 04:00:00+00:00 4.777778 29.882222 -64.000000 10.0 4 8 2022-09-21 05:00:00+00:00 5.000000 30.033333 60.333333 8.0 5 9 2022-09-21 05:00:00+00:00 4.500000 28.777500 -66.875000 8.0 5 校正模型訓練與驗證 完成資料的整備後，接下來我們開始建立候選的校正模型。由於我們在這個案例中，總共討論 3 種迴歸方式、3 種歷史資料長度與 8 種特徵組合，因此我們總共將產出 3 x 3 x 8 = 72 個候選模型。\n首先，針對在產製校正模型時須參考特定長度的歷史資料，我們設計 SlideDay 函式，根據所輸入的歷史資料長度 day，回傳輸入資料 Hourly_DF 中從 enddate 開始往前共計 day 天的資料。\ndef SlideDay( Hourly_DF, day, enddate ): startdate= enddate- timedelta( days= (day-1) ) time_mask= Hourly_DF[\"timestamp\"].between( pd.Timestamp(startdate, tz='utc'), pd.Timestamp(enddate, tz='utc') ) return Hourly_DF[ time_mask ] 接著，我們將 site 測站自 enddate 起往前 day 天的感測資料 Training_DF，根據 feature 特徵組合整理成為訓練資料。然後我們套用 method 迴歸方法，讓所產製的校正模型所產生的預測值可以逼近訓練資料中的 EPA_PM25 欄位的資料值。我們同時計算校正模型本身的平均絕對誤差 (Mean Absolute Error, MAE) 和均方誤差 (Mean Squared Error, MSE)。其中，平均絕對誤差 (MAE) 為目標值和預測值之差的絕對值之和，可以能反映預測值誤差的實際情況，數值越小代表成效越好。均方誤差 (MSE) 則為預測值和實際觀測值間差的平方的均值，MSE 的值越小，說明預測模型描述實驗資料具有更好的精確度。\ndef BuildModel( site, enddate, feature, day, method, Training_DF ): X_train = Training_DF[ FEATURES_METHOD[ feature ] ] Y_train = Training_DF[ \"EPA_PM25\" ] model_result = {} model_result[\"site\"], model_result[\"day\"], model_result[\"feature\"], model_result[\"method\"] = site, day, feature, method model_result[\"datapoints\"], model_result[\"modelname\"] = X_train.shape[0], (site + \"_\" + str(day) + \"_\" + METHOD_SW[method] + \"_\" + feature) model_result[\"date\"] = enddate.strftime( \"%Y-%m-%d\" ) # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) model_result['create_timestamp_utc'] = Now_Time ### training model ### print( \"[BuildR]-\\\"{method}\\\" with {day}/{feature}\".format(method=method, day=day, feature=feature) ) # fit lm = METHOD_FUNTION[ method ] lm.fit( X_train, Y_train ) # get score Y_pred = lm.predict( X_train ) model_result['Train_MSE'] = MSE = sk_metrics.mean_squared_error( Y_train, Y_pred ) model_result['Train_MAE'] = sk_metrics.mean_absolute_error( Y_train, Y_pred ) return model_result, lm 除了針對訓練資料在建立校正模型時的 MAE 和 MSE 評估外，我們也同時考慮所產製的校正模型在非訓練資料上的預測成果表現。因此，我們針對所產製的模型 lm，根據其所需要帶入的特徵資料 feature，帶入測試資料產出預測值，並與測試資料中的 EPA_PM25 欄位進行比對，計算 MAE 和 MSE，以作為後續評估不同校正模型適用性的參考。\ndef TestModel( site, feature, modelname, Testing_DF, lm ): X_test = Testing_DF[ FEATURES_METHOD[ feature ] ] Y_test = Testing_DF[ \"EPA_PM25\" ] # add timestamp field Now_Time = datetime.utcnow().strftime( \"%Y-%m-%d %H:%M:%S\" ) ### testing model ### # predict Y_pred = lm.predict( X_test ) # get score test_result = {} test_result[\"test_MSE\"] = round( sk_metrics.mean_squared_error( Y_test, Y_pred ), 3) test_result[\"test_MAE\"] = round( sk_metrics.mean_absolute_error( Y_test, Y_pred ), 3) return test_result 最後，我們整合剛剛已經完成的 SlideDay、BuildModel 和 TestModel，逐一完成共計 72 個校正模型的產製，並且分別計算其針對訓練資料與測試資料的 MAE 和 MSE，且將所有結果存入 AllResult_DF 物件中。\nAllResult_list = [] for day in DAYS: for method in METHODS: for feature in FEATURES_METHOD: Training_DF = SlideDay(All_DF, day, ENDDATE)[ FEATURES_METHOD[feature] + [\"EPA_PM25\"] ] result, lm = BuildModel( SITE, TESTDATE, feature, day, method, Training_DF ) test_result = TestModel(SITE, feature, result[\"modelname\"], SlideDay(All_DF, 1, TESTDATE), lm) R_DF = pd.DataFrame.from_dict( [{ **result, **test_result }] ) AllResult_list.append( R_DF ) AllResult_DF = pd.concat(AllResult_list) AllResult_DF.head() [BuildR]-\"LinearRegression\" with 8/PHTR [BuildR]-\"LinearRegression\" with 8/PH [BuildR]-\"LinearRegression\" with 8/PT [BuildR]-\"LinearRegression\" with 8/PR [BuildR]-\"LinearRegression\" with 8/P [BuildR]-\"LinearRegression\" with 8/PHR [BuildR]-\"LinearRegression\" with 8/PTR [BuildR]-\"LinearRegression\" with 8/PHT [BuildR]-\"RandomForestRegressor\" with 8/PHTR [BuildR]-\"RandomForestRegressor\" with 8/PH [BuildR]-\"RandomForestRegressor\" with 8/PT [BuildR]-\"RandomForestRegressor\" with 8/PR [BuildR]-\"RandomForestRegressor\" with 8/P [BuildR]-\"RandomForestRegressor\" with 8/PHR [BuildR]-\"RandomForestRegressor\" with 8/PTR [BuildR]-\"RandomForestRegressor\" with 8/PHT [BuildR]-\"SVR\" with 8/PHTR [BuildR]-\"SVR\" with 8/PH [BuildR]-\"SVR\" with 8/PT [BuildR]-\"SVR\" with 8/PR [BuildR]-\"SVR\" with 8/P [BuildR]-\"SVR\" with 8/PHR [BuildR]-\"SVR\" with 8/PTR [BuildR]-\"SVR\" with 8/PHT [BuildR]-\"LinearRegression\" with 5/PHTR [BuildR]-\"LinearRegression\" with 5/PH [BuildR]-\"LinearRegression\" with 5/PT [BuildR]-\"LinearRegression\" with 5/PR [BuildR]-\"LinearRegression\" with 5/P [BuildR]-\"LinearRegression\" with 5/PHR [BuildR]-\"LinearRegression\" with 5/PTR [BuildR]-\"LinearRegression\" with 5/PHT [BuildR]-\"RandomForestRegressor\" with 5/PHTR [BuildR]-\"RandomForestRegressor\" with 5/PH [BuildR]-\"RandomForestRegressor\" with 5/PT [BuildR]-\"RandomForestRegressor\" with 5/PR [BuildR]-\"RandomForestRegressor\" with 5/P [BuildR]-\"RandomForestRegressor\" with 5/PHR [BuildR]-\"RandomForestRegressor\" with 5/PTR [BuildR]-\"RandomForestRegressor\" with 5/PHT [BuildR]-\"SVR\" with 5/PHTR [BuildR]-\"SVR\" with 5/PH [BuildR]-\"SVR\" with 5/PT [BuildR]-\"SVR\" with 5/PR [BuildR]-\"SVR\" with 5/P [BuildR]-\"SVR\" with 5/PHR [BuildR]-\"SVR\" with 5/PTR [BuildR]-\"SVR\" with 5/PHT [BuildR]-\"LinearRegression\" with 3/PHTR [BuildR]-\"LinearRegression\" with 3/PH [BuildR]-\"LinearRegression\" with 3/PT [BuildR]-\"LinearRegression\" with 3/PR [BuildR]-\"LinearRegression\" with 3/P [BuildR]-\"LinearRegression\" with 3/PHR [BuildR]-\"LinearRegression\" with 3/PTR [BuildR]-\"LinearRegression\" with 3/PHT [BuildR]-\"RandomForestRegressor\" with 3/PHTR [BuildR]-\"RandomForestRegressor\" with 3/PH [BuildR]-\"RandomForestRegressor\" with 3/PT [BuildR]-\"RandomForestRegressor\" with 3/PR [BuildR]-\"RandomForestRegressor\" with 3/P [BuildR]-\"RandomForestRegressor\" with 3/PHR [BuildR]-\"RandomForestRegressor\" with 3/PTR [BuildR]-\"RandomForestRegressor\" with 3/PHT [BuildR]-\"SVR\" with 3/PHTR [BuildR]-\"SVR\" with 3/PH [BuildR]-\"SVR\" with 3/PT [BuildR]-\"SVR\" with 3/PR [BuildR]-\"SVR\" with 3/P [BuildR]-\"SVR\" with 3/PHR [BuildR]-\"SVR\" with 3/PTR [BuildR]-\"SVR\" with 3/PHT 輸出本日最佳校正模型 在討論「本日最佳校正模型」時，必需先認知所謂的「最佳模型」必須在某一日結束後，才有可能總結 24 小時的資料作為測試資料，依據前述的方式獲得不同候選模型的 MAE 與 MSE 後，經過比較分析才能得到真正的最佳模型。因此，在本日 24 小時尚未結束前，是不可能有系統地產製真正的最佳校正模型的。\n但是，基於實務上的需要，我們常常必須在資料產製時，就需要能有校正模型可以套用；因此在實務上，我們通常會假設「昨日的最佳模型，在今日也會有不錯的表現」，並以此最為今日系統校正的方法。例如，假設我們在決定最佳模型時，是考慮帶入測試資料時能得到最小 MSE 的候選模型，那麼我們可以用下面的語法獲知該模型的資訊：\nFIELD= \"test_MSE\" BEST= AllResult_DF[ AllResult_DF[FIELD]== AllResult_DF[FIELD].min() ] BEST 接著，為了套入本日的情境，我們以昨日最佳校正模型的參數（歷史資料長度、迴歸方法、特徵資料組合），搭配從本日為基準重新計算日期區間的訓練資料與測試資料，重新產製今日的校正模型。\n# 訓練新模型 BEST_DC= BEST.to_dict(orient=\"index\")[0] Training_DF= SlideDay(All_DF, BEST_DC[\"day\"], TESTDATE)[ FEATURES_METHOD[BEST_DC[\"feature\"]]+ [\"EPA_PM25\"] ] result, lm= BuildModel( SITE, TODAY, BEST_DC[\"feature\"], BEST_DC[\"day\"], BEST_DC[\"method\"], Training_DF ) result [BuildR]-\"SVR\" with 3/PHT {'site': 'wanhua', 'day': 3, 'feature': 'PHT', 'method': 'SVR', 'datapoints': 80, 'modelname': 'wanhua_3_SVR_PHT', 'date': '2022-09-30', 'create_timestamp_utc': '2022-09-30 11:19:48', 'Train_MSE': 3.91517342356589, 'Train_MAE': 1.42125724796098} 從這個範例中，我們可以看到新產製出來的校正模型，其 Train_MSE 為 3.915，比昨日的 2.179 增加不少。然而，由於我們並沒有方法在本日結束前獲知本日真正的最佳模型，在沒有其他更好選擇的情況下，我們只能先以昨日最佳模型進行套用，並以下列語法匯出 .joblib 檔案，提供模型的開放分享使用（詳細使用方法請見參考資料）。\n# 儲存模型 model_dumpname= result[\"modelname\"]+ \".joblib\" # 儲存路徑 MODEL_OUTPUT_PATH= \"\" try: joblib.dump( lm, MODEL_OUTPUT_PATH+ model_dumpname ) print( \"[BuildR]-dump {}\".format( MODEL_OUTPUT_PATH+model_dumpname ) ) except Exceptionas e: print( \"ERROR! [dump model] {}\".format( result[\"modelname\"] ) ) error_msg(e) [BuildR]-dump wanhua_3_SVR_PHT.joblib 校正成果簡介 本章節所介紹的系統校正方法，自 2020/5 起已正式套用在民生公共物聯網中的中研院校園空品微型感測器系統上，並將每日產出的校正模型發布在 Dynamic Calibration Model 網站上。在套用的過程中，共計選定 31 個環保署空品測站分別裝置兩台空氣盒子，並於每日考量三種歷史資料長度、八種資料特徵組合、七種迴歸方法，共計 3 x 8 x 7 = 168 種排列組合，針對這 31 個測站位置分別產製每日的最佳校正模型；接著，針對每一台空氣盒子的感測資料，皆參考其地理位置最接近的測站最佳校正模型，作為其資料校正的套用模型，並產製與發佈各式的資料。從本機制上線後的實際運作觀察，確實有效拉近微型空品感測與環保署空品感測的數據差異（如下圖所示），對於民生公共物聯網在空品領域中的跨系統資料整合，建立良好的合作模式。\n參考資料 Dynamic Calibration Model Status Report (https://pm25.lass-net.org/DCF/) scikit-learn: machine learning in Python (https://scikit-learn.org/stable/) Joblib: running Python functions as pipeline jobs (https://joblib.readthedocs.io/) Jason Brownlee, Save and Load Machine Learning Models in Python with scikit-learn, Machine Learning Mastery (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/) ",
    "description": "我們使用空品類別資料，示範台灣微型空品感測器與官方測站進行動態校正的演算法，以做中學的方式，一步步從資料準備，特徵擷取，到機器學習、資料分析、統計與歸納，重現感測器動態校正模型演算法的原理與實作過程，讓讀者體驗如何透過疊加基本的資料分析與機器學習步驟，逐步達成進階且實用的資料應用服務。",
    "tags": [
      "Python",
      "空"
    ],
    "title": "6.3. 感測器聯合校正",
    "uri": "/ch6/ch6.3/"
  },
  {
    "content": "\nTable Of Contents 套件安裝與引用 讀取資料 Leafmap 基本操作 資料基本呈現方法 資料叢集呈現方法 更改 Leafmap 底圖 整合 OSM 資源 Heatmap 應用 分割視窗功能 Leafmap 成果網頁化 小結 參考資料 我們在之前的章節中，已經示範了如何使用程式語言針對地理屬性的資料進行分析，同時也示範了如何使用 GIS 軟體進行簡易的地理資料分析與呈現。接下來我們介紹如何使用 Python 語言中的 Leafmap 套件進行 GIS 應用，以及 Streamlit 套件進行網站開發。最後，我們將結合 Leafmap 和 Streamlit，自行製作簡單的網頁 GIS 系統，並將資料處理與分析後的結果，透過網頁方式來呈現。\n套件安裝與引用 在本章節中，我們將會使用到 pandas, geopandas, leafmap, ipyleaflet, osmnx, streamlit, geocoder 及 pyCIOT 等套件，這些套件除了 pandas 外，在我們使用的開發平台 Colab 上皆沒有預先提供，因此我們需要先自行安裝。由於本次安裝的套件數量較多，為了避免指令在執行後產生大量的文字輸出訊息，因此我們在安裝的指令中，增加了 ‘-q’ 的參數，可以讓畫面的輸出更精簡。\n!pip install -q geopandas !pip install -q leafmap !pip install -q ipyleaflet !pip install -q osmnx !pip install -q streamlit !pip install -q geocoder !pip install -q pyCIOT 待安裝完畢後，即可使用下列的語法先行引入相關的套件，完成本章節的準備工作：\nimport pandas as pd import geopandas as gpd import leafmap import ipyleaflet import osmnx import geocoder import streamlit from pyCIOT.data import * 讀取資料 在本章的範例中，我們使用民生公共物聯網資料平台上的環保署空品測站資料，以及中央氣象局與國震中心地震監測站資料。\n在環保署空品測站的部分，我們使用 pyCIOT 套件擷取所有環保署空品測站的最新一筆量測結果，並且透過 pandas 套件的 json_normalize() 方法，將所獲得的 JSON 格式資料，轉換成 DataFrame 格式，並且只留下其中測站名稱、緯度、經度與臭氧 (O3) 濃度資訊，以待後面操作時使用。這部分資料擷取與處理的程式碼如下：\nepa_station = Air().get_data(src=\"OBS:EPA\") df_air = pd.json_normalize(epa_station) df_air['O3'] = 0 for index, row in df_air.iterrows(): sensors = row['data'] for sensor in sensors: if sensor['name'] == 'O3': df_air.at[index, 'O3'] = sensor['values'][0]['value'] df_air = df_air[['name','location.latitude','location.longitude','O3']] df_air 接著我們用類似的方法，擷取中央氣象局與國震中心地震監測站的測站資料，並且只留下其中測站名稱、緯度與經度資訊，以待後面操作時使用。這部分資料擷取與處理的程式碼如下：\nquake_station = Quake().get_station(src=\"EARTHQUAKE:CWB+NCREE\") df_quake = pd.json_normalize(quake_station) df_quake = df_quake[['name','location.latitude','location.longitude']] df_quake 以上我們已經成功示範空品資料 (air) 和地震資料 (quake) 的讀取範例，在接下來的探討中，我們將利用這些資料進行 leafmap 套件的操作與應用，相同的方法也可以輕易改成使用其他民生公共物聯網資料平台上的其他資料而得到類似的結果，大家可以自行嘗試看看。\nLeafmap 基本操作 資料基本呈現方法 根據我們目前已處理好的空品測站資料 df_air 與地震測站資料 df_quake，我們首先這兩份資料的格式，從原本 pandas 套件所提供的 DataFrame 格式，轉成支援地理資訊屬性的 geopandas 套件所提供的 GeoDataFrame 格式；接著我們利用 leafmap 的 add_gdf() 方法，將兩份資料分為兩個圖層一次加入地圖。\ngdf_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') gdf_quake = gpd.GeoDataFrame(df_quake, geometry=gpd.points_from_xy(df_quake['location.longitude'], df_quake['location.latitude']), crs='epsg:4326') m1 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m1.add_gdf(gdf_air, layer_name=\"EPA Station\") m1.add_gdf(gdf_quake, layer_name=\"Quake Station\") m1 從程式輸出的地圖中，我們可以在地圖的右上角看到兩份資料的內容，已經依照兩個圖層的方式加入地圖，使用者可以依照自己的需求點選所要查詢的圖層進行瀏覽。然而，當我們想同時瀏覽兩個圖層的資料時，會發現兩個圖層使用相同的圖示進行呈現，因此在地圖上將產生混淆。\n為了解決這個問題，我們介紹另外一種資料呈現的方式，使用 ipyleaflet 套件所提供的 GeoData 圖層資料格式，並使用 leafmap 的 add_layer() 方法將 GeoData 圖層加入地圖。為了方便辨認，我們使用藍色的小圓形圖案表示空品測站的資料，並使用紅色的小圓形圖案表示地震測站的資料。\ngeo_data_air = ipyleaflet.GeoData( geo_dataframe=gdf_air, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3}, name=\"EPA stations\", ) geo_data_quake = ipyleaflet.GeoData( geo_dataframe=gdf_quake, point_style={'radius': 5, 'color': 'black', 'fillOpacity': 0.8, 'fillColor': 'red', 'weight': 3}, name=\"Quake stations\", ) m2 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m2.add_layer(geo_data_air) m2.add_layer(geo_data_quake) m2 資料叢集呈現方法 在某些資料應用場合，當地圖上的資料點位數量太多時，反而不容易進行觀察，這時我們可以使用叢集 (cluster) 的方式來呈現資料，也就是當小範圍內資料點位數量太多時，會將這些點位聚集在一起，並且顯示點位的數量；當使用者拉近 (zoom in) 地圖時，隨著地圖比例尺的修改，這些原本叢集的點位會被慢慢抽離，當小範圍內只剩下一個點位時，便可以直接看到點位的資訊。\n我們使用地震測站的資料進行示範，使用 leafmap 的 add_points_from_xy() 方法，便能將 df2 的資料以叢集的方式放上地圖。\nm3 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m3.add_points_from_xy(data=df_quake, x = 'location.longitude', y = 'location.latitude', layer_name=\"Quake Station\") m3 更改 Leafmap 底圖 Leafmap 使用 OpenStreetMap 的圖層做為預設的地圖底圖，但是也提供了超過百種的其他底圖選項，使用者可以依照自己的喜好與需求進行更改，並且可以使用下列語法獲知目前 leafmap 所支援的底圖：\nlayers = list(leafmap.basemaps.keys()) layers 我們從這些底圖中挑選 SATELLITE 和 Stamen.Terrain 作為示範，使用 leafmap 套件的 add_basemap() 方法將底圖加入成為新的圖層，加入後 leafmap 預設會開啟所有圖層，並按照加入順序進行疊加，但仍然可以透過右上角的圖層選單，點選自己所要使用的圖層。\nm4 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m4.add_gdf(gdf_air, layer_name=\"EPA Station\") m4.add_basemap(\"SATELLITE\") m4.add_basemap(\"Stamen.Terrain\") m4 除了使用 leafmap 所提供的底圖外，也可以使用 Google Map 的 XYZ Tiles 服務，加入 Google 衛星影像的圖層，其方法如下：\nm4.add_tile_layer( url=\"https://mt1.google.com/vt/lyrs=y\u0026x={x}\u0026y={y}\u0026z={z}\", name=\"Google Satellite\", attribution=\"Google\", ) m4 整合 OSM 資源 Leafmap 除了內建的多項資源外，也整合了許多外部的地理資訊資源，其中 OSM (OpenStreetMap) 便是一個著名且內容豐富的開源地理資訊資源，有關 OSM 所提供的各式資源，可以在 OSM 網站中查詢完整的屬性列表。\n在下列的範例中，我們透過 leafmap 套件提供的 add_osm_from_geocode() 方法，示範如何獲取城市的邊界輪廓，作為地圖呈現使用。我們以台中市為例，搭配之前使用的環保署空品測站點位資料，可以清楚看到哪些測站位於台中市內。\ncity_name = \"Taichung, Taiwan\" m5 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m5.add_layer(geo_data_air) m5.add_osm_from_geocode(city_name, layer_name=city_name) m5 我們接著繼續使用 leafmap 套件提供的 add_osm_from_place() 方法，針對台中市境內進一步尋找特定的設施，並加入地圖圖層。下方的程式以工廠設施為例，透過 OSM 的土地利用資料，找出台中市內的相關工廠地點與區域，可以與空品測站位置搭配進行分析判讀使用。有關 OSM 更多的設施種類，可以參考完整的屬性列表。\nm5.add_osm_from_place(city_name, tags={\"landuse\": \"industrial\"}, layer_name=city_name+\": Industrial\") m5 此外，leafmap 套件也提供以特定地點為中心，搜尋 OSM 鄰近設施的方法，對於分析與判讀資料，提供十分便利的功能。例如，在以下的範例中，我們使用 add_osm_from_address() 方法，搜尋台中清水車站 (Qingshui Station, Taichung) 方圓 1,000 公尺內的相關宗教設施 (屬性為 “amenity”: “place_of_worship”)；同時，我們使用 add_osm_from_point() 方法，搜尋台中清水車站 GPS 座標 (24.26365, 120.56917) 方圓 1,000 公尺內的相關學校設施 (屬性為 “amenity”: “school”)。最後，我們將這兩項查詢的結果，分別用不同的圖層疊加到既有的地圖上。\nm5.add_osm_from_address( address=\"Qingshui Station, Taichung\", tags={\"amenity\": \"place_of_worship\"}, dist=1000, layer_name=\"Shalu worship\" ) m5.add_osm_from_point( center_point=(24.26365, 120.56917), tags={\"amenity\": \"school\"}, dist=1000, layer_name=\"Shalu schools\" ) m5 Heatmap 應用 熱力圖是一種以顏色變化來顯示事件強度的一種二維空間表示法，將熱力圖與地圖搭配時，可以根據所使用的地圖比例不同，表達不同尺度下的事件強度狀態，是資料視覺化表示方法中一個非常普遍使用與功能強大的工具。然而，在繪製熱力圖時，使用者必須確認資料本身的特性適合使用熱力圖來呈現，否則極易與我們在單元五所介紹的 IDW 和 Kriging 等圖形化資料內插表示法產生混淆。例如，我們以上面空品測站資料的 O3 濃度資料為例，繪製對應的熱力圖如下：\nm6 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m6.add_layer(geo_data_air) m6.add_heatmap( df_air, latitude='location.latitude', longitude='location.longitude', value=\"O3\", name=\"O3 Heat map\", radius=100, ) m6 這張圖乍看之下並無明顯的問題，但如果我們將地圖拉近，放大台中市的區域後，便會發現熱力圖的樣貌發生極大的變化，在不同的尺度下呈現完全不同的結果。\n上述的範例其實便是一個熱力圖被誤用的案例，因為 O3 濃度的資料反映的是當地的 O3 濃度，其數值並不能因為地圖比例更改而隨著測站叢集後直接進行累加，同時也不能因為地圖比例更改而將累加的數值均勻分散給地圖上的鄰近區域；因此，範例中所使用的 O3 濃度資料，並不適用於熱力圖來表示，而應使用第五章所介紹的地理內插方法來繪製圖形。\n為了呈現熱力圖的真實效果，我們改用地震測站的點位資料，並且加入一個新的欄位 num，且預設值設為 10，接著我們用下列的程式碼產製台灣地震測站狀態的熱力圖。\ndf_quake['num'] = 10 m7 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m7.add_layer(geo_data_quake) m7.add_heatmap( df_quake, latitude='location.latitude', longitude='location.longitude', value=\"num\", name=\"Number of Quake stations\", radius=200, ) m7 分割視窗功能 在資料分析判讀的過程中，有時常需要切換不同的底圖以獲取不同的地理資訊，leafmap 套件因此提供 split_map() 的方法，可以將原本的地圖輸出分為所有兩個子畫面，並且各自套用不同的底圖，以方便地圖資訊的閱讀。其範例程式碼如下：\nm8 = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m8.add_gdf(gdf_air, layer_name=\"EPA Station\") m8.split_map( left_layer=\"SATELLITE\", right_layer=\"Stamen.Terrain\" ) m8 Leafmap 成果網頁化 Leafmap 套件的功能十分強大，為了能將處理好的地圖資訊快速分享出去，也特別提供了整合 Streamlit 套件的方式，可以將 Leafmap 的地理資訊 GIS 技術專長與 Streamlit 的 Web 技術專長結合，快速打造 Web GIS 系統。以下我們用一個簡單的範例進行示範，大家可以按照此原則自行擴充，建構自己的 Web GIS 服務。\n在 Streamlit 套件的使用中，建構一個網頁系統分為兩個步驟：\n將所要執行的 Python 程式內容打包成一個 Streamlit 物件，並且將打包過程寫入 app.py 檔案 在系統上執行 app.py 由於我們的操作過程都是使用 Google Colab 平台，在這個平台中我們可以直接將 app.py 用特殊的語法 %%writefile 寫入特殊的暫存區中，接著再由 Colab 直接從暫存區中讀取與執行。因此，針對步驟一的檔案寫入部分，我們可以按照下列的方式進行：\n%%writefile app.py import streamlit as st import leafmap.foliumap as leafmap import json import pandas as pd import geopandas as gpd from pyCIOT.data import * contnet = \"\"\" Hello World! \"\"\" st.title('Streamlit Demo') st.write(\"## Leafmap Example\") st.markdown(contnet) epa_station = Air().get_data(src=\"OBS:EPA\") from pandas import json_normalize df_air = json_normalize(epa_station) geodata_air = gpd.GeoDataFrame(df_air, geometry=gpd.points_from_xy(df_air['location.longitude'], df_air['location.latitude']), crs='epsg:4326') with st.expander(\"See source code\"): with st.echo(): m = leafmap.Map(center=(23.8, 121), toolbar_control=False, layers_control=True) m.add_gdf(geodata_air, layer_name=\"EPA Station\") m.to_streamlit() 針對步驟二的部分，我們則用下列的指令執行：\n!streamlit run app.py \u0026 npx localtunnel --port 8501 執行後便會出現類似下方的執行結果：\n接著可以點選其中 “your url is:” 這個字串後面的這個網址，便會在瀏覽器中出現類似下方的內容\n最後，我們點選 “Click to Continue”，便能成功開啟 app.py 中所打包的 Python 程式碼內容，在這個範例中，便能看到 leafmap 套件所呈現的環保署空品測站分佈圖。\n小結 在本章節我們介紹了如何用 Python 語言的 Leafmap 套件進行地理資料的呈現與資源整合，也同時介紹了如何結合 Streamlit 套件的網頁功能，在 Google Colab 平台上建立簡易的網頁化地理資訊系統服務，必須補充說明的是，Leafmap 還有許多更進階的功能，以及與前面其他章節內容的整併與呈現，皆受限於文章篇幅無法在這篇文章中涵蓋到，這些更進一步的探索學習，皆可參考其他章節的內容，或下方的其他參考資料做更多加深加廣的學習。\n參考資料 Leafmap Tutorial (https://www.youtube.com/watch?v=-UPt7x3Gn60\u0026list=PLAxJ4-o7ZoPeMITwB8eyynOG0-CY3CMdw) leafmap: A Python package for geospatial analysis and interactive mapping in a Jupyter environment (https://leafmap.org/) Streamlit 超快速又輕鬆建立網頁 Dashboard (https://blog.jiatool.com/posts/streamlit/) Streamlit Tutorial (https://www.youtube.com/watch?v=fTzlyayFXBM) Map features - OpenStreetMap Wiki (https://wiki.openstreetmap.org/wiki/Map_features) Heat map - Wikipedia (https://en.wikipedia.org/wiki/Heat_map) ",
    "description": "我們介紹 leafmap 套件在民生公共物聯網資料平台中使用不同類型數據進行地理信息表示和空間分析的能力，並演示了 leafmap 和 streamlit 套件的結合共同構建 Web GIS 的應用。透過跨域與跨工具的資源整合，將能拓展讀者對數據分析和信息服務未來的想像。",
    "tags": [
      "Python",
      "空",
      "地"
    ],
    "title": "7.3. Leafmap 應用",
    "uri": "/ch7/ch7.3/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "advanced",
    "uri": "/levels/advanced/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "API",
    "uri": "/tags/api/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Authors",
    "uri": "/authors/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "beginner",
    "uri": "/levels/beginner/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "intermediate",
    "uri": "/levels/intermediate/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Levels",
    "uri": "/levels/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Python",
    "uri": "/tags/python/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "水",
    "uri": "/tags/%E6%B0%B4/"
  },
  {
    "content": "民生公共物聯網資料應用 「民生公共物聯網」主要源自政府為整合與貼近民生公共相關服務，擬定民眾四大迫切需求，包括空氣品質、地震、水資源，以及災防等議題，於民國 106 年政府的「前瞻基礎建設 - 數位建設」計畫中，集結科技部、交通部、經濟部、內政部、環保署、中研院、農委會，共同建構的政府大型跨部會計畫，應用大數據、人工智慧、物聯網技術，建置各項智慧生活服務系統，協助政府與民眾共同面對環境變化所帶來的挑戰；同時，此計畫亦考量不同使用者的經驗，包括政府決策單位、學界、產業，以及一般民眾，以提供政府智慧化治理目標，協助產業/學界的發展，提升民眾的幸福感。\n為了收納所有民生公共物聯網系統所產出的各式資料，提供穩定、高品質的感測資料作為各項環境治理用途；同時也為了降低環境資訊落差，提供更即時與全面的環境資料數據，使民眾可隨時查詢生活周遭環境的即時感測資訊和時空變化，並做為產業加值應用開發的基礎，讓民間創意能量得以發揮，產出能解決民眾問題之優質服務，在「民生公共物聯網」中也特別規劃「民生公共物聯網資料服務平台」，以統一的資料格式，提供即時資料介接與歷史資料查詢服務，並且提高使用者瀏覽與搜尋的速度，建立感測資料儲存機制，提供模擬分析或人工智慧之應用。\n為了持續民生公共物聯網與其資料平台的良好發展基礎，也為了持續往下扎根，培育更多元的使用者族群踏入民生公共物聯網的各項資訊應用範疇，本次「民生公共物聯網資料應用專案」的主要目標有三： 1) 向下扎根，針對大專生與高中生，提供循序漸進的自學教材，進行跨越資訊、地理、地球科學與人文社會等領域的跨領域學習；2) 示範應用，針對既有的民生公共物聯網資料應用，以抽絲剝繭的方式，降低入門門檻，並引導產製更深一層的創新；3) 橫向擴展，提供不同於目前資料平台的資料存取方式，以大數據分析常用的 Python 語言，重新發展資料存取套件，拉近更多元的技術能量，豐沛資料平台的使用族群。\n",
    "description": "",
    "tags": null,
    "title": "民生公共物聯網資料應用",
    "uri": "/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "地",
    "uri": "/tags/%E5%9C%B0/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "吳姃家",
    "uri": "/authors/%E5%90%B3%E5%A7%83%E5%AE%B6/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "沈姿雨",
    "uri": "/authors/%E6%B2%88%E5%A7%BF%E9%9B%A8/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "災",
    "uri": "/tags/%E7%81%BD/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "空",
    "uri": "/tags/%E7%A9%BA/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "洪軾凱",
    "uri": "/authors/%E6%B4%AA%E8%BB%BE%E5%87%B1/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "高慧君",
    "uri": "/authors/%E9%AB%98%E6%85%A7%E5%90%9B/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "陳伶志",
    "uri": "/authors/%E9%99%B3%E4%BC%B6%E5%BF%97/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "陳宏穎",
    "uri": "/authors/%E9%99%B3%E5%AE%8F%E7%A9%8E/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "彭昱齊",
    "uri": "/authors/%E5%BD%AD%E6%98%B1%E9%BD%8A/"
  },
  {
    "content": "評論政策 我們歡迎對我們的文章所發表的評論，也很想聆聽您的意見，但請記住：\n友好、禮貌和相關； 不允許褻瀆、辱罵、人身攻擊或不當內容； 如果您騷擾他人或取笑他人的不幸，您的意見將被刪除； 不允許垃圾郵件文章、洗版或試圖出售任何東西的文章。 這裡的主要規則很簡單：尊重他人如同尊重自己。我們不希望看到發布煽動性內容、破壞性言論或其他不當內容的情事，這都將導致您的言論被禁止。\n我們的管理員將盡力不刪除評論，因為我們相信社群內的每個人都有各自獨立，且往往是強烈的意見和觀點。但是，我們保留出於上述原因以及以下原因刪除評論的權利：\n任何違反隱私的行為； 任何基於種族、宗教、膚色、國籍、殘疾、性取向的貶損和/或以任何可察覺的方式貶損的內容； 任何其他對社群有害的評論。 我們的管理員將負責捕捉不當內容，但我們也希望我們的社群可以一同幫助我們。如果您看到不當評論，您可以點擊留言右上角的旗標向我們舉報，我們將審核評論並決定是否保留或刪除該評論。\n在網站上找不到您的評論？ 有時我們會遇到技術困難，您的評論可能不會出現在我們的網站上。 我們可能沒有批准它，因為它違反了上述規則之一。 我們可能已將其刪除，因為它違反了我們的評論政策。 為什麼我被禁止發表評論？ 雖然我們已經盡最大的努力，讓每個人都可以在我們的社群中發表意見，但有時評論者仍會越界。我們會不時阻止和禁止違反規則的評論者，並且使用各種技術和資料來執行阻止，包括電子郵件、IP 地址和任何其他可用資料。\n這些措施可以防止辱罵性評論者未來在網站上發表評論，即使未來或其他評論不是辱罵性的。根據我們當時使用的軟體，禁止措施也可能導致該評論者之前的其他評論消失。\n",
    "description": "",
    "tags": null,
    "title": "評論政策",
    "uri": "/comment-policy/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "黃仁暐",
    "uri": "/authors/%E9%BB%83%E4%BB%81%E6%9A%90/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "概述",
    "uri": "/tags/%E6%A6%82%E8%BF%B0/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "鄭宇伸",
    "uri": "/authors/%E9%84%AD%E5%AE%87%E4%BC%B8/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "鍾明光",
    "uri": "/authors/%E9%8D%BE%E6%98%8E%E5%85%89/"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "羅泉恆",
    "uri": "/authors/%E7%BE%85%E6%B3%89%E6%81%86/"
  },
  {
    "content": "本網站為基於台灣民生公共物聯網各項開放資料，所撰寫之資料應用說明與範例。本網站所使用之各項開放資料，其授權規定請參考原始網站之說明內容；本網站所提供程式範例中，所使用到的 Python 語言套件與套裝軟體，其使用授權，請參考各套件與軟體之相關網頁的說明；本網站所提供之文章內容，採用 CC-BY (https://creativecommons.org/licenses/by/3.0/) 的創用 CC 授權。\n本網站的指導單位、執行單位、主持人與相關團隊資訊如下：\n指導單位：國家實驗研究院 科技政策研究與資訊中心 執行單位：中央研究院 資訊科學研究所 專案主持人：陳伶志 專家顧問團隊： 洪智傑，國立中興大學資訊管理系 汪立本，國立台灣大學土木工程系 黃仁暐，國立成功大學電機工程系 黃維嘉，LASS 社群 劉致灝，國家災害防救科技中心 洪翠屏，臺北市立育成高級中學 高慧君，臺北市立南港高級中學 編輯團隊：吳姃家、沈姿雨、洪軾凱、高慧君、陳宏穎、彭昱齊、黃仁暐、鄭宇伸、鍾明光、羅泉恆 審查團隊： 劉嘉凱，智庫驅動股份有限公司 許武龍，LASS社群 謝欣成，國立中央大學化學系 劉育宏，高雄市立前鎮高中 柯建華，基隆市立基隆高中 洪挺晏，國立臺灣師範大學附屬高中 ",
    "description": "",
    "tags": null,
    "title": "關於我們",
    "uri": "/about/"
  }
]
